\documentclass{article}
\usepackage[tmargin=0.05in, bmargin=0.05in]{geometry}
\input{theorem}
\input{prooftree}
\usepackage{stmaryrd}
\usepackage{latexsym}
\usepackage{yfonts}
\usepackage{amsmath}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{tikz}
\usetikzlibrary{calc,arrows,cd,decorations.pathreplacing}
\usetikzlibrary{decorations.pathmorphing}
\usepackage{tcolorbox}
\tcbuselibrary{breakable}
\usepackage{listings}
\lstset{basicstyle=\ttfamily\footnotesize,breaklines=true}

\def\gol{\sqsubset}
\def\gor{\sqsupset}

\def\sp{\bullet}
\def\ot{\leftarrow}
\def\prequiv{\dashv\vdash}
\def\fdom{{\mathbf{d}f}}
\def\fcod{{\mathbf{e}f}}
\def\fidom{\mathbf{\bar d}f}
\def\ficod{\mathbf{\bar e}f}

\def\thn{\mathrel|}
\def\coe{\mathsf{coe}}
\def\bpush{\mathbf{push}}
\def\bpull{\mathbf{pull}}

\def\wo{\setminus}
\def\fA{\mathsf{A}}
\def\fC{\mathsf{C}}
\def\fX{\mathsf{X}}
\def\ff{\mathsf{f}}
\def\fm{\mathsf{m}}
\def\rspan{\mathsf{Span}}
% \def\El#1{\mathsf{El}({#1})}
\def\El#1{\ulcorner{#1}\urcorner}
\def\U{\mathsf{U}}
\def\uu{\mathsf{u}}
\def\uni{\U}
\def\mor#1#2{#1 \to \underline{#2}}

\def\ridp{\mathsf{idp}}
\def\ssem#1{\langle\!\langle{#1}\rangle\!\rangle}

\definecolor{lred}{rgb}{0.95,0.8,0.8}
\definecolor{cyan}{rgb}{0.80,0.95,0.95}
\definecolor{orange}{rgb}{0.95,0.90,0.80}
\definecolor{bluegray}{rgb}{0.6,0.6,0.85}
\definecolor{lgray}{gray}{0.9}
\newtcolorbox{declbox}{colframe=lred,colback=lred,grow to right by=-10mm,grow to left by=-10mm,
boxrule=0pt,boxsep=0pt,breakable}
\newtcolorbox{thmbox}{colframe=cyan,colback=cyan,grow to right by=-10mm,grow to left by=-10mm,
boxrule=0pt,boxsep=0pt,breakable}
\newtcolorbox{defnbox}{colframe=orange,colback=orange,grow to right by=-10mm,grow to left by=-10mm,
boxrule=0pt,boxsep=0pt,breakable}
\newtcolorbox{tangentbox}{colframe=lgray,colback=lgray,grow to right by=-10mm,grow to left by=-10mm,
boxrule=0pt,boxsep=0pt,breakable}

\input{linear}
\def\rpath#1{[#1]}
\def\rfst{\mathsf{fst}\,}
\def\rsnd{\mathsf{snd}\,}
\def\bunfold{\mathbf{unfold}}
\def\bfold{\mathbf{fold}}
\def\susp#1{\langle {#1} \rangle}
\def\ssusp#1{\langle\!\langle {#1} \rangle\!\rangle}
\def\dto{\mathrel{\dot\to}}
\def\dns{{\downarrow}}
\def\ups{{\uparrow}}
\def\sprov{\Vdash}
\def\signat{{\color{red}\Sigma}}
\def\ep#1{\underline{#1}}
\def\wat{\mathbin{@}}
\def\wwat{\mathbin{\underline\wat}}
\def\cn{{:}}
\def\dv{{\div}}
\def\ww{{\mathsf w}}
\def\base{{\textfrak b}}

\def\munit{I}
\def\tt{{\mathsf t}}
\def\ii{{\mathbb I}}
\def\T{\mathbb{T}}
\def\TP{\mathbf{TPre}}
\def\pp{\textsf{\textbf p}}
\def\nn{{\mathsf n}}
\def\PP{\textsf{\textbf P}}
\def\NN{\textsf{\textbf N}}
\def\zz{{\mathsf z}}
\def\sem#1{[\![#1]\!]}
\def\usem#1{\lceil\!\!\lceil{#1}\rceil\!\!\rceil}
\def\zem#1{\langle\!\langle#1\rangle\!\rangle}
\def\col#1{{\mathsf{C}}_{#1}}
\def\lax{\mathop\bigcirc}
\def\rjust{\mathsf{just}}
\def\rcvt{\mathsf{cvt}}
\def\rtype{\mathsf{type}}
\def\rdtype{\mathsf{disc}}
\def\rkind{\mathsf{kind}}
\def\rmode{\mathsf{mode}}
\def\rprel{\mathbf{Prel}}
\def\rrfam{\mathbf{Rfam}}
\def\rset{\mathbf{Set}}
\def\rfinset{\mathbf{FinSet}}
\def\rrel{\mathbf{Rel}}
\def\rcat{\mathbf{Cat}}
\def\rfam{\mathbf{Fam}}
\def\binj{\mathbf{inj}}
\def\rid{\mathsf{id}}
\def\rtrue{\mathrel\mathrm{true}}
\def\pbck{\ar[dr, phantom, pos=0, "\lrcorner"]}
\def\bpbck{\ar[ddrr, phantom, pos=0, "\lrcorner"]}
\def\erule#1#2{\begin{prooftree}#1\justifies #2\end{prooftree}}
\def\lpar{\bindnasrepma}
\def\lamp{\binampersand}
\def\btwo{\mathbf{2}}
\def\bone{\mathbf{1}}

\newarrow {Equiv} =====
\def\FinSet{\mathbf{FinSet}}
\def\x{\times}
\def\maps{\ar[dd,mapsto,shorten <=2em, shorten >=2em]}
\def\mapt{\ar[ddd,mapsto,shorten <=2em, shorten >=2em]}
\def\frommaps{\ar[from=uu,mapsto,shorten <=2em, shorten >=2em, crossing over]}
\def\frommapt{\ar[from=uuu,mapsto,shorten <=2em, shorten >=2em, crossing over]}
\def\C{\mathbf{C}}
\def\G{\mathbf{G}}
\def\D{\mathbf{D}}
\def\DD{\mathbb{D}}
\def\E{\mathbf{E}}
\def\M{\mathbf{M}}
\def\pair#1#2{\langle#1,#2\rangle}
\def\ltri{{\lhd}}
\def\rtri{{\rhd}}
\def\tri{\rhd}
\def\ll{{<}}
\def\rr{{>}}
\def\cc{\mathsf{c}}
\def\dd{\mathsf{d}}
\def\o{\circ}
\def\rctx{\,\mathsf{ctx}}
\def\rdctx{\,\mathsf{dctx}}
\def\del{\partial}
\def\also#1{\ \textcolor{blue}{\celse #1}}
\newcounter{nodemaker}
\setcounter{nodemaker}{0}
\def\twocell#1#2{%
  \global\edef\mynodeone{twocell\arabic{nodemaker}}%
  \stepcounter{nodemaker}%
  \global\edef\mynodetwo{twocell\arabic{nodemaker}}%
  \stepcounter{nodemaker}%
  \ar[#1,phantom,shift left=3,""{name=\mynodeone}]%
  \ar[#1,phantom,shift right=3,""'{name=\mynodetwo}]%
  \ar[Rightarrow,#2,from=\mynodeone,to=\mynodetwo]%
}
\def\twocellswap#1#2{%
  \global\edef\mynodeone{twocell\arabic{nodemaker}}%
  \stepcounter{nodemaker}%
  \global\edef\mynodetwo{twocell\arabic{nodemaker}}%
  \stepcounter{nodemaker}%
  \ar[#1,phantom,shift right=3,""{name=\mynodeone}]%
  \ar[#1,phantom,shift left=3,""'{name=\mynodetwo}]%
  \ar[Rightarrow,#2,from=\mynodeone,to=\mynodetwo]%
}
\def\maps{\ar[dd,mapsto,shorten <=2em, shorten >=2em]}
\author{Jason Reed}
\definecolor{fib}{HTML}{ff7f00}
\definecolor{green}{HTML}{007f00}
\definecolor{red}{HTML}{df3f3f}
\definecolor{opfib}{HTML}{007fff}

% For lightening colors I used
% http://trycolors.com/ 6 parts white to 1 part color
\definecolor{opfibl}{HTML}{DBEDFF}
\definecolor{greenl}{HTML}{DBEDDB}
\definecolor{redl}{HTML}{FAE4E4}
\def\njudge#1{\fcolorbox{opfib}{white}{#1}}
\def\nrule#1{\fcolorbox{white}{opfibl}{#1}}
\def\neqn#1{\fcolorbox{white}{redl}{#1}}



\def\judge#1{\vskip 2em\noindent \njudge{$#1$}\vskip 0.5em\noindent}
\def\rule#1{\vskip 2em \noindent \nrule{$#1$}\vskip 0.5em\noindent}
\def\eqn#1{\vskip 2em \noindent \neqn{$#1$}\vskip 0.5em\noindent}
\def\prof{\nrightarrow}
\def\tensor{\otimes}
\def\htensor{\mathrel{\hat\otimes}}
\def\lol{\multimap}
\def\hlol{\mathrel{\hat\multimap}}
\def\wtf{{\color{red}???}}

\def\re{\mathbf{E}}
\def\sh{\sharp}
\def\shp{\mathrel{\sharp}}
\def\zero{\texttt{0}}
\begin{document}

\def\
\tikzset{
   commutative diagrams/.cd,
   arrow style=tikz,
   diagrams={>=stealth}}


\section{Carefully Checking A Case I Believe Should Work}
Suppose we have functors
\[R : (\PP \x \PP^\op) \x (\PP \x \PP^\op) \x \PP \to \rset\]
\[P_1, P_2 : \PP \to \rset \]
I want to see whether I can validate the focusing $\tensor {R}$ rule
\[
\erule
{\Delta_1 \prov [P_1] \qquad\Delta_2  \prov [P_2] }
{\Delta_1, \Delta_2\prov [P_1 \tensor P_2]}
\]
Which I take to mean, if there are natural transformations
\[ \zeta_1 : \sem{\Delta_1} \to \sem {P_1}\qquad
 \zeta_2 : \sem {\Delta_2} \to \sem {P_2}\]
then there is a natural transformation
\[ \zeta : \sem{\Delta_1} \htensor \sem {\Delta_2} \to \sem{P_1} \htensor \sem {P_2}\]
where
\[ (F_1 \htensor F_2)(\alpha) =
\int^{\beta_1 \beta_2} F_1(\beta_1) \x F_2(\beta_2) \x R(\beta_1, \beta_1, \beta_2, \beta_2, \alpha)\]
Ok, to be a natural transformation means for every object $\alpha \in \PP$ we need an arrow
\[(\Delta_1, \Delta_2)_\alpha \to (P_1 \tensor P_2)_\alpha\] i.e.
\[
\int^{\beta_1 \beta_2} \sem{\Delta_1}(\beta_1) \x \sem{\Delta_2}(\beta_2) \x R(\beta_1, \beta_1, \beta_2, \beta_2, \alpha)
\]
\[
\to \int^{\beta_1 \beta_2} \sem{P_1}(\beta_1) \x \sem{P_2}(\beta_2) \x R(\beta_1, \beta_1, \beta_2, \beta_2, \alpha)
\]
and for every morphism $m : \alpha \to \alpha'$ we need
\[
\begin{tikzcd}
  (\Delta_1, \Delta_2)_\alpha \ar[r]\ar[d, "{(\Delta_1, \Delta_2)_m}"']
& (P_1 \tensor P_2)_\alpha\ar[d, "{(P_1 \tensor P_2)_m}"]\\
  (\Delta_1, \Delta_2)_{\alpha'} \ar[r]& (P_1 \tensor P_2)_{\alpha'}
\end{tikzcd}
\]
Ok, recall how we get in and out of coends. We make something of a coend by
making it at the diagonal. We use something of coend type by getting it at the
diagonal and being required to use it in such a way that it respects transport.

So we're handed a
\[ \sem{\Delta_1}(\beta_1) \x \sem{\Delta_2}(\beta_2) \x R(\beta_1, \beta_1, \beta_2, \beta_2, \alpha) \]
and we make
\[ \sem{P_1}(\beta_1) \x \sem{P_2}(\beta_2) \x R(\beta_1, \beta_1, \beta_2, \beta_2, \alpha)\]
by just plugging in the natural transformations $\Delta_1 \to P_1$ and $\Delta_2 \to P_2$
that we assumed we had.

Let's verify the coend property is respected. If we have morphisms $m_1 : \beta_1 \to \beta'_1$
and $m_2 : \beta_2 \to \beta'_2$, we want to confirm that this diagram commutes:
\[
\begin{tikzcd}
&[-10em]\sem{\Delta_1}(\beta_1) \x \sem{\Delta_2}(\beta_2)
\x R(\beta_1, \beta'_1, \beta_2, \beta'_2, \alpha)\ar[dr]\ar[dl]\\
  \sem{\Delta_1}(\beta_1) \x \sem{\Delta_2}(\beta_2)
\x R(\beta_1, \beta_1, \beta_2, \beta_2, \alpha)\ar[d]&&[-10em]
\sem{\Delta_1}(\beta'_1) \x \sem{\Delta_2}(\beta'_2)
\x R(\beta'_1, \beta'_1, \beta'_2, \beta'_2, \alpha)\ar[d]\\
  \sem{P_1}(\beta_1) \x \sem{P_2}(\beta_2)
\x R(\beta_1, \beta_1, \beta_2, \beta_2, \alpha)\ar[dr]&&
\sem{P_1}(\beta'_1) \x \sem{P_2}(\beta'_2)
\x R(\beta'_1, \beta'_1, \beta'_2, \beta'_2, \alpha)\ar[dl]\\
&\sem{P_1}(\beta'_1) \x \sem{P_2}(\beta'_2)
\x R(\beta'_1, \beta_1, \beta'_2, \beta_2, \alpha)
\end{tikzcd}
\]
which is just the product-wise overlaying of naturality of how we go $\Delta_1 \to P_1$
and $\Delta_2 \to P_2$, and the interchange of functoriality in the arguments of $R$.

Do we have naturality? If I take the diagonal elements of the coend as authoritative, then I see
(for $m : \alpha \to \alpha'$)
\[
\begin{tikzcd}
\sem{\Delta_1}(\beta_1) \x \sem{\Delta_2}(\beta_2)
\x R(\beta_1, \beta_1, \beta_2, \beta_2, \alpha)
\ar[r]\ar[d, "\cdots m"']
&
\sem{P_1}(\beta_1) \x \sem{P_2}(\beta_2)
\x R(\beta_1, \beta_1, \beta_2, \beta_2, \alpha)
\ar[d, "\cdots m"]
\\
\sem{\Delta_1}(\beta_1) \x \sem{\Delta_2}(\beta_2)
\x R(\beta_1, \beta_1, \beta_2, \beta_2, \alpha')
\ar[r]
&
\sem{P_1}(\beta_1) \x \sem{P_2}(\beta_2)
\x R(\beta_1, \beta_1, \beta_2, \beta_2, \alpha')
\end{tikzcd}
\]
Yeah, since the composite arrow $\Delta_1, \Delta_2 \to P_1 \tensor P_2$ is just
the identity on the $R$-component, naturality is easy here.

\subsection{Just Making Sure I Have The Right Intuitions about Ends}

For $F, G : \C\x\C^\op \to \rset$, a morphism $\int_c F(c, c) \to \int_c G(c, c)$ is given by
a map $F(c, c) \to G(c, c)$ for each $c$ such that
\[
\begin{tikzcd}
&F(c, d)\ar[dr]\ar[dl]\\
F(c, c)
\ar[d]&&
F(d, d)\ar[d]\\
 G(c, c)\ar[dr]&&
G(d, d)\ar[dl]\\
&G(d, c)
\end{tikzcd}
\]
Right? For $\int_c F(c, c)$ is the equalizer
\[
\begin{tikzcd}
  \int_c F(c, c) \ar[r, >->]& \prod_{c} F(c, c) \ar[r, shift left]\ar[r, shift right] & \prod_{c, d} F(c, d)
\end{tikzcd}
\]
So we construct a map into $\int_c G(c, c)$ by
\[
\begin{tikzcd}
  \int_c F(c, c) \ar[r, >->]& \prod_{c} F(c, c) \ar[r, shift left]\ar[r, shift right] \ar[d]& \prod_{c, d} F(c, d)\\
  \int_c G(c, c) \ar[r, >->]& \prod_{c} G(c, c) \ar[r, shift left]\ar[r, shift right] & \prod_{c, d} G(c, d)
\end{tikzcd}
\]
Right, no, I am thinking about this a little sloppily. The hexagon above is literally
what it means to belong to an end $\int_c F(c, c) \to G(c, c)$.
What would suffice to commute to get $\int_c F(c, c) \to \int_c G(c, c)$ is actually
\[
\begin{tikzcd}
&\int_c F(c, c)\ar[dr, "\pi"]\ar[dl, "\pi"']\\
F(c, c)
\ar[d]&&
F(d, d)\ar[d]\\
 G(c, c)\ar[dr]&&
G(d, d)\ar[dl]\\
&G(d, c)
\end{tikzcd}
\]
If I had a natural transformation $F \to G$, then I would have
\[
\begin{tikzcd}
&\int_c F(c, c)\ar[dr, "\pi"]\ar[dl, "\pi"']\\
F(c, c)\ar[dr]
\ar[d]&&
F(d, d)\ar[d]\ar[dl]\\
 G(c, c)\ar[dr]&F(d, c)\ar[d]&
G(d, d)\ar[dl]\\
&G(d, c)
\end{tikzcd}
\]
Anyway, it makes me think I need to actually check the commutativity of
\[
\begin{tikzcd}
&[-10em]\sem{\Delta_1}(\beta_1) \x \sem{\Delta_2}(\beta_2)
\x R(\beta_1, \beta'_1, \beta_2, \beta'_2, \alpha)\ar[dr]\ar[dl]\\
  \sem{\Delta_1}(\beta_1) \x \sem{\Delta_2}(\beta_2)
\x R(\beta_1, \beta_1, \beta_2, \beta_2, \alpha)\ar[d]&&[-10em]
\sem{\Delta_1}(\beta'_1) \x \sem{\Delta_2}(\beta'_2)
\x R(\beta'_1, \beta'_1, \beta'_2, \beta'_2, \alpha)\ar[d]\\
  \sem{P_1}(\beta_1) \x \sem{P_2}(\beta_2)
\x R(\beta_1, \beta_1, \beta_2, \beta_2, \alpha)\ar[dr]&&
\sem{P_1}(\beta'_1) \x \sem{P_2}(\beta'_2)
\x R(\beta'_1, \beta'_1, \beta'_2, \beta'_2, \alpha)\ar[dl]\\
&\int^{\beta_1\beta_2}\sem{P_1}(\beta_1) \x \sem{P_2}(\beta_2)
\x R(\beta_1, \beta_1, \beta_2, \beta_2, \alpha)
\end{tikzcd}
\]
ah... but this almost feels plausible, since the processes $\Delta_1 \to P_1$ and
$\Delta_2 \to P_2$ {\em were} natural transformations, and the relation $R$
is sort of `at rest'.

Let me try to get some better notation. $P$ and $Q$ are the positive props. Write
covariant arguments subscript and contravariant arguments superscript.

I have $P, Q, \Delta, \Gamma: \PP \to \rset$. I have natural tranformations $\zeta : \Delta \to P$
and $\xi : \Gamma \to Q$. I have objects $\alpha, \beta,\gamma,\beta',\gamma'\in\PP$.
 I have morphisms $m : \beta \to \beta', n : \gamma \to \gamma' \in \PP$.
{\em If} we can satisfy the diagram
\[
\begin{tikzcd}
&[-5em]
\Delta_\beta \x \Gamma_\gamma\x R_{\beta \gamma\alpha}^{\beta' \gamma'}
\ar[dr, "{\Delta_m \x \Gamma_n\x R_{m n1}^{1}}"]
\ar[dl, "{{\Delta_1} \x {\Gamma_1}\x R_{111}^{m n}}"']
\\
\Delta_\beta \x \Gamma_\gamma\x R_{\beta \gamma\alpha}^{\beta \gamma}
\ar[d, "\zeta_\beta \x \xi_\beta \x 1"']
&&[-5em]
{\Delta}_{\beta'} \x {\Gamma}_{\gamma'}\x R_{\beta' \gamma'\alpha}^{\beta' \gamma'}
\ar[d, "\zeta_{\beta'} \x \xi_{\beta'} \x 1"]
\\
{P}_\beta \x {Q}_\gamma\x R_{\beta \gamma\alpha}^{\beta \gamma}
\ar[dr, "\iota_{\beta\gamma}"']
&&
{P}_{\beta'} \x {Q}_{\gamma'}\x R_{\beta' \gamma'\alpha}^{\beta' \gamma'}
\ar[dl, "\iota_{\beta'\gamma'}"]
\\
&
\int^{\beta\gamma}{P}_\beta \x {Q}_\gamma\x R_{\beta \gamma\alpha}^{\beta \gamma}
\end{tikzcd}
\]
then we get a morphism
\[\int^{\beta\gamma}\Delta_\beta \x \Gamma_\gamma \x R^{\beta\gamma}_{\beta\gamma\alpha}
\to \int^{\beta\gamma}P_\beta \x Q_\gamma \x R^{\beta\gamma}_{\beta\gamma\alpha}\]
as we wish. But naturality of $\zeta, \xi$ means the upper right cell of
\[
\begin{tikzcd}
&
\Delta_\beta \x \Gamma_\gamma\x R_{\beta \gamma\alpha}^{\beta' \gamma'}
\ar[dr]
\ar[dl]
\ar[d]
\\
\Delta_\beta \x \Gamma_\gamma\x R_{\beta \gamma\alpha}^{\beta \gamma}
\ar[d]
&
P_\beta \x Q_\gamma\x R_{\beta \gamma\alpha}^{\beta' \gamma'}
\ar[dl]\ar[dr]
&
{\Delta}_{\beta'} \x {\Gamma}_{\gamma'}\x R_{\beta' \gamma'\alpha}^{\beta' \gamma'}
\ar[d]
\\
{P}_\beta \x {Q}_\gamma\x R_{\beta \gamma\alpha}^{\beta \gamma}
\ar[dr]
&&
{P}_{\beta'} \x {Q}_{\gamma'}\x R_{\beta' \gamma'\alpha}^{\beta' \gamma'}
\ar[dl]
\\
&
\int^{\beta\gamma}{P}_\beta \x {Q}_\gamma\x R_{\beta \gamma\alpha}^{\beta \gamma}
\end{tikzcd}
\]
commutes. The upper left cell is basically trivial. The bottom cell is the coend
property of $\int^{\beta\gamma}{P}_\beta \x {Q}_\gamma\x R_{\beta \gamma\alpha}^{\beta \gamma}$.

\subsection{Syntax/Semantics}

Ok, what derivation in the `syntactic semantics' underlies the use of
the $\tensor$-right rule here? We translate
\[
\erule
{\Delta \prov [P] \qquad\Gamma  \prov [Q] }
{\Delta, \Gamma\prov [P \tensor Q]}
\]
to
\[
\erule
{\Delta  \prov P \wat \alpha_{\Delta} \qquad\Gamma   \prov Q \wat \alpha_{\Gamma} }
{\Delta, \Gamma\prov (P \tensor Q) \wat (\alpha_{\Delta} \tensor \alpha_{\Gamma})}
\]
which I can isomorphically reinterpret as
\[
\erule
{\beta, \Delta_\beta  \prov P_\beta \qquad
 \gamma, \Gamma_\gamma   \prov Q_\gamma }
{\alpha, \beta, \gamma, R^{\beta\gamma}_{\beta\gamma\alpha}, \Delta, \Gamma
\prov (P \tensor Q)_\alpha}
\]
or, expanding out the meaning of $\tensor$ as a logical formula,
\[
\erule
{\beta, \Delta_\beta  \prov P_\beta \qquad
 \gamma, \Gamma_\gamma   \prov Q_\gamma }
{\alpha, \beta, \gamma, R^{\beta\gamma}_{\beta\gamma\alpha}, \Delta, \Gamma
\prov \exists \beta \gamma . P_\beta \x Q_\gamma \x R^{\beta\gamma}_{\beta\gamma\alpha}}
\]
So the essential thing I'm doing is, in fact, $\exists R$:
\[
\erule
{
\[
\[\justifies \beta, \Delta_\beta  \prov P_\beta \]
\[\justifies \gamma, \Gamma_\gamma   \prov Q_\gamma\]
\[\justifies R^{\beta\gamma}_{\beta\gamma\alpha}  \prov R^{\beta\gamma}_{\beta\gamma\alpha}\]
\justifies
\alpha, \beta, \gamma, R^{\beta\gamma}_{\beta\gamma\alpha}, \Delta_\beta, \Gamma_\gamma
\prov  P_\beta \x Q_\gamma \x R^{\beta\gamma}_{\beta\gamma\alpha}
\]}
{\alpha, \beta, \gamma, R^{\beta\gamma}_{\beta\gamma\alpha}, \Delta_\beta, \Gamma_\gamma
\prov \exists \beta \gamma . P_\beta \x Q_\gamma \x R^{\beta\gamma}_{\beta\gamma\alpha}}
\]
So my gut feeling is that there's a stronger-than-normal sense in which
$\Delta_\beta \prov P_\beta$ --- namely, something arising from the fact that it
holds in an environment where the free variable
$\beta$ is only used monovariantly --- and the same is true of $\Gamma_\gamma \prov Q_\gamma$.
And also perhaps somehow the {\em identity} entailment
$R^{\beta\gamma}_{\beta\gamma\alpha} \prov R^{\beta\gamma}_{\beta\gamma\alpha}$
is in this stronger-than-normal subset as well.

I conjecture this stronger-than-normal entailment is what $\exists R$ and $\forall L$ need
to require in their body. Perhaps there are two strengthenings of the hypothetical judgment,
one appropriate for $\exists$, and one for $\forall$. The $\exists$ strengthening I suppose
is preserved by product. Not sure about $\forall$ yet.

What's something that I do know? That, supposing there are natural transformations
\[\Delta \to P \qquad \Gamma \to Q\]
then there {\em is} an inhabitant of the end
\[\int_{\alpha\beta\gamma} ( \Delta_\beta \x \Gamma_\gamma \x R^{\beta\gamma}_{\beta\gamma\alpha}) \to P_\beta \x Q_\gamma \x R^{\beta\gamma}_{\beta\gamma\alpha}\]
{\em and also} of the end-coend
\[\int_{\alpha\beta\gamma} ( \Delta_\beta \x \Gamma_\gamma \x R^{\beta\gamma}_{\beta\gamma\alpha}) \to \int^{\alpha\beta\gamma} P_\beta \x Q_\gamma \x R^{\beta\gamma}_{\beta\gamma\alpha}\]
which by mere coend-fu is the same as the map of coends
\[\left(\int^{\alpha\beta\gamma}  \Delta_\beta \x \Gamma_\gamma \x R^{\beta\gamma}_{\beta\gamma\alpha}\right) \to \int^{\alpha\beta\gamma} P_\beta \x Q_\gamma \x R^{\beta\gamma}_{\beta\gamma\alpha}\]

Let's confirm that my observation that $\forall L$ doesn't hold extends to $\exists R$.
What I mean by $\exists R$ is the general claim that
\[
\erule
{\int_\alpha F^\alpha_\alpha \to G^\alpha_\alpha}
{\int_\alpha F^\alpha_\alpha \to \int^\alpha G^\alpha_\alpha}
\]
If we have some $M :\int_\alpha F^\alpha_\alpha \to G^\alpha_\alpha$,
we do have $M_d : F^d_d \to G^d_d$ for every particular $d \in \C$.
So it seems like we can build $P : \int_\alpha F^\alpha_\alpha \to \int^\alpha G^\alpha_\alpha$
by setting
\[P_d = \lambda x . \pair d {M_d(x)} \]
but we have to check that $P$ actually lives in the end. (There's nothing
to check for getting into the coend; the injections always work)
So we want to check that we have convergence for any morphism $f : d \to e$ like
\[
\begin{tikzcd}
  F^d_d \to \int^\alpha G^\alpha_\alpha \ar[dr]
&&  F^e_e \to \int^\alpha G^\alpha_\alpha \ar[dl]\\
&
  F^e_d \to \int^\alpha G^\alpha_\alpha
\end{tikzcd}
\]
That is, we must check
\[ P_d \o F^f \equiv P_e \o F_f \]
i.e.
\[ (x : F^e_d) \to   \pair d {M_d(F^f(x))} = \pair e {M_e(F_f(x))} : \int^\alpha  G^\alpha_\alpha\]
but what we have is that the coend on $G$ iself licenses
\[ (y : G^e_d) \to   \pair d {G^f(y)} = \pair e {G_f(y)} : \int^\alpha  G^\alpha_\alpha\]
and the end-property of $M$ licenses
\[
\begin{tikzcd}
  F^d_d \to  G^d_d \ar[dr]
&&  F^e_e \to  G^e_e \ar[dl]\\
&
  F^e_d \to  G^d_e
\end{tikzcd}
\]
i.e.
\[ (x : F^e_d) \to   (G_f \o M_d \o F^f)(x) = (G^f \o M_e \o F_f)(x) :  G^d_e\]
But we don't know that $M_d(F^f(x))$ or $M_e(F_f(x))$ actually arise from $G$-transport, so
we're stuck, as I'd expected/hoped.

\subsection{Ways This Can Go Ok}

One thing that should suffice is when $F$ and $G$ are {\em both} covariant.
Then I must check
\[ (x : F_d) \to   \pair d {M_d(x)} = \pair e {M_e(F_f(x))} : \int^\alpha  G_\alpha\]
knowing
\[ (y : G_d) \to   \pair d {y} = \pair e {G_f(y)} : \int^\alpha  G_\alpha\]
\[ (x : F_d) \to   G_f \o M_d  =  M_e \o F_f :  G_e\]
This means the coend equation has one side where we don't need to require transport;
we already have that
\[\pair d {M_d(x)} = \pair e {G_f(M_d(x))}\]
and then the end-property of $M$ says
\[\pair e {G_f(M_d(x))} = \pair e {M_e(F_f(x))}\]
which finishes the proof.

\subsubsection{$G$ covariant}
Does this work if only $G$ is purely covariant, even if $F$ isn't?
Then we must check
\[ (x : F^e_d) \to   \pair d {M_d(F^f(x))} = \pair e {M_e(F_f(x))} : \int_\alpha  G_\alpha\]
knowing
\[ (y : G_d) \to   \pair d {y} = \pair e {G_f(y)} : \int^\alpha  G_\alpha\]
\[ (x : F^e_d) \to   G_f \o M_d \o F^f =  M_e \o F_f :  G_e\]
In this case, we calculate
\[\pair d {M_d(F^f(x))} = \pair e {G_f(M_d(F^f(x)))} = \pair e {M_e(F_f(x))}\]
As required.

\subsubsection{$G$ contravariant}
Does this work if only $G$ is purely contravariant?
Then we must check
\[ (x : F^e_d) \to   \pair d {M_d(F^f(x))} = \pair e {M_e(F_f(x))} : \int_\alpha  G^\alpha\]
knowing
\[ (y : G^e) \to   \pair d {G^f(y)} = \pair e {y} : \int^\alpha  G^\alpha\]
\[ (x : F^e_d) \to    M_d \o F^f = G^f \o M_e \o F_f :  G^d\]
Here we calculate `backwards',
\[\pair e {M_e(F_f(x))} = \pair d {G^f(M_e(F_f(x)))} = \pair d {M_d(F^f(x))}\]
Ok, this is great! Existential introduction works as long as $G$ is monovariant!
But this isn't enough. I need to be able to mix in other variances of the variable.

\subsubsection{$G$ product of two variances}
What happens if we have $G^\alpha$ and $H_\alpha$, and we're trying to validate
\[
\erule
{\int_\alpha F^\alpha_\alpha \to G^\alpha \x H_\alpha}
{\int_\alpha F^\alpha_\alpha \to \int^\alpha G^\alpha \x H_\alpha}
\]
Then we have  $M_d : F^d_d \to G^d$ and $N_d : F^d_d \to H_d$ and we need to check
\vskip 1in
\[ (x : F^e_d) \to   \pair d {M_d(F^f(x)), N_d(F^f(x))} = \pair e {M_e(F_f(x)), N_e(F_f(x))} : \int_\alpha  G^\alpha \x H_\alpha\]
knowing
\[ (y : G^e) (z : H_d)\to   \pair d {G^f(y), z} = \pair e {y, H_f(z)} : \int^\alpha  G^\alpha \x H_\alpha\]
\[ (x : F^e_d) \to    M_d \o F^f = G^f \o M_e \o F_f :  G^d\]
\[ (x : F^e_d) \to   H_f \o N_d \o F^f =  N_e \o F_f :  H_e\]

Huh, here I might be stuck. I can't get started with the  coend equality on either
side, because it requires either that one of the components of the tuple is
 already $G$- or $H$-transport.

But can I establish for certain proofs that they {\em do} have a representative
upstream along one of these transports? Maybe that's what's special about the identity proof
of the multiplicative connective relation.

\subsection{Looking at simpler proofs involving bivariant functors}

Ok, but surely I {\em can} do
\[ \int_\alpha \left( R^\alpha_\alpha \to \int^\alpha R^\alpha_\alpha \right)\]
because that's the same thing (by coend-fu) as
\[ \left(\int^\alpha R^\alpha_\alpha\right) \to \int^\alpha R^\alpha_\alpha\]
What's special about this proof? For every $d \in \C$ we take
a $R^d_d$ to the injection in $\int^\alpha R^\alpha_\alpha$.
The required end property is that
\[
\begin{tikzcd}
  R^d_d \to  \int^\alpha R^\alpha_\alpha \ar[dr]
&&  R^e_e \to  \int^\alpha R^\alpha_\alpha \ar[dl]\\
&
  R^e_d \to  \int^\alpha R^\alpha_\alpha
\end{tikzcd}
\]
\[ (x : R^e_d) \to   ( M_d \o R^f)(x) = ( M_e \o R_f)(x) :  \int^\alpha R^\alpha_\alpha\]
that is,
\[ (x : R^e_d) \to    \pair d {R^fx} = \pair e {R_f x} :  \int^\alpha R^\alpha_\alpha\]
But this is, of course, exactly the coend property.

\subsection{What's the more general $\forall L$ case that I can make work?}
Let's try working with just ends, not coends.
Based on expecting that synchronous shift rules to work, I expect
to be able to validate {\em something} like
\[
\erule
{\int_\alpha (F_\alpha \to G^\alpha_\alpha) \to H^\alpha_\alpha }
{\int_\alpha \left( \int_\beta F_\beta \to G^\beta_\beta \right) \to  H^\alpha_\alpha}
\]
but this is already more general than
\[
\erule
{\int_\alpha G^\alpha_\alpha \to H^\alpha_\alpha }
{\int_\alpha \left( \int_\beta  G^\beta_\beta \right) \to  H^\alpha_\alpha}
\]
which I know doesn't work; this is the counterexample where $G$ is $0111$
and $H$ is $0112$. The critical thing is that the $G$-to-$H$ proof is
trivial/special/simple somehow.

A case I can make work is
\[
\erule
{\int_\alpha F_\alpha \to G^\alpha_\alpha }
{\int_\alpha \left( \int_\beta F_\beta \right) \to  G^\alpha_\alpha}
\]
for there I  have $M : \int_\alpha F_\alpha \to G^\alpha_\alpha $
and I make $P : {\int_\alpha \left( \int_\beta F_\beta \right) \to  G^\alpha_\alpha}$
by
\[ P_d = \lambda w . M_d(w_d)\]
and what I know is
\[  G_f \o M_d = G^f \o M_e \o F_f  \]
\[  F_f w_d = w_e  \]
and what I need to check is
\[  G_f \o P_d = G^f \o P_e   \]
i.e.
\[  G_f(M_d(w_d)) = G^f(M_e(w_e))   \]
which is done by reasoning
\[G^f(M_e(w_e)) = G^f(M_e(F_f(w_d))) = G_f(M_d(w_d)) \]

\subsection{Specialness}
So the `specialness' in a putative fixed inference rule
\[
\erule
{\Gamma, [t/\beta](F \to G) \prov H }
{\Gamma,   \forall \beta. F \to G \prov  H}
\]
Seems like it probably wants to go into the {\em conclusion} $H$, for
otherwise the `special assumption' $F \to G$ needs to break down into more
kinds of special things; the specialness of $F$ being monovariant,
and of $G$ being usable-only-at-the-identity. Well... no, I guess I have
to implement those sorts of specialness also even if $H$ is special. Not sure
I have enough asymmetry to guide me here.

And if I'm to use that reasoning, then in the `fixed' $\exists R$ rule
\[
\erule
{\Gamma, [t/\beta](F \x G)  }
{\Gamma\prov   \exists \beta. F \x G }
\]
it seems like the only thing I can make special is the `complex
thing', in case the conclusion.

This actually is leading me towards thinking that it is the `complex' side that
deserves special judgmental treatment, because it seems to line up so well with
being in focus position.

\subsection{A Syntactic Conjecture}

I want things to shape up like this, where we have some usual rules
like
\[
\erule
{\Gamma \prov [[t/\beta] P]}
{\Gamma \prov [\exists \beta . P]}
\qquad
\erule
{\Gamma[[t/\beta] N] \prov Q}
{\Gamma[\forall \beta . N] \prov Q}
\]
\[
\erule
{\Gamma \prov [P]\qquad \Gamma \prov [P]}
{\Gamma \prov [P \x P]}
\qquad
\erule
{\Gamma \prov [P] \qquad \Gamma[N] \prov Q}
{\Gamma[P \to N] \prov Q}
\]
 there are some restrictions about
how you can get {\em out} of focus. We do get the identity rules
\[
\erule
{}
{\Gamma[a_-] \prov a^-}
\qquad
\erule
{}
{\Gamma, a^+ \prov [a_+]}
\]
but otherwise blurring requires monovariance
\[
\erule
{\Gamma, P \prov Q \qquad P\ \mathsf{mono}}
{\Gamma[\ups P] \prov Q}
\qquad
\erule
{\Gamma \prov N \qquad N\ \mathsf{mono}}
{\Gamma \prov [\dns N]}
\]
Actually maybe what happens is the quantifier synchronous rules
build up an explicit substitution, so we know what variables we
have around, and the $\mathsf{mono}$ restrictions require that the blurred
proposition is mono in {\em each} of those variables, but maybe not
the same for each variable? Or maybe we already have to choose the
variance at the moment we unbind the quantifier, so it spans across products?
Yeah, no, I think something more like the latter, since I know already that
I can't generally do
\[
\erule
{\int_\alpha F^\alpha_\alpha \to G^\alpha \x H_\alpha}
{\int_\alpha F^\alpha_\alpha \to \int^\alpha G^\alpha \x H_\alpha}
\]
right?

Hm, I'm not really sure. So far I haven't found a concrete counterexample
for $\exists R$. What's the analogous thing for $\forall L$?
I'm guessing it's
\[
\erule
{\int_\alpha (F_\alpha \x H^\alpha) \to G^\alpha_\alpha }
{\int_\alpha \left( \int_\beta F_\beta \x H^\beta \right) \to  G^\alpha_\alpha}
\]
which I can right away successfully refute, since
\[\begin{tikzcd}
  K_{d}^{e} = 0 \ar[r]\ar[d] & K_{e}^{e} = 1\ar[d]\\
  K_{d}^{d} = 1 \ar[r] & K_{e}^{d} = 1
\end{tikzcd}\]
can in fact be factored as
\[
\left(\begin{tikzcd}
  F_{d} = 0 \ar[r]\ar[d] & F_{e} = 1\ar[d]\\
  F_{d} = 0 \ar[r] & F_{e} = 1
\end{tikzcd}\right)
\x
\left(\begin{tikzcd}
  H^{e} = 0 \ar[r]\ar[d] & H^{e} = 0\ar[d]\\
  H^{d} = 1 \ar[r] & H^{d} = 1
\end{tikzcd}\right)\]
whoops, no that's or, not and. And even if I did disjoint-sum, that'd give me $0112$, not $0111$.

Ok, I don't have a counterexample yet, but let's go through the
reasoning and see if it suggests something. I have
\[M : {\int_\alpha (F_\alpha \x H^\alpha) \to G^\alpha_\alpha }\]
and I want to make
\[P : {\int_\alpha \left( \int_\beta F_\beta \x H^\beta \right) \to  G^\alpha_\alpha}\]
by
\[P_d = \lambda w . M_d (w_d) \]
What I know is that
\[  G_f \o M_d \o (\rid \x H^f) = G^f \o M_e \o (F_f \x \rid) \]
\[  (F_f \x \rid) w_d = (\rid \x H^f) w_e  \]
and what I need to check is
\[  G_f \o P_d = G^f \o P_e   \]
i.e.
\[  G_f(M_d(w_d)) = G^f(M_e(w_e))   \]
And I'm stuck.

What's something that could be special about $M$ that would get me unstuck?
Something about the way that it uses its second argument? I
want either of
$M_d(\pi_1 w_d , \pi_2 w_d)$ or $M_e(\pi_1 w_e , \pi_2 w_e)$
to be able to be rewritten somehow...

Suppose we had a $q : H^e$ such that $\pi_2 w_d = H^fq : H^d$
and $w_e = (F_f \pi_1 w_d ,  q)$.
Then we could reason that

\[G_f(M_d(w_d)) = G_f(M_d(\pi_1 w_d , \pi_2 w_d))\]
\[ = G_f(M_d(\pi_1 w_d , H^f q))\]
\[ = G^f(M_e(F_f \pi_1 w_d ,  q))\]
\[ = G^f(M_e(w_e))\]

But wait, we do have that $w_e : H^e$. Does it make sense to assume that
$\pi_2 w_d = H^f \pi_2 w_e$? Wait, step back,
\[  (F_f \x \rid) w_d = (\rid \x H^f) w_e  \]
does actually split as
\[  F_f \pi_1 w_d = \pi_1 w_e  \]
\[   \pi_2 w_d = H^f \pi_2 w_e  \]
Doesn't it?
Why can I not already reason
\[G_f(M_d(w_d)) = G_f(M_d(\pi_1 w_d , \pi_2 w_d))\]
\[ = G_f(M_d(\pi_1 w_d , H^f \pi_2 w_e))\]
\[ = G^f(M_e(F_f \pi_1 w_d ,  \pi_2 w_e))\]
\[ = G^f(M_e(\pi_1 w_e, \pi_2 w_e))\]
\[ = G^f(M_e(w_e))\]
Okay, maybe this works after all, and I can imagine justifying monovariance
requirements `at the leaves' of focusing trees!

\subsection{Making Sure The Dual Thing Works}

I'm a little nervous as I thought I'd gotten decisively stuck here before.
Let's try again.

Recall we're trying to validate
\[
\erule
{\int_\alpha F^\alpha_\alpha \to G^\alpha \x H_\alpha}
{\int_\alpha F^\alpha_\alpha \to \int^\alpha G^\alpha \x H_\alpha}
\]
So we have
\[ M : {\int_\alpha F^\alpha_\alpha \to G^\alpha \x H_\alpha} \]
and we build
\[ P : {\int_\alpha F^\alpha_\alpha \to \int^\alpha G^\alpha \x H_\alpha}\]
as
\[ P_d = \lambda x . \pair d {M_d(x)} \]
and we need to check
\[ P_d \o F^f = P_e \o F_f \]
i.e.
\[ (x : F^e_d) \to   \pair d {M_d(F^f(x))} = \pair e {M_e(F_f(x))} : \int_\alpha  G^\alpha \x H_\alpha\]
i.e.
\[ (x : F^e_d) \to   \pair d {M^1_d(F^f(x)), M^2_d(F^f(x))} = \pair e {M^1_e(F_f(x)), M^2_e(F_f(x))} : \int_\alpha  G^\alpha \x H_\alpha\]

and what we know is the coend equality, for any $f : d \to e$,
\[ (y : G^e \x H_d)\to   \pair d {G^f(y^1), y^2} = \pair e {y^1, H_f(y^2)} : \int^\alpha  G^\alpha \x H_\alpha\]
and the end property of $M$,
\[  M^1_d \o F^f = G^f \o M^1_e \o F_f \qquad  H_f \o M^2_d \o F^f =  M^2_e \o F_f \]
But wait; stare at our obligation, and apply the end-property of $M$. It becomes
\[    \pair d {G^fM^1_eF_fx, M^2_dF^fx} = \pair e {M^1_eF_fx, H_fM^2_dF^fx}\]
and this {\em is} exactly the coend equality for $y = \pair {M^1_e F_f x} {M^2_d F^f x}$.

Great, this holds, it was just clouded by the noise of notation.

\subsection{Trying to Generalize a Little}

Let's say a functor $F : \C^\op \x \C \to \rset$ is `universalizable' or just `u-able',
if we can go
\[
\erule
{\int_\alpha F^\alpha_\alpha \to G^\alpha_\alpha }
{\int_\alpha \left(\int_\beta F^\beta_\beta\right) \to  G^\alpha_\alpha }
\]
for any $G$. I conjecture that u-ability is preserved by products. Let $F$ and
$H$ be given, suppose both u-able,
say
\[\nabla_G F : \left(\int_\alpha F^\alpha_\alpha \to G^\alpha_\alpha \right)
\to \int_\alpha \left(\int_\beta F^\beta_\beta\right) \to  G^\alpha_\alpha\]
\[\nabla_G H : \left(\int_\alpha H^\alpha_\alpha \to G^\alpha_\alpha \right)
\to \int_\alpha \left(\int_\beta H^\beta_\beta\right) \to  G^\alpha_\alpha\]
I want to construct
\[\nabla_G (F \x H) : \left(\int_\alpha F^\alpha_\alpha \x H^\alpha_\alpha \to G^\alpha_\alpha \right)
\to \int_\alpha \left(\int_\beta F^\beta_\beta \x H^\beta_\beta\right) \to  G^\alpha_\alpha\]
So I observe
\[\nabla_{H \to G} F : \left(\int_\alpha F^\alpha_\alpha \to H^\alpha_\alpha \to G^\alpha_\alpha \right)
\to \int_\alpha \left(\int_\beta F^\beta_\beta\right) \to  H^\alpha_\alpha \to G^\alpha_\alpha\]
I can probably pull out the constant
\[\left(\int_\alpha F^\alpha_\alpha \to H^\alpha_\alpha \to G^\alpha_\alpha \right)
\to  \left(\int_\beta F^\beta_\beta\right) \to  \int_\alpha H^\alpha_\alpha \to G^\alpha_\alpha\]
I can compose with $\nabla_G H$ to get
\[\left(\int_\alpha F^\alpha_\alpha \to H^\alpha_\alpha \to G^\alpha_\alpha \right)
\to  \left(\int_\beta F^\beta_\beta\right) \to
\int_\alpha \left(\int_\beta H^\beta_\beta\right) \to  G^\alpha_\alpha\]
pull out the constant again to get
\[\left(\int_\alpha F^\alpha_\alpha \to H^\alpha_\alpha \to G^\alpha_\alpha \right)
\to  \left(\int_\beta F^\beta_\beta\right) \to
 \left(\int_\beta H^\beta_\beta\right) \to \int_\alpha G^\alpha_\alpha\]
So all I need now is just that
\[  \left(\int_\beta F^\beta_\beta \x H^\beta_\beta\right) \to \left(\int_\beta F^\beta_\beta\right) \x
 \left(\int_\beta H^\beta_\beta\right)  \]
which I'm pretty sure is true.

\subsection{Wait What}
But this is just showing that conjunction is {\em negative}, isn't it?
I should expect an {\em implication} in this position. But for that I need
to invent what I mean by existentializable/e-able. It's that I can do
either of the following two equivalent things:
\[\Delta_G F : \left(\int_\alpha G^\alpha_\alpha \to F^\alpha_\alpha\right) \to
\int_\alpha G^\alpha_\alpha \to \int^\alpha F^\alpha_\alpha \]

\[\Delta_G F : \left(\int_\alpha G^\alpha_\alpha \to F^\alpha_\alpha\right) \to
\left(\int^\alpha G^\alpha_\alpha\right) \to \int^\alpha F^\alpha_\alpha \]

Ok. Suppose $F$ is existentializable (i.e. positive) and $H$ is universalizable (i.e. negative).
We want to show that $F \to H$ is universalizable. I want the coends/ends to look more
visually distinct,
so I'm going to write them as $\exists/\forall$.We have
\[\Delta_G F : \forall (G \to F) \to \exists G \to \exists F \]
\[\nabla_G H : \forall(H \to G) \to \forall H \to \forall  G\]
and we want
\[\nabla_G(F \to H) : \forall((F \to H) \to G) \to\forall(F \to H) \to \forall G \]
Ok, it seems likely that the outer call is going to be to $\nabla_G H$.
We we have is $\forall((F \to H) \to G)$ and $\forall(F \to H)$
and what we need is $\forall(H \to G)$ and $\forall H$. This seems stuck.

What is universalizability/existentializability supposed to be telling me? That I'm allowed
to use $\forall L/\exists R$. So why is it ok to use the $\forall L$ rule at $P \to N$?

\[
\erule
{\Gamma, P[t] \to N[t] \prov Q}
{\Gamma, \forall x . P \to N \prov Q}
\]
I can't see anything.

\subsection{Products Preserve E-ability?}
Let's try to look at whether
existentializability as stated preserves products, and if so what that proof
looks like. It might be more closely analagous since I'm doing a multiplicative
connective instead of an additive.

We have
\[\Delta_G F : \forall (G \to F) \to \exists G \to \exists F \]
\[\Delta_G H : \forall (G \to H) \to \exists G \to \exists H \]
and we want
\[\Delta_G (F \x H) : \forall (G \to F \x H) \to \exists G \to \exists (F \x H) \]
This seems more straightforwardly about being able to do pair-introduction on coends,
and projecting components out of ends.
\subsection{What's hard about the implication case?}
Do I need to reason about the {\em implies left} rule, in the sense of
\[\forall F \to \forall (H \to G) \to \forall (( F \to H) \to G)\]
? No, because what I {\em have} is $\forall (( F \to H) \to G)$.

Can I ask for a stronger notion of universalizability, like
\[\nabla_{FG} H : \forall((F \to H) \to G) \to \forall (F \to H) \to \forall  G\]
? And then together with
\[\Delta_G F : \forall (G \to F) \to \exists G \to \exists F \]
we would want to prove
\[\nabla_{JG} H : \forall((J \to F \to H) \to G) \to \forall (J \to F \to H) \to \forall  G\]
Eh, this is probably too strong, it would let us prove this just by invoking
$\nabla_{(J \x F)G} H$ without even using $\Delta_G F$.

I suspect I need to prove some other little lemma about $\to$, though,
parallel to the little lemmas about $\x$ required for the other facts.

\subsection{Resummary}

So far I believe in

\[
\erule
{\forall (F^+ \x H^- \to G)}
{ \forall (F^+ \x H^-) \to \forall G}
\]
\[
\erule
{\forall (G \to F^+ \x H^-)}
{\exists G \to \exists (F^+ \x H^-)}
\]

Can I prove anything like
\[
\erule
{\forall ((F^+ \to H^-) \to G)}
{ \forall (F^+ \to H^-) \to \forall G}
\]
Or maybe with some other assignment of polarities to $F$ and $H$?

Say we have $M : \int_c ((F_c \to H^c) \to G^c_c)$ and I want to construct
$P : (\int_c F_c \to H^c) \to \int_c G^c_c$. I do this with
$P(w)_d = M_d(w_d)$. I want to check that the output $P$ is actually in the end,
so for any $f : d \to e$
\[G_f (P(w)_d) = G^f (P(w)_e) : G^d_e \]
i.e.
\[(w : \int_c F_c \to H^c) \to G_f (M_d(w_d)) = G^f (M_e(w_e)) : G^d_e \]
What I know is that $M$ and $w$ belong to their respective ends
\[G_f \o M_d \o (F \to H)^f = G^f \o M_e \o (F \to H)_f \]
\[ H_f \o w_d \o F^f  = H^f \o w_e \o F_f \]
and recall that $(F \to H)^f = \lambda z . H^f \o z \o F_f$
and $(F \to H)_f = \lambda z . H_f \o z \o F^f$ which in this case is just the identity,
so what we really know is
\[G_f \o M_d \o (F \to H)^f = G^f \o M_e  \]
\[  w_d   = H^f \o w_e \o F_f \]
So we reason
\[G_fM_dw_d = G_fM_d(H^f \o w_e \o F_f) = G^fM_ew_e\]
precisely as required.

This seems like it would work if I revesed $+$ and $-$ in $F$ and $H$.
Does it work with both $+$? Actually, let me just write out what I have for the most
general case (which oughtn't work) so I can at least copy and paste and reduce that.
So I'm studying
\[
\erule
{\forall ((F \to H) \to G)}
{ \forall (F \to H) \to \forall G}
\]
which comes out to still defining $P(w)_d = M_d(w_d)$ as before. I want
to check that the output $P$ is in the end, so the thing to verify is
\[(w : \int_c F^c_c \to H^c_c) \to G_f (M_d(w_d)) = G^f (M_e(w_e)) : G^d_e \]
What I know is
\[G_f \o M_d \o (F \to H)^f = G^f \o M_e \o (F \to H)_f \]
\[ (F \to H)_f (w_d)  = (F \to H)^f(w_e) \]

\subsubsection{What I proved already}

If $F^+$ and $H^-$, then $(F \to H)_f$ is the identity. I reason
\[G_fM_dw_d = G_fM_d(F \to H)^f w_e  = G^fM_ew_e\]
If $F^-$ and $H^+$, then $(F \to H)^f$ is the identity. I reason
\[G^fM_ew_e = G^fM_e(F \to H)_f w_d  = G_fM_dw_d\]
But really what's going on in both these cases is just that $F \to H$ itself
as a functor is already monovariant. That's not terrifically interesting.
What's much more interesting is if $F$ and $H$ are of the same variance.


A more convenient way of writing what I know in that case is
\[(z : F^d_e \to H^e_d) \to  G_f M_d  (H^f \o z \o F_f) = G^f  M_e  (H_f \o z\o F^f)\]
\[ H_f \o w_d\o F^f  =  H^f \o w_e \o F_f\]

\subsubsection{Both Positive}
What I know is
\[(z : F_e \to H_d) \to  G_f M_d  ( z \o F_f) = G^f  M_e  (H_f \o z)\]
\[ H_f \o w_d  =   w_e \o F_f\]
I don't seem to be able to do much with this, but the more I think about this, the more
expected it is. The situation that arises in the focusing semantics is not that
$H$ is an arbitrary bivariant functor, but one that matches up with $G$ very closely.
I need to figure out what conditions makes the `identity term' acceptable to make
progress on this front.

Let's just eta-expand some more things, though, to make sure I'm not missing something.
What I know is
\[(z : F_e \to H_d) \to  G_f M_d  ( \lambda (x : F_d) . z (F_f(x))) = G^f  M_e  (\lambda (x : F_e) . H_f ( z(x)))\]
\[(x : F_d) \to  H_f ( w_d(x))  =   w_e ( F_f(x))\]
and what I'm trying to prove is
\[ G_f M_d (\lambda (x : F_d) . w_d (x)) = G^f M_e (\lambda (y : F_e) . w_e (y))\]
This doesn't seem to reveal anything.
Critically what I seem to be unable to do is take $w_d$ or $w_e$ and turn either one of
them into the type $z$ needs to have.

However, a special property $M$ could have which would give me some
traction is that if, for any $f$, there was some $M_f : (F_d \to H_e) \to G^d_d $
 such that $M_d w_d = M_f (H_f \o w_d)$ then I could reason that
\[ G_f M_d w_d = G_f M_f (H_f \o w_d) \]
\[= G_f M_f (w_e \o F_f) \]
But there I'm stuck again. I need to get to $G_f M_d$ somehow.
Starting from the other end, if I make a dual assumption about the existence
of a $M^f : (F_d \to H_e) \to G_e^e$, that
$M_ew_e = M^f(w_e \o F_f)$, then
I'd have
\[ G^f M_e w_e = G^f M^f (w_e \o F_f)\]
And so a sufficient assumption --- supplanting, I suppose, the end condition previously assumed
for $M$ --- is that
\[G_f M_f = G^f M^f\]

Ok, a thing that's interesting is that the end condition {\em follows from} this condition. If
we have a $z : F_e \to H_d$, then
\[G_f M_d (z \o F_f)  = G_f M_f (H_f \o z \o F_f) = G^f M^f (H_f \o z \o F_f) = G^f M_e (H_f \o z)\]

\subsection{Let's Try Generalizing This a Bit}

Suppose we have an $M  : \forall((F \to H) \to G)$ but also we have for any $f$,
\[M_f : (F_d^e \to H_e^d) \to G^d_d \]
\[M^f : (F_d^e \to H_e^d) \to G^e_e \]
such that for any $q : F^d_d \to H^d_d$, we have
\[ M_f(H_f \o q \o F^f) = M_d q \]
and for any $q : F^e_e \to H^e_e$, we have
\[ M^f(H^f \o q \o F_f) = M_e q \]
and
\[G_f \o M_f = G^f \o M^f\]
Given all this junk, we aim to show that we can build
\[ \forall (F \to H) \to \forall G\]
We do this by setting $P (w)_d = M_d(w_d)$ as usual. We want to check that
the output is in the end, so we check
\[G_f P(w)_d = G^f P(w)_e\]
i.e.
\[G_f M_d w_d = G^f M_e w_e\]
but we also know that $w : \int_c F^c_c \to H^c_c$, so
\[ H_f \o w_d \o F^f = H^f \o w_e \o F_f : F^e_d \to H^d_e \]
which means we can reason
\[G_f M_d w_d = G_f M_f (H_f \o w_d \o F^f)\]
\[ = G^f M^f (H_f \o w_d \o F^f)\]
\[ = G^f M^f (H^f \o w_e \o F_f)\]
\[ = G^f M_e w_e\]

\subsection{ Can I do this for suitable `identity' proofs?}
Let's suppose $G = H$. Suppose specifically that $M : \forall (F \to H) \to H$
is of the form
\[M_d (q) = q (N_d) \]
for some $N_d : F_d^d$. Can I make
\[M_f : (F_d^e \to H_e^d) \to H^d_d \]
? No, I don't think so. Transport goes in the wrong direction. I could make
either of
\[M_f : (F_d^e \to H_e^d) \to H^d_e
\qquad M_f : (F_d^e \to H_d^d) \to H^d_d \]
Couldn't I? Wait, no, I still can't figure out how to get $F^e_d$. Transport's in the
wrong direction there, too.

But hold on a minute, expecting $F$ to be bivariant is not reasonable; That's the
position at which I expect things to be monovariant. I think maybe $H$ ought
to be possibly bivariant, though?

Ok, let's suppose $F$ is covariant. Then $N_d : F_d$. I might try making some of
\[M_f : (F_d \to H_e^d) \to H^d_e
\qquad M_f : (F_d \to H_d^d) \to H^d_d \]
\[M^f : (F_d \to H_e^d) \to H^d_e
\qquad M^f : (F_d \to H_e^e) \to H^e_e \]

This will require me to change conditions on what $M_f$ and $M^f$ must satisfy, though.
\subsection{Another Iteration of Guessing}

Suppose we have an $M  : \forall((F \to H) \to G)$ but also we have for any $f$,
\[M^f : (F^e_d \to H^d_e) \to G^d_e \]
such that for any $q : F^d_d \to H^d_d$, we have
\[ M^f(H_f \o q \o F^f) = G_f M_d q \]
and for any $q : F^e_e \to H^e_e$, we have
\[ M^f(H^f \o q \o F_f) = G^f M_e q \]
Given all this junk, we aim to show that we can build
\[ \forall (F \to H) \to \forall G\]
We do this by setting $P (w)_d = M_d(w_d)$ as usual. We want to check that
the output is in the end, so we check
\[G_f P(w)_d = G^f P(w)_e\]
i.e.
\[G_f M_d w_d = G^f M_e w_e\]
but we also know that $w : \int_c F^c_c \to H^c_c$, so
\[ H_f \o w_d \o F^f = H^f \o w_e \o F_f : F^e_d \to H^d_e \]
which means we can reason
\[G_f M_d w_d =  M^f (H_f \o w_d \o F^f)\]
\[ =  M^f (H_f \o w_d \o F^f)\]
\[ =  M^f (H^f \o w_e \o F_f)\]
\[ = G^f M_e w_e\]

\subsection{Another Iteration of `identity'}
\subsubsection{$F$ Covariant}
Let's suppose $G = H$, and $F$ is covariant, and $M : \forall (F \to H) \to H$
is given by
\[M_d (q) = q (N_d) \]
for some global section $N_d : F_d$ with $F_f N_d = N_e$.
We are able to define
\[M^f : (F_d \to H^d_e) \to H^d_e \qquad M^f(q) = q(N_d)\]
We must check
\[
 M^f(H_f \o q ) = H_f M_d q
\qquad
 M^f(H^f \o q \o F_f) = H^f M_e q
\]
i.e.
\[
 H_f  q  N_d = H_f  q N_d
\qquad
 H^f  q  F_f N_d = H^f  q N_e
\]
and these follow from the assumptions.
\subsubsection{$F$ Contravariant}
Let's suppose $G = H$, and $F$ is contravariant, and $M : \forall (F \to H) \to H$
is given by
\[M_d (q) = q (N_d) \]
for some global section $N_e : F_e$ with $F^f N_e = N_d$.
We are able to define
\[M^f : (F^e \to H^d_e) \to H^d_e \qquad M^f(q) = q(N_e)\]
We must check
\[
 M^f(H_f \o q \o F^f) = H_f M_d q
\qquad
 M^f(H^f \o q ) = H^f M_e q
\]
i.e.
\[
 H_f  q  F^f N_e = H_f  q N_d
\qquad
 H^f  q N_e = H^f  qN_e
\]
and these follow from the assumptions.

\subsection{A Generalization of the last Guess}

Suppose we have an $M : \forall (F \to G)$ and also for any $f$ we have
\[M^f : F^d_e \to G^d_e\]
such that for any $x : F^d_d$, we have
\[ M^f F_f  x  = G_f M_d x \]
and for any $y : F^e_e $, we have
\[ M^fF^f y  = G^f M_e y \]
In diagrams, this looks like
\[
\begin{tikzcd}
  F^d_d \ar[r, "M_d"] \ar[d, "F_f"'] & G^d_d \ar[d, "G_f"]\\
  F^d_e \ar[r, "M^f"']  & G^d_e\\
  F^e_e \ar[r, "M_e"'] \ar[u, "F^f"] & G^e_e \ar[u, "G^f"']
\end{tikzcd}
\]
Given all this junk, we aim to show that we can build
\[ P : \forall F \to \forall G\]
We do this by setting $P (x)_d = M_d(x_d)$. We want to check that
the output is in the end, so we check
\[G_f P(x)_d = G^f P(x)_e\]
i.e.
\[G_f M_d x_d = G^f M_e x_e\]
but we also know that $x : \int_c F^c_c$, so
\[ F_f  x_d = F^f  x_e  : F^d_e \]
which means we can reason
\[G_f M_d x_d =  M^f F_f  x_d \]
\[ =  M^f F_f  x_d\]
\[ =  M^f F^f  x_e\]
\[ = G^f M_e x_e\]
This whole business is just chasing the diagram
\[
\begin{tikzcd}
&  F^d_d \ar[r, "M_d"] \ar[d, "F_f"'] & G^d_d \ar[d, "G_f"]\\
\int_c F^c_c \ar[ur, bend left, "x_d"]\ar[dr, bend right, "x_e"']&  F^d_e \ar[r, "M^f"']  & G^d_e\\
&  F^e_e \ar[r, "M_e"'] \ar[u, "F^f"] & G^e_e \ar[u, "G^f"']
\end{tikzcd}
\]

\subsection{So what is this data?}
Claim one: To have $M_d$ and $M^f$ for some type $\forall(F \to G)$ is stronger than having
an end in $\forall(F \to G)$. For I would just need to check that
\[G_f \o M_d \o F^f = G^f \o M_e \o F_f\]
but this is the same as
\[M^f \o F_f \o F^f = M^f \o F^f \o F_f\]
and that follows from functoriality of $F$. This is chasing the diagram
\[
\begin{tikzcd}
&  F^d_d \ar[r, "M_d"] \ar[d, "F_f"'] & G^d_d \ar[d, "G_f"]\\
F^e_d \ar[ur, bend left, "F^f"]\ar[dr, bend right, "F_f"']&  F^d_e \ar[r, "M^f"']  & G^d_e\\
&  F^e_e \ar[r, "M_e"'] \ar[u, "F^f"] & G^e_e \ar[u, "G^f"']
\end{tikzcd}
\]

Claim two: this is not as strong as having a natural transformation
$F \to G : \C\x\C^\op \to \rset$. What would naturality mean for that?
For any $f : d \to e$ and $f' : e' \to d'$, we'd need
\[
\begin{tikzcd}
  F^{d'}_{d}
 \ar[r, "F^{f'}_f"]
\ar[d, "M^{d'}_{d}"']
& F^{e'}_e
\ar[d, "M^{e'}_{d}"]
\\
  G^{d'}_{d} \ar[r, "G^{f'}_f"'] & G^{e'}_e
\end{tikzcd}
\]
This just seems like a very different requirement. The requirement I just made up with $M^f$
would not require any `components of the natural transformation between two objects' if
there are no arrows between them, but the actual natural transformation would.

Is it the case that {\em if} I have a natural transformation, then I have one of
these super-ends? I suppose so, since I can define $M_d$ as $M^{d}_d$, and $M^f$
as $M^{\dom f}_{\cod f}$.

Ok, so all signs are pointing to this being an intermediate
requirement between end and natural transformation so far.
Of course, when $F$ is contravariant and $G$ is covariant, that distinction collapses,
but this definition is meaningful even when $F$ and $G$ are bivariant.

\subsection{Do Super-ends Compose?}
I think they do.
\[
\begin{tikzcd}
  F^d_d \ar[r, "M_d"] \ar[d, "F_f"'] & G^d_d \ar[r, "N_d"]\ar[d, "G_f"]& H^d_d \ar[d, "H_f"]\\
  F^d_e \ar[r, "M^f"']  & G^d_e\ar[r, "N^f"']& H^d_e\\
  F^e_e \ar[r, "M_e"'] \ar[u, "F^f"] & G^e_e \ar[r, "N_e"']\ar[u, "G^f"']& H^e_e \ar[u, "H^f"']
\end{tikzcd}
\]

\subsection{Do Super-Ends satisfy $\exists R$ Also?}
Suppose I have $M : \forall(F \to G)$ and $M^f : F^d_e \to G^d_e$ as usual.
I want to make $P : \forall( F \to \exists G)$.
So I set $P_d(x) = \pair d {M_d(x)}$. I need to check that
\[P_d \o F^f = P_e \o F_f : F^e_d \to \int^c G^c_c \]
i.e.
\[(x : F^e_d) \to \pair d {M_dF^f x}  = \pair e {M_e F_f x} :  \int^c G^c_c \]
Hmmm, maybe I need a dual notion of super-end here, like
\[
\begin{tikzcd}
  F^d_d \ar[r, "M_d"] \ar[from=d, "F^f"] & G^d_d \ar[from=d, "G^f"']\\
  F^e_d \ar[r, "M^f"']  & G^e_d\\
  F^e_e \ar[r, "M_e"'] \ar[from=u, "F_f"'] & G^e_e \ar[from=u, "G_f"]\\
\end{tikzcd}
\]
So that I can reason
\[\pair d {M_dF^f x} = \pair d {G^f M^f x}\]
\[ = \pair e {G_f M^f x}\]
\[= \pair e {M_eF_f x}\]
And for sure the identity function $F^e_d \to F^e_d$ works!
\end{document}
