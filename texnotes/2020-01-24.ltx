\documentclass{article}
\usepackage{amssymb}
\input{theorem}

\input{prooftree}
\def\erule#1#2{\begin{prooftree}#1\justifies #2\end{prooftree}}
\def\pair#1#2{\langle #1 , #2 \rangle}

\usepackage{tikz}
\usepackage{tikz-cd}
\usetikzlibrary{calc}
\definecolor{morange}{rgb}{1,0.56,0}
\definecolor{lorange}{rgb}{1,0.95,0.8}
\definecolor{mgreen}{rgb}{0,0.56,0}
\definecolor{lgreen}{rgb}{0.95,1,0.8}
\definecolor{mblue2}{rgb}{0,0.2,1.0}
\definecolor{lblue}{rgb}{0.8,0.95,1}
\definecolor{mred}{rgb}{0.9,0.1,0.1}
\definecolor{mgreen}{rgb}{0.1,0.5,0.1}
\definecolor{mblue}{rgb}{0.3,0.3,0.9}
\def\bitf#1{#1 [smooth, tension=0.8] coordinates {(-1.6,2) (-1,1) (0,0)}}
\def\bitg#1{#1 [smooth, tension=0.8] coordinates {(1,2) (0.55,1) (0,0)}}
\def\bitgrev#1{#1 [smooth, tension=0.8] coordinates {(0,0) (0.55,1) (1,2)}}
\def\bitfg#1{#1 [smooth, tension=0.8] coordinates {(0,0) (0,-2) }}
\def\ep{\varepsilon}
\def\bcase{\mathop\mathbf{case}}
\def\bof{\mathop\mathbf{of}}
\def\binj{\mathbf{inj}}
\def\blet{\mathrel\mathbf{let}}
\def\bin{\mathrel\mathbf{in}}
\def\bmatch{\mathrel\mathbf{match}}
\def\bwith{\mathrel\mathbf{with}}
\def\pbck{\ar[dr, phantom, pos=0, "\lrcorner"]}
\def\ups{{\uparrow}}
\def\dns{{\downarrow}}
\def\grad{\nabla}
\def\uups{{\Uparrow}}
\def\ddns{{\Downarrow}}
\def\adjust{\big|}
\def\O{\mathcal{O}}
\def\rid{\mathsf{id}}
\def\ridp{\mathsf{idp}}
\def\rcoe{\mathsf{coe}}
\def\rtype{\mathsf{type}}
\def\int{\square}
\def\bd{\partial}
\def\prov{\vdash}
\def\prequiv{\dashv\vdash}
\def\imp{\Rightarrow}
\def\cqed{\hskip2mm{\vrule width .5em height .5em depth 0em}} % at the end of a |P.
\def\o{\circ}
\def\lx{\bigcirc}
\def\B{\mathbb{B}}
\def\C{\mathbf{C}}
\def\R{\mathbb{R}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bv{\mathbf{v}}
\def\bt{\mathbf{t}}
\def\bp{\mathbf{p}}
\def\bz{\mathbf{0}}
\def\S{\mathfrak{S}}
\def\M{\mathbb{M}}
\def\X{\mathbf{X}}
\def\Y{\mathcal{Y}}
\def\x{\times}
\def\st{\mathrel|}
\def\rset{\mathbf{Set}}
\def\rcat{\mathbf{Cat}}
\def\op{\mathsf{op}}
\def\P{\mathbb{P}}
\def\I{\mathbb{I}}
\def\U{\mathbb{U}}
\def\N{\mathbb{N}}
\def\Z{\mathbb{Z}}
\def\tw{\mathbf{2}}
\def\dash{\hbox{---}}
\def\dom{\mathop{\mathrm{dom}}}
\def\cod{\mathop{\mathrm{cod}}}
\def\celse{\mathrel{|}}
\def\cn{{:}}
\def\rok{\mathrel\mathsf{ok}}
\def\llam#1{\langle {#1} \rangle}
\def\hf{{\odot}}
\def\lerp#1#2#3{#1[\![#2,#3]\!]}
\def\nope{\emptyset}

\begin{document}
\tikzset{>=stealth}
\tikzset{
   commutative diagrams/.cd,
   arrow style=tikz,
   diagrams={>=stealth}}

\section{Main}
\subsection{Criticality}
I want to see how far I can get proving some lemmas relating the two
approaches to criticality that I think should coincide.

Let $X,Y$ be sets of variables. Suppose $g : \R^{X + Y} \to \R$ and $f : \R^X \to \R$.
We say $g$ {\em extends} $f$, written $g \succ f$, if $f = g[0/Y]$.
If $\bx \in \R^X$ then we write $\bx^{+}$ for the evident point of $\R^{X + Y}$ that assigns 0 to every $y\in Y$.


Let $P$ be some predicate on points in the domain of $g: \R^{X+ Y} \to \R$.
We say say of some point $\bx$ in the domain of $g$ that it {\em essentially has the property $P$} if
there exists a neighborhood $U_x \x U_y \subseteq \R^X \x \R^Y$ of $\bx$ such that
for any $\by \in U_y$, there exists exactly one $\bx \in U_x$ such that $(\bx, \by) : \R^{X+Y}$
has the property $P$.

Say a point is an $n$-cell if it's $n$-critical but $(n+1)$-stable.

\begin{conjecture} Let $X$ be given and set $i = |X|$.
Suppose we have $f: \R^X \to \R$ and an $i$-critical point $\bx$ of $f$.
Then the following
conditions are equivalent:
\begin{enumerate}
\item $\bx$ is an $i$-cell.
\item for all $g \succ f$, the point $\bx^+$ is essentially $i$-critical in $g$.
\end{enumerate}
\end{conjecture}

This conjecture makes sense because for the single-copy-of-$\R$
codomain I'm moderately confident I know what the definition of
`$n$-stable' is: it's $D^n$ being nonzero. I don't yet know what the definition is
for multiple copies of $\R$ in the codomain, but I could call this a constraint on
any putative definition of stability for that situation.

\begin{conjecture}[Constraint] Let $X$ be given and set $i = |X|$.
Suppose we have $f: \R^X \to \R^C$ and an $i$-critical point $\bx$ of $f$.
Then the following
conditions are equivalent:
\begin{enumerate}
\item $\bx$ is an $i$-cell.
\item for all $g \succ f$, the point $\bx^+$ is essentially $i$-critical in $g$.
\end{enumerate}
\end{conjecture}

Let's just warm up by proving the above {\em conjecture} holds in the
way I expect it to for small values of $i$. Here's how the $i=1$ case goes.
First a very general lemma I probably should have already proven:
\begin{lemma}
\label{cell}
Around any point in the domain of a smooth $f: \R^n \to \R$ we can find a neighborhood which is an $n$-cell cube.
(By which I mean the usual set of stability requirements on its faces)
\end{lemma}

\begin{proof}
If the point $\bx$ has $f(\bx) \ne 0$, then by continuity we can find a 0-cell neighborhood throughout which $f \ne 0$.
If we have $f = 0$ but $f_x \ne 0$, then wlog $f_x > 0$, and we can find an interval $[x_-, x_+]$ such that
$f[x_-/x]\bx < 0$  and $f[x_+/x]\bx> 0$ and then by continuity, we can find a cube small
enough such that those are true of the whole face. Probably I can inductively continue this kind of reasoning.
\cqed
\end{proof}


\begin{lemma}
\label{equivalence.1d}
Suppose we have $f(x): \R \to \R$ and an $x$ where $f(x) = 0$.

Then the following conditions are equivalent:
\begin{enumerate}
\item $f_x(x) \ne 0$.
\item for all $g : \R \x \R^Y \to \R$ such that $g(x,\bz) = f(x)$, the point $(x, \by)$ is essentially 1-critical in $g$.
\end{enumerate}
\end{lemma}

\begin{proof}
$\imp$: suppose $f_x(x) \ne 0$. We must show that there is some neighborhood
around $(x,\bz)$ within which we can
  say that for all $\by \in  \R^Y$,
 there exists a unique $x_0$ such that $g(x_0,\by) = 0$.
%
We first of all observe that $g_x(x,\bz) = f_x(x) \ne 0$. The
  derivative must exist since we assume $g$ to be smooth, and it can't
  have any other value than that of $f_x$, since it's constrained to
  agree with $f_x$ when $\by = 0$.

Also since $g$ is smooth, its $x$-derivative is continuous, so we can
find a neighborhood $U = U_x \x U_y \subseteq \R \x \R^Y$ containing $(x, \bz)$, such that $g_x \ne 0$ throughout $U$.
Assume wlog that $g_x > 0$.
Furthermore find a neighborhood within that that is a $n$-cell cube $[x_0,x_1] \x U_y$.

This will be the neighborhood we exhibit for the purpose of showing
that $(x,\bz)$ is essentially 1-critical. So let some $\by \in U_y$ be given.
We know along the line from $(x_0, \bz)$ to $(x_1, \bz)$ that $g_x > 0$, and
$g_x = 0$ at $(x,\bz)$. So $g(x_0, \bz) < 0$ and $g(x_1, \bz) > 0$. Because $[x_0,x_1] \x U_y$ is an $n$-cell cube,
we know that fixing $x = x_0$ or $x = x_1$ gives us a 1-stable face, so $g(x_0, \by) < 0$ and $g(x_1, \by) > 0$
for all $\by \in U_y$. Therefore by intermediate value theorem, there {\em exists} an $x$ yielding a zero at $(x, \by)$
for every $\by$. If there were more than one such value, this would contradict $g_x > 0$.

$\Leftarrow$: suppose we have a $x_c$ such that $f(x_c) = 0$ and $f_x(x_c) = 0$.
Define $g(x,y) : \R^2 \to \R = f(x) + (x - x_c) y$. Plainly
this reduces to $f(x)$ when $y = 0$. We must show that $(x_c,0)$ is not essentially 1-critical.
So let some neighborhood $[x_0, x_1] \x [y_0, y_1] \ni (x_c, 0)$ be given.

We want to show that for very small $\Delta y$, we have at least two candidates for $x$ that
will make $g$ zero. One is setting $x = x_c$. For the other, observe first that we can't have $f$ being the zero function,
for then there would be lots of solutions at $y = 0$. Otherwise $f$, assuming it's a polynomial or analytic or something,
is wlog of the form $f(x) = (x-x_c) f_0(x)$. When $y$ is small enough, we can try setting $x = x_c+h$
where $h = f_0^{-1}(-y) - x_c$.
In this case we can compute
\[g(x,y) = g(x_c+h,y)\]
\[ = f(x_c+h) + ((x_c + h) - x_c)y\]
\[ = f(x_c+h) + hy\]
\[ = hf_0(x_c+h) + hy\]
Observe that $f_0(x_c + h) = -y$ and we are done.\cqed
\end{proof}

\subsection{Higher-Level Considerations}
\subsubsection{Judgmental Thoughts}
I expect/intend that any predicates I want to define on functions
defined on cubes should be completely isovariant or equivalent or
indifferent or whatever with respect to equivalence as defined by the
existence of higher-dimensional cells traversing between the two cells
in question. So these should be equalities in the HoTT sense --- every
predicate, which is to say every type, should admit transport across
them.

I might view a 3-cell, for example, as an expression
\[x,y,z \prov e : \R \]
How do I think about stability? The context of variables which $e$ is stable with respect to is ordered,
because the boundary conditions care about which dimension's boundary I take. So maybe I should have two contexts?
The judgment is something like
\[\Omega; \Gamma \prov e \]
to mean $e$ actually is a $|\Omega|$-cell, that it's $|\Omega|+1$-stable everywhere. Let's think about
how substitutions of constants for variables works.
If I have
\[x, \Omega; \cdot \prov e\]
then I should get
\[; \Omega \prov [b/x]e\]
for any $b\in \tw$, because the $x \in \{0,1\}$ boundary should be 1-stable. If I have
\[x,y,z; \cdot \prov e\]
then
\[x;z  \prov [b/y]e\]
because the $y\in \{0,1\}$ boundary should be 2-stable --- we should
be able to know that the derivative {\em with respect to $x$}, which
is still in $\Omega$, is not zero.
So I think the general pattern is like
\[
\erule
{\Omega, v, \Omega'; \Gamma \prov e}
{\Omega; \Omega', \Gamma \prov [b /v]e}
\]
Let's test this out on what I think of as an equivalence cell between two 2-cells. It would be
\[x,y; z \prov e \]
which is an equivalence
\[x,y ; \cdot \prov [0/z]e \equiv [1/z]e \]
Everything in sight is 3-stable, since there are only two items in the $\Omega$ context.
So I think everything in the $\Gamma$ context represents equivalences.

What is the substitution principle that arises from the ability to reparameterize earlier
variables dependent on later ones? Substitution judgment like
\[ \Omega' \prov \theta : \Omega\]
and the substitution principle like

\[
\erule
{ \Omega'[;\Gamma] \prov \theta : \Omega \qquad \Omega ; \Gamma \prov e }
{\Omega'; \Gamma \prov \theta e}
\]
Not sure whether I get the $\Gamma$ in the substitution principle yet.

\[
\erule
{x, \Omega' \prov e \qquad \Omega' \prov \theta : \Omega}
{x, \Omega' \prov (\theta, [e/x]) : x, \Omega}
\]
Hm, this seems ugly. Can I do any better with single substitutions? Something like
\[
\erule
{\Omega' \prov e' \qquad \Omega,v, \Omega' \prov e}
{\Omega,  \Omega' \prov [e'/v]e}
\]
Do I have weakening in $\Omega$ contexts? That would give this an appropriate amount of expressivity, as
then
\[
\erule
{w, \Omega' \prov e' \qquad \Omega,v, \Omega' \prov e}
{\Omega,  w, \Omega' \prov [e'/v]e}
\]
would be admissible, because $\Omega,v,\Omega' \prov e$ would imply $\Omega, v,w,\Omega'\prov e$.

Let's think about $x,y \prov e$ in that case. We know that $[b/x]e$ is
1-stable, and $[b/y]e$ is 2-stable, and $e$ itself is 3-stable.
Weakening would tell us that $x,v,y \prov e$. We need to show that
$[b/x]e$ is 1-stable, $[b/v]e$ is 2-stable, and $[b/y]$ is 3-stable,
and $e$ itself is 4-stable. The problematic case is we don't actually
know that $[b/v]e$ --- which is just $e$ --- is 2-stable. So no, I don't think weakening in $\Omega$ works.

Probably the substitution principle of bits for single variables should be embodiable as a real first-class substitution,
so that in some sense
\[ \Omega \prov [b/v] : \Omega, v, \Omega' \]
But what happened to the $\Gamma$ stuff? Maybe I really should be thinking of $\Gamma$ as stronger than $\Omega$,
and writing $\Gamma;\Omega$ rather than $\Omega;\Gamma$,
and having instead
\[ \Gamma, \Omega'; \Omega \prov \rid_\Omega[b/v]\rid_{\Omega'} : \Omega, v, \Omega' \]
and
\[ \Gamma, \Gamma' \prov \mathsf {w} : \Gamma' \]
because weakening for $\Gamma$-style contexts {\em should} be admissible?

\subsubsection{Composition}
Refer back to \texttt{2019-12-04} page 3 for some similar thoughts.

If I have $\Gamma; \Omega,v, \Omega' \prov e_1$ and $\Gamma ; \Omega, v, \Omega' \prov e_2$,
then I have $\Gamma;\Omega,v,\Omega'\prov e_1 *_{|\Omega|} e_2$.

I should have definitional equalities like
\[[\dns v / v](e_1 *_{|\Omega|} e_2) \equiv e_1\]
\[[\ups v / v](e_1 *_{|\Omega|} e_2) \equiv e_2\]
and I guess more generally this would be like
\[[\dns e / v](e_1 *_{|\Omega|} e_2) \equiv [e/v]e_1\]
\[[\ups e / v](e_1 *_{|\Omega|} e_2) \equiv [e/v]e_2\]
Maybe I should think of composition as a case statement
\[
\erule
{\Gamma; \Omega,v, \Omega' \prov e_1 \qquad \Gamma ; \Omega, v, \Omega' \prov e_2}
{\Gamma; \Omega,w,\Omega' \prov \bcase w \bof \dns v \imp e_1 \celse \ups v \imp e_2}
\]
I expect an $\eta$-principle like
\[e = ([\dns v / v]e) *_{|\Omega|} ([\ups v / v]e)\]

\subsubsection{Associativity}

How would I express associativity? Let's just do the 1-d case at first.
I have $x \prov f,g,h$, and I can form $x \prov (f * g) * h$ and
$x \prov f * (g * h)$. The equality cell wants to be an expression
$y ; x \prov e$ such that $x \prov [1/y]e \equiv (f * g) * h$ and
$x \prov [0/y]e \equiv f * (g * h)$. I think it's something like a substitution
of an interpolation:
\[\iota(x,y) = y[x, ((\dns * \ups\dns) * \ups \ups)x]\]
\[ [\iota(x,y) / x]f * (g * h) \]
because I should get some kind of principle that
\[ [v_1 * v_2 / x]e \equiv [v_1/x]e * [v_2/x]e \]
so that when $y = 0$, of course $[x/x]$ does nothing and we get back  $f * (g * h)$, as required.
And when $y = 1$, we have
\[[((\dns * \ups\dns) * \ups \ups)x/x](f * (g * h))\]
\[ = [(\dns * \ups\dns)x/x](f * (g * h)) * [\ups \ups x/x](f * (g * h))\]
\[ = ([\dns x/x] (f * (g * h)) * [\ups\dns x/x] (f * (g * h))) * h\]
\[ = (f * g) * h\]

\subsection{Revisiting The Proof of Lemma~\ref{equivalence.1d}}

I felt like I was a little bit sloppy reasoning about local inverses
in Lemma~\ref{equivalence.1d}. Suppose we have an $x_c$ such that $f(x) = 0$ and $f_x(x_c) = 0$.
Set $g(x,y) : \R^2 \to \R = f(x_c) + (x - x_c) y$. We want to show that $(x_c,0)$ is not essentially
1-critical. So let some neighborhood $[x_0, x_1] \x [y_0, y_1] \ni (x_c, 0)$ be given.
We want to show that there exists a $\Delta y$ such that there exists multiple $x$ such that
$g(x,\Delta y) = 0$. One will always be $x = x_c$. The other nearby zero I expect to
exist for some small $\Delta x$ is $g(x_c + \Delta x, -f(x_c + \Delta x)/\Delta x)$ because
\[g(x_c + \Delta x, f(x_c + \Delta x)/\Delta x) \]
\[= f(x_c + \Delta x) - \Delta x f(x_c + \Delta x) / \Delta x\]
\[ = 0\]
But I want to solve for $\Delta x$ given $\Delta y$, not the other way around.
But I see that $\Delta y \approx f_x(x_c)$ but that's known to be zero. So should I go
out to 2nd order? $\Delta y \approx f_{xx}(x_c) \Delta x$? then $\Delta x = \Delta y / f_{xx}$?
That'll fail if $f_{xx} = 0$, though.

Ah, I see that things are complicated by the fact that I {\em can't} always pick $\Delta y$
to be small positive or always negative. Depending on the structure of the function $f(x)$
I might be forced to pick one or the other.

But wait, I'm the one picking $\Delta y$! I should be ok with picking $\Delta x$ first, then figuring
out what $\Delta y$ must be. So I want $f(x - x_c) / (x - x_c)$ to exist and be continuous around $x_c$
by virtue of $f(x_c) = 0$. If $f$ were a polynomial, I would know this. This seems like a basic analysis fact.

\begin{lemma}
Suppose $f$ is a smooth function $\R \to \R$. If $f(0) = 0$,
then there exists a smooth function $g : \R \to \R$ such that $xg = f$.
\end{lemma}

\begin{proof}
We set $g(x) = f(x) / x$ and set $g(0) = f'(0)$. $\cdots$
\end{proof}

Oh yeah, I see this in \texttt{https://math.stackexchange.com/questions/541046/quotient-of-two-smooth-functions-is-smooth}
and \texttt{https://math.stackexchange.com/questions/524276/zeros-of-c-infty-functions}. Proved with Taylor series.

Ok, so this means that $f^\div = f(x) / (x - x_c)$ is a legitimate smooth function, and I know $f^\div(x_c) = 0$
because $f_x(x_c) = 0$. So by continuity we can assume without loss that we've already chosen a subinterval
of $[x_0,x_1]$ such that $-f^\div$ never escapes $[y_0,y_1]$. Pick any $x_0 \ne x_c$ in that interval at all, and
let $y_0 = -f^\div(x_0) = -f(x_0) / (x_0 - x_c)$, and then we have
\[g(x_0, y_0) = f(x_0) + (x_0-x_c)(-f(x_0) / (x_0 - x_c)) = 0\]

\subsection{Criticality Correspondence Conjecture at Higher Dimensions}
I think I mostly actually see the shape of the proof of the $\imp$ direction of the main conjecture,
for the general $n$-dimensional case. The key lemma is a slight strengthening of the inverse function theorem:
\begin{lemma}[Inversion]
Suppose we have $f : \R^X \to \R^X$ and a point $\bx \in \R^X$ such that $f(\bx) = \bz$.
Suppose
\[ \det_{i,j \in X} {\bd f^i\over\bd x_j} \ne 0\]
Let $g : \R^X \x \R^T \to \R^X$ by given such that $g(\bx, \bz) = f(\bx)$.
Then the point $(\bx, \bz)$ is essentially a root of $g$.

That is, there exists a neighborhood
$U_x \x U_t \subseteq \R^X \subseteq \R^T$ of $(\bx, \bz)$ such that for all $\bt \in U_t$, there
exists a unique $\bx \in U_x$ such that $g(\bx, \bt) = \bz$.
\end{lemma}

\begin{proof}
All of the partial derivatives of components of $f$ are continuous, so find a neighborhood
$U = U_x \x U_t \subseteq \R^X \subseteq \R^T$ of $(\bx, \bz)$ where
\[ \det_{i,j \in X} {\bd f^i\over\bd x_j} \ne 0\]
throughout $U$.

Then for a particular $\bt$, we have that $g_\bt(\dash) := g(\dash, \bt) : \R^X \to \R^X$ is a smooth
function with nonzero Jacobian determinant at $(\bx, \bt)$, so it locally has an inverse, call it $g^{-1}_\bt$.
So the unique $\bx$ we're after is $g^{-1}_\bt(\bz)$. The problem is that the inverse function theorem
as normally stated gives us some arbitrary neighborhood around $\bz$ and around $\bx$, but only get back one
such neighborhood individually for each $\bt$, and naturally we can't take the intersection of arbitrarily
many open sets.
However I suspect the local compactness of $\R^n$ lets us find a single neighborhood that works for all $\bt$ uniformly.
\cqed
\end{proof}

\begin{lemma}[Main Conjecture, $\imp$]
\label{main.forward}
Suppose we have $f: \R^X \to \R$ and an $i$-critical point $\bx$ of $f$, for  $i = |X|$.
If $\bx$ is an $i$-cell, then for all $g \succ f$, the point $\bx^+$ is essentially $i$-critical in $g$.
\end{lemma}

\begin{proof}
We know
\begin{enumerate}
\item $D^j f (\bx) = 0$ for all $j \in [i]$
\item $D^{i+1}f(\bx) \ne 0$
\end{enumerate}
The function $\R^X \to \R^X$ we want to plug into
the inversion lemma is the tuple $ (D^1 f, D^2 f, \ldots, D^i f)$.
The premises of the inversion lemma are then precisely assumptions 1 and 2 above.
The conclusion of the inversion lemma is precisely the conclusion we need, for
being a root of the tuple of functions above is the same as being $i$-critical.
\cqed
\end{proof}

I'm going to try to warm up to the $n$-dimensional case of $\Leftarrow$ for the main conjecture by attempting this:
\begin{lemma}
Suppose we have $f: \R^n \to \R^n$ such that $f(\bz) = \bz$ and
\[|J f(\bz)| = \det_{i,j \in [n]}{\bd f^i \over \bd x_j}(\bz) = \bz\]
Then there exists some
\[\hat f(x_1,\ldots, x_n,\hat x_1, \ldots, \hat x_n) : \R^n \x \R^n \to \R^n\] which extends $f$,
(i.e.  $\hat f(\bx,\bz) = f(\bx)$)
such that the point $(\bz, \bz)$ is ``not essentially critical'' in $\hat f$.

By this I mean that for any small neighborhood around $(\bz,\bz)$, there exists a $\hat\bx_0$ in it, and
$\bx_0 \ne \bx_1$ such that $\hat f(\bx_0,\hat\bx_0) = 0$ and $\hat f(\bx_1,\hat\bx_0) = 0$.
\end{lemma}

\begin{proof}
I set for each $i\in [n]$
\[\hat f^i(\bx, \hat \bx) = f^i(\bx) + \sum_{j \in [n]}\hat x^j {\bd f^i\over \bd x_j}(\bx)  \]
\[ = f^i(\bx) + (J f(\bx))^i_j \hat \bx^j\]

I'm going to let $\bx_0$ just be $\bz$. Since the Jacobian at $\bz$ is singular... no, that's not enough.
\end{proof}

\subsection{How Products Don't Work}
I at least have some insight now as to why taking a product of the multiple components of
a multi-output function doesn't behave quite the same with respect to Jacobian determinant reasoning.

For consider $f(x,y) = x^2 - y^2$. Its roots form a nice $\x$ shape. At the origin, we have $f = 0$
and $f_x = 0$ and $f_y = 0$, so it's 3-critical. This is in contrast to the function $g : \R^2 \to \R^2$
that we might imagine which is $g(x,y) = (x - y, x + y)$, even though we note that $f = g_1 g_2$.
For the origin at $g$ surely ought to be 2-critical, because it's at the intersection
of a 1-critical point of $g_1$, and a 1-critical point of $g_2$, but there seems to be no reason
to think that it should be 3-critical. At the origin we can compute the determinant
\[
\left|
\begin{array}{cc}
  g_{1;x} &   g_{1;y}\cr
  g_{2;x} &   g_{2;y}
\end{array}
\right|
=
\left|
\begin{array}{cc}
  1 &   -1\cr
  1 &   1
\end{array}
\right| = 2\ne0
\]
Now it is the case that $f$ extends to a higher-dimensional function which has multiple 2-critical points
very near the origin, namely $\hat f(x,y,z) = x^2 - y^2 + z$,
and for small positive $\Delta z$ we can set $x = 0$ and $y = \pm\sqrt{\Delta z}$,
and observe that $(0,\pm\sqrt{\Delta z}, \Delta z)$ is 2-critical:
\[f(0,\pm\sqrt{\Delta z}, \Delta z) = 0 = f_x(0,\pm\sqrt{\Delta z}, \Delta z)\]
But there's no way that I can see (and indeed a generalization of
Lemma~\ref{main.forward} ought to show that this {\em can't} happen)
to think of $\hat f(x,y,z)$ as a product of two functions that
coincide with $g_1$ and $g_2$ when $z$ happens to be zero.

So the criticality conditions for a function $f : \R^n \to \R^p$ can't be merely the criticality
conditions for the function $\R^n \to \R$ that is $\prod_{i\in [p]} f_i$.

\subsection{Paging Back in the Implicit Function Theorem}

Suppose I have a function $f : \R^n \to \R$, and $f(\bx_c) = 0$. I have
some $\bp$ such that $\grad f(\bx) \cdot \bp = 0$. Assume $\grad f \ne 0$ if necessary.
I want to estimate, near the line $\{\bx +  t\bp \st t \in \R \}$, where the zeroes of $f$.
So let's define
 \[ \bx(t) = \bx_c + t \bp + \ell(t)\grad f(\bx_c) \qquad  g(t) = f(\bx(t)) \]
 and then we want to find I think an $\ell(t) : \R \to \R$ such that
 \[ g(t) = 0\]
for any $t \in \R$. So any $t$-derivatives should identically be zero as well. Let's try to compute
derivatives
 \[ {\bd \over \bd t}g(t) = {\bd \over \bd t} f(\bx_c + t \bp + \ell(t)\grad f(\bx_c)) \]
 \[ = \sum_{i\in [n]} \left( {\bd \over \bd t}\left(\bx_c + t \bp_i + \ell(t)(\grad f(\bx_c))_i\right) \right)
f_{x_i} (\bx_c + t \bp + \ell(t)\grad f(\bx_c)) \]
 \[ = \sum_{i\in [n]} \left(  \bp_i + \ell'(t)(\grad f(\bx_c))_i \right)f_{x_i} (\bx + t \bp + \ell(t)\grad f(\bx_c)) \]
 \[ = (\bp + \ell' \grad f(\bx_c)) \cdot \grad f(\bx(t))\]
 \[  \ell' = -{\bp \cdot \grad f(\bx(t))\over \grad f(\bx_c) \cdot \grad f(\bx(t))}\]
\def\gg{\mathfrak{g}}
If we let $\gg$ be $\grad f(\bx_c)$, and supposing other uses of $f$ and its derivatives
are computed at $\bx + t \bp + \ell\gg$, what we've learned is
\[ 0 = (\bp + \ell' \gg) \cdot \grad f\]
and therefore
 \[  \ell' = -{\bp \cdot \grad f \over \gg \cdot \grad f  }\]

What about $\ell''$?
 \[ 0 = {\bd \over \bd t} \sum_{i\in [n]} ( \bp_i + \ell'\gg_i ) f_{x_i}  \]
 \[ 0 =  \sum_{i\in [n]}  \ell''\gg_i  f_{x_i}  + (\bp_i + \ell'\gg_i) {\bd \over \bd t}f_{x_i} \]
 \[ 0 =  \ell'' \gg \cdot \grad f +  \sum_{i\in [n]}   (\bp_i + \ell'\gg_i) {\bd \over \bd t}f_{x_i} \]
 \[ 0 =  \ell'' \gg \cdot \grad f +  \sum_{i\in [n]}   (\bp_i + \ell'\gg_i) \sum_{j\in [n]} (\bp_j + \ell'\gg_j) f_{x_ix_j} \]
Hm, I guess I could define the tensors
\[L^{i_1i_2 \cdots i_N} = \prod_{j\in [N]} \bp_{i_j} + \ell' \gg_{i_j}
\qquad
d^N f_{i_1i_2 \cdots i_N} = {\bd \over \bd x_{i_1}}\cdots {\bd \over \bd x_{i_N}} f
\]
and then
\[ 0 = \ell''\gg \cdot \grad f + L^{ij} d^2 f_{ij}\]
so
\[  \ell'' = - { L^{ij} d^2 f_{ij} \over \gg \cdot \grad f}\]
Can I get a third derivative?
 \[ 0 = {\bd\over\bd t}\left( \ell'' \gg^i  df_i +     (\bp^i + \ell'\gg^i)  (\bp^j + \ell'\gg^j) d^2 f_{ij} \right)\]
 \[  =  \ell''' \gg^i  df_i + \ell'' L^idf_i + {\bd \over \bd x_k}      (\bp^i + \ell'\gg^i)  (\bp^j + \ell'\gg^j) d^2 f_{ij} \]
Arg, no, now I'm making mistakes.

Let's start with
\[\gg = \grad f(\bx_c)\]
\[\bv(t) = t\bp + \ell \gg\]
\[\bx(t) = \bx_c + \bv(t)\]
\[0 = f(\bx(t))\]
And just write $f_i$ instead of $f_{x_i}$, and really embrace upstairs-downstairs summation notation. Then I get
\[0 = {\bd\over\bd t} f\]
\[ =  \bv' \cdot \grad f\]
\[ =  \bv_t^i  f_{i}\]
\[0 = {\bd \over \bd t} (\bv_t^i  f_{i})\]
\[ = \bv_{tt}^i  f_{i} + \bv_{t}^i\bv_t^j  f_{ij}\]
\[0 = {\bd \over \bd t} (\bv_{tt}^i  f_{i} + \bv_{t}^i\bv_t^j  f_{ij})\]
\[ =  \bv_{ttt}^i  f_{i} + 3\bv_{tt}^i\bv_t^j  f_{ij} + \bv_{t}^i\bv_t^j\bv_{t}^k  f_{ijk}\]
\[0 =  \bv_{tttt}^i  f_{i} + 4\bv_{ttt}^i\bv_t^j  f_{ij} + 6\bv_{tt}^i\bv_t^j\bv_{t}^k  f_{ijk}+  3\bv_{tt}^i\bv_{tt}^j  f_{ij} + \bv_{t}^i\bv_t^j\bv_{t}^k\bv_{t}^\ell  f_{ijk\ell}\]
Ok, this is making a lot more sense now. Write these as partitions, i.e.
\[[a_1, \ldots, a_n] = f_{i_1\cdots i_n}\prod_{j\in [n]} \bv^{i_j}_{t^{a_j}} \]
\[ = \sum_{i_1} \cdots \sum_{i_n} \left( {\bd \over \bd {x_{i_1}}}\cdots {\bd \over \bd_{x_{i_n}}}
 f\right) \prod_{j\in [n]} \left(\bd \over \bd t\right)^{a_j} \bv^{i_j} \]
 I get
\[ 0 = [1]\]
\[ 0 = [2] + [1,1]\]
\[ 0 = [3] + 3[2,1] + [1,1,1]\]
\[ 0 = [4] + 4[3,1] + 6[2,1,1] + 3[2,2] + [1,1,1,1] \]
\[ 0 = [5] + 5[4,1] + 10[3,2] + 4[3,1,1] + 6[3,1,1] + 15[2,2,1] \]
\[+ 10[2,1,1,1] + [1,1,1,1,1] \]

because
\[ {\bd\over \bd t} [a_1, \ldots, a_n] = \bigg([a_1 + 1, \ldots, a_n] + \cdots + [a_1, \ldots, a_n + 1] \bigg) + [a_1, \ldots, a_n, 1]\]
Yeah, this is definitely \texttt{oeis A000110}.

So does this help me compute $\ell$?
I know $\bv_t = \bp + \ell_t \gg$ and $0 = [1]$, therefore
\[0 = \bv_t^i f_i\]
\[ = (\bp + \ell_t \gg)^i f_i\]
\[ \ell_t = -{\bp^i f_i \over  \gg^i f_i}\]
as I had before. Subsequently I know $\bv_{tt} =  \ell_{tt} \gg$ and $0 = [2] + [1,1]$, therefore
\[0 = \bv_{tt}^i f_i + \bv_t^i\bv_t^j f_{ij}\]
\[0 = \ell_{tt} \gg^i f_i + \bv_t^i\bv_t^j f_{ij}\]
\[ \ell_{tt} =  -{ \bv_t^i\bv_t^j f_{ij} \over \gg^i f_i}\]
\[ =  -{ (\bp + \ell_t\gg)^i(\bp + \ell_t\gg)^j  f_{ij} \over \gg^i f_i}\]
\[ =  -{ (\bp^i\bp^j + 2\bp^i\ell_t\gg^j + \ell_t\gg^i\ell_t\gg^j)    f_{ij} \over \gg^i f_i}\]
\def\GG{\mathfrak{G}}
let's define $\GG = \gg^i f_i$, and then
\[ =  -{ (\bp^i\bp^j + 2\bp^i\ell_t\gg^j + \ell_t\gg^i\ell_t\gg^j)    f_{ij} \over \GG}\]
\[ =  -{ (\bp^i\bp^j \GG^2 + 2\bp^i\ell_t\gg^j \GG^2 + \ell_t\gg^i\ell_t\gg^j \GG^2)    f_{ij} \over \GG^3}\]
\[ =  -{ (\bp^i\bp^j \GG^2 - 2(\bp^kf_k)\bp^i\gg^j \GG + (\bp^kf_k)^2\gg^i\gg^j )    f_{ij} \over \GG^3}\]
\[ =  -{ (\bp^i\bp^j \gg^kf_k\gg^\ell f_\ell - 2(\bp^kf_k)\bp^i\gg^j\gg^\ell f_\ell + (\bp^kf_k)(\bp^\ell f_\ell)\gg^i\gg^j )    f_{ij} \over \GG^3}\]
\[ =  -{ (\bp^i\bp^j \gg^k\gg^\ell - 2\bp^i\gg^j\bp^k\gg^\ell  + \bp^k\bp^\ell\gg^i\gg^j ) f_kf_\ell   f_{ij} \over \GG^3}\]
\[ =  -{ (\bp^i \gg^k - \bp^k \gg^i) (\bp^j \gg^\ell - \bp^\ell \gg^j ) f_kf_\ell   f_{ij} \over \GG^3}\]
And then I know $\bv_{ttt} = \ell_{ttt} \gg$ and $0 = [3] + 3[2,1] + [1,1,1]$, therefore
\[ 0 = \bv_{ttt}^i f_i + \bv_{tt}^i \bv_t^j f_{ij} +  \bv_{t}^i \bv_t^j\bv_t^k f_{ijk} \]
\[ 0 =\ell_{ttt} \gg^i f_i + \bv_{tt}^i \bv_t^j f_{ij} +  \bv_{t}^i \bv_t^j\bv_t^k f_{ijk} \]
\[ \ell_{ttt} = - { \bv_{tt}^i \bv_t^j f_{ij} +  \bv_{t}^i \bv_t^j\bv_t^k f_{ijk} \over \GG} \]

Actually since I know that $\bp^k f_k$ at $\bx_c$ is zero, maybe I can simplify that $\ell_{tt}$ expression to
\[ =  -{ \bp^i \bp^j \gg^k   \gg^\ell f_kf_\ell   f_{ij} \over \GG^3}
 =  -{ \bp^i \bp^j   f_{ij} \over \gg^if_i}
 =  -{ \bp^i \bp^j   f_{ij} \over f^if_i} \]
That's not so bad at all!

Do I dare attempt to expand the $\ell_{ttt}$ expression? Well,
\[\bv^i_{tt} \bv^j_t f_{ij} \GG^4 = \ell_{tt} \gg^i (\bp^j + \ell_t\gg^j) f_{ij} \GG^4\]
\[ = (\GG^3\ell_{tt}) \gg^i (\bp^j \GG + \ell_t \GG \gg^j) f_{ij} \]
\[ = (\GG^3\ell_{tt}) \gg^i (\bp^j \GG - \bp^k f_k \gg^j) f_{ij} \]
\[ = -((\bp^i \gg^k - \bp^k \gg^i) (\bp^j \gg^\ell - \bp^\ell \gg^j ) f_kf_\ell   f_{ij}) \gg^o (\bp^m \GG - \bp^n f_n \gg^m) f_{o m} \]
\[ = -((\bp^i \gg^k - \bp^k \gg^i) (\bp^j \gg^\ell - \bp^\ell \gg^j ) f_kf_\ell   f_{ij}) \gg^o (\bp^m \gg^n f_n - \bp^n f_n \gg^m) f_{o m} \]
and at $\bx_c$ I could find that this is
\[ = -((\bp^i \gg^k ) (\bp^j \gg^\ell  ) f_kf_\ell   f_{ij}) \gg^o (\bp^m \gg^n f_n) f_{o m} \]
\[ = - \GG^3 \bp^i \bp^j   f_{ij} \gg^o \bp^m  f_{o m} \]
And then I could try to compute
\[ \GG^3 \bv_{t}^i \bv_t^j\bv_t^k f_{ijk} =   \GG^3 (\bp + \ell_t\gg)^i (\bp + \ell_t\gg)^j(\bp + \ell_t\gg)^k f_{ijk}   \]
\[ =    (\GG\bp - \bp^{i'}f_{i'}\gg)^i
(\GG\bp - \bp^{j'}f_{j'}\gg)^j
(\GG\bp - \bp^{k'}f_{k'}\gg)^k f_{ijk}   \]
and I can use the fact that at $\bx_c$ we have $\bp \cdot f = 0$ early, and say this is
\[ =   \GG^3\bp^i\bp^j\bp^k f_{ijk} \]
Ack, I think I made a mistake somewhere.

At $\bx_c$, we have $\ell_t = 0$. We know $\bv_t = \bp + \ell_t\gg$, so $\bv_t = \bp$ at $\bx_c$.
So because
\[ \ell_{tt} = - {\bv^i_t \bv^j_t f_{ij} \over \GG}\]
we know
\[ \ell_{tt} = - {\bp^i \bp^j f_{ij} \over \GG}\]
at $\bx_c$. Because I know
\[ \bv_{tt} = \ell_{tt} \gg \]
I can infer
\[ \bv_{tt} = - {\bp^i \bp^j f_{ij} \over \GG} \gg\]
Because
\[ \ell_{ttt} = - { 3\bv_{tt}^i \bv_t^j f_{ij} +  \bv_{t}^i \bv_t^j\bv_t^k f_{ijk} \over \GG} \]
I can get
\[ \ell_{ttt} = - { -3{\bp^k_t \bp^\ell_t f_{k\ell} \over \GG} \gg^i \bp^j f_{ij} +  \bp^i \bp^j\bp^k f_{ijk} \over \GG} \]
\[ = - { -3\bp^k_t \bp^\ell_t  \gg^i \bp^j f_{ij}f_{k\ell}  +  \bp^i \bp^j\bp^k f_{ijk}\GG \over \GG^2} \]
\[ = - { -3 \bp^k_t \bp^\ell_t  \bp^j \gg^i  f_{ij}f_{k\ell}  +  \bp^i \bp^j\bp^k \gg^\ell f_{ijk}f_\ell \over \GG^2} \]

Ok, armed with this, let's try the next bigger case. We have
\[ 0 = [4] + 4[3,1] + 6[2,1,1] + 3[2,2] + [1,1,1,1] \]
so
\[ 0 =  \bv_{tttt}^i  f_{i} + 4\bv_{ttt}^i\bv_t^j  f_{ij} + 6\bv_{tt}^i\bv_t^j\bv_{t}^k  f_{ijk} \]
\[+ 3\bv_{tt}^i\bv_{tt}^j  f_{ij} + \bv_{t}^i\bv_t^j\bv_{t}^k\bv_{t}^\ell  f_{ijk\ell}\]
\[ 0 =  \ell_{tttt}\gg^i  f_{i} + 4\ell_{ttt}\gg^i\bp^j  f_{ij} + 6\ell_{tt}\gg^i\bp^j\bp^k  f_{ijk} \]
\[+ 3\ell_{tt}\gg^i\ell_{tt}\gg^j  f_{ij} + \bp^i\bp^j\bp^k\bp^\ell  f_{ijk\ell}\]
\[ \ell_{tttt} =  - { 4\ell_{ttt}\gg^i\bp^j  f_{ij} + 6\ell_{tt}\gg^i\bp^j\bp^k  f_{ijk}
+ 3\ell_{tt}\gg^i\ell_{tt}\gg^j  f_{ij} + \bp^i\bp^j\bp^k\bp^\ell  f_{ijk\ell} \over  \GG}\]
I think very tentatively this is something like
\[-{
[pppp][g][g] + 3[pp][pp][gg] - 6[ppg][pp][g] + 12 [gp][gp][pp] - 4[ppp][g][gp]
\over \GG^3}
\]
where I define
\[[p^n g^m] = \bp^{i_1}\cdots \bp^{i_n}\gg^{j_1}\cdots \gg^{j_m} f_{i_1\cdots i_n j_1 \cdots j_m}\]
I may have gotten a sign wrong somewhere, but I think it's going to be an answer in terms of certain partitions of $p$ and $g$.

\subsection{Linear Counterexamples}

I'm having difficulty adapting the preceding reasoning into a counterexample that works specifically
with the tuple of functions $(D^1 f, D^2 f, D^3 f, \ldots)$.

I had a thought that maybe it's the case I can restrict my attention to adjusting the original function by
some linear map. As in: we have $f : \R^n \to \R$, we know that $f$ is $(n+1)$-critical at the origin, and we are trying
to find some extension of $f$ to $g : \R^{n+1} \to \R$ such that there are lots of $n$-critical points arbitrarily
near the origin, more than one per value of $x_{n+1}$. Let $t$ be a synonym for $x_{n+1}$.

First imagine the extension of $f$ to
\[ h(\bx,t) = f(\bx) + t \]
and then imagine swinging some hyperplane through the origin around as $t$ varies --- this amounts to picking a
vector $\bp$ and setting
\[ g(\bx, t) = h(\bx, (\bp \cdot \bx)t) = f(\bx) + (\bp \cdot \bx)t \]

\subsection{Type Theory Thoughts}

The `abstraction bottleneck' I want to exist is like so: there should
be a very strong, propositional-equality-level, $J$-rule obeying
invariance of predicates on properly formed {\em cells}, with respect to the notion
of equivalence that comes with higher-dimensional cells joining them which are as stable
as the lower dimensional cells.

This means that although the `implementation language' might know about actual concrete functions
$\R^n \to \R$ and be able to talk about compositions with $\ups$ and $\dns$, above this abstraction
boundary those kinds of compositions don't make sense. I can't ask what {\em the} first half of a cell
looks like, when I only have been given the cell up to reparameterization in the first place.

\subsection{Higher-Order Categories}

Resummarizing what consequences all the above has on how I
conceptualize $\omega$-categories: A category is a space that tells us
which paths (2-paths, 3-paths, etc.) exist, which is something like a
presheaf category over the ways that (higher) paths can be
reparameterized, plus some sheaf-like condition to require
composition. But I'm less certain what that sheaf-like condition is,
now, since smoothness doesn't compose as easily as continuity.

The essential information about what (higher) paths the category allows or not seems like it can be captured
by asking for a smooth map from some underlying manifold into $\R^n = \{ (p^1, \ldots, p^n) \st p^i \in \R \}$,
and then talking about when you're allowed
to make some $p^i = 0$ --- e.g., the constraint
\[(p^3 = 0) \imp  (p^3_x > 0)\]
would effectively create a one-directional 1-cell, and
\[(p^4 = 0 \land p^4_{x} = 0 ) \imp (p^4_{xx} > 0 \land p^4_y < 0)\]
would create a 2-cell. Saying `create' is sort of backwards, though --- what's actually happening when we make one
of these constraints is actually {\em deleting} cells that would otherwise be able to exist.
So both of these above examples are turning what would otherwise begin life as 1- and 2-dimensional equivalences
{\em into} mere directed cells by forbidding some of the cells that participate in those equivalences.

I think I want to impose some kind of restrictions on which restrictions make sense, however. I have
an intuition that, for example, making demands about the $x$-derivative of $p^i$ only makes sense if I've
already required that $p^i = 0$. Likewise, making $< 0$ or $> 0 $ or $= 0$ demands about $p^i_{xx}$ or $p^i_{y}$ should
only happen if I've already said $p^i_x = 0$.

Is it as simple as the following?
\begin{quote}
  If I assert anything about $p^i_V$, and $W \prec V$, then I should have already asserted $p^i_W$.
\end{quote}
where $W \prec V$ is defined by
\[
\erule
{\forall k \in [n] . i_k \le j_k }
{x_{i_1}\cdots x_{i_n} \preceq x_{j_1}\cdots x_{j_n} V}
\]
so basically anything that resutls from dropping variables or
reducing the indices of variables should have already been zero.

I note that
\[(p^4 = 0 \land p^4_{x} = 0 ) \imp (p^4_{xx} > 0 \land p^4_y < 0)\]
is logically equivalent to
\[(p^4 = 0 \land p^4_{x} = 0 \land \lnot(p^4_{xx} > 0 \land p^4_y < 0)) \imp \bot\]
and therefore to
\[(p^4 = 0 \land p^4_{x} = 0 \land (p^4_{xx} \le 0 \lor p^4_y \le 0)) \imp \bot\]
which is logically equivalent to the conjunction of the two constraints
\[(p^4 = 0 \land p^4_{x} = 0 \land p^4_{xx} \le 0)  \imp \bot\]
\[(p^4 = 0 \land p^4_{x} = 0 \land p^4_{y} \le 0 ) \imp \bot\]
or if I really want to expand out the $\le$s, I could say the four constraints
\[(p^4 = 0 \land p^4_{x} = 0 \land p^4_{xx} = 0)  \imp \bot\]
\[(p^4 = 0 \land p^4_{x} = 0 \land p^4_{y} = 0 ) \imp \bot\]
\[(p^4 = 0 \land p^4_{x} = 0 \land p^4_{xx} < 0)  \imp \bot\]
\[(p^4 = 0 \land p^4_{x} = 0 \land p^4_{y} < 0 ) \imp \bot\]

So I like to think that all (finite?) higher-dimensional categories can be described by a list
of constraints like this, each one the negation of some conjunction of value/derivative constraints.

How does this work for multi-$p$-variable arrangements? Suppose I abbreviate $p = p^1$ and $q = p^2$.
I suppose $p = 0$ and $q = 0$, and subsequently I should be able to say something about what kinds
of 2-cells are allowed to traverse this intersection. Let's see... the intersection itself would be governed by
something like
\[ (p =0, p_x < 0, q = 0, q_x > 0) \imp \bot\]
where I'm picking the $<,>$ arbitrarily; there are 4 such examples.
What kind of situation has two 2-cells, both of which are $p/q$ intersections?
Hmm, I'm finding that there might be {\em two} points (labelled $\bx_1, \bx_2$ below), {\em both} with
\[ p =0, p_x < 0, q = 0, q_x > 0 \]
\[\begin{tikzpicture}

\node (x) at (1.5,0)[right] {$x$};
\node (y) at (0,-0.9)[below] {$y$};
\draw[->, opacity=0.25] (-1.5,0)--(x);
\draw[->, opacity=0.25] (0,0.9)--(y);

\node[label={left,red}:{$p>0$}] (p) at (-3,0) {};
\node[label={right,blue}:{$q>0$}] (q) at (3,0) {};
\node[label={above}:{$\bx_1$}] (x1) at (0,1.73205) {};
\node[label={below}:{$\bx_2$}] (x2) at (0,-1.73205) {};

\fill[red, opacity=0.25] (-1,0) circle (2);
\fill[blue, opacity=0.25] (1,0) circle (2);
\draw[red, line width=1] (-1,0) circle (2);
\draw[blue, line width=1] (1,0) circle (2);
\fill (x1) circle (2pt);
\fill (x2) circle (2pt);
\end{tikzpicture}\]
and the only obvious thing that distinguishes them is either of $p_y$ or $q_y$, which in this particular example
are jointly positive for $\bx_1$, and negative for $\bx_2$. Can I picture the various 2-cells that could exist
for the full matrix of $p_x \lessgtr 0, q_x \lessgtr 0, p_y \lessgtr 0, q_y \lessgtr 0$?
I probably can assume that $p_x q_y - p_y q_x$ isn't zero, and that I know its sign. Actually wait a minute, maybe
the $y$-derivatives themselves aren't the important part. I can picture a situation like
\[\begin{tikzpicture}
\begin{scope}[shift={(0,0)}]
\node (x) at (2,0)[right] {$x$};
\node (y) at (0,-0.9)[below] {$y$};
\draw[->, opacity=0.25] (-2,0)--(x);
\draw[->, opacity=0.25] (0,2)--(y);
\end{scope}
\node[label={left,red}:{$p>0$},label={below left, red}:{$p = -x  +y + 1.5$}] (p) at (-2,1) {};
\node[label={right,blue}:{$q>0$},label={below right, blue}:{$q = 4x - y - 3$}] (q) at (3,1) {};
\node[label={right}:{$\bx_3$}] (x1) at (0.5,1) {};

\node  at (1.5,2) {$\bx_3 = ( )$};
\fill[red, opacity=0.25] (-1.5,3)--(2,-0.5)--(-2,-0.5)--(-2.25,3);
\fill[blue, opacity=0.25] (0,3)--(1,-1)--(3,-1)--(3,3);
\draw[red, line width=1] (-1.5,3)--(2,-0.5); % -x + y + 1.5 < 0
\draw[blue, line width=1] (0,3)--(1,-1); % 4x - y - 3 > 0
\fill (x1) circle (2pt);
\end{tikzpicture}\]
and I see that $p_y > 0$ and $q_y < 0$. But by invariance under the kind of
reparameterizations I intuitively think {\em should} leave everything important invariant, namely in this case
moving things around monotonically in the $x$ direction parameterized arbitrarily by the $y$ coordinate ---
this means $\bx_3$ really ought to be the ``same'' 2-cell cell as $\bx_1$. So what matters here is $p_x < 0$,
and $q_x > 0$, and... perhaps the sign of $p_x q_y - q_x p_y $? In this case I compute $-1 \cdot -1 - 4 \cdot 1.5 = -5 < 0$.
And likewise in the previous case with the circles, where $\textcolor{red}{p = 4 - (x+1)^2 - y^2}$ and
$\textcolor{blue}{q = 4 - (x-1)^2 - y^2}$, and $\bx_1 = (0, -\sqrt{3})$, I compute
\[p_x(\bx_1) = -2\qquad p_y(\bx_1)= 2\sqrt{3}\]
\[q_x(\bx_1) = 2\qquad q_y(\bx_1)= 2\sqrt{3}\]
and I get $p_x q_y - p_y q_x = -4 \sqrt{3} - 4\sqrt{3} = -8\sqrt{3} < 0$.

\subsubsection{Invariance}
Can I quick convince myself that this determinant can't flip sign under the kinds of transforms I'm thinking of?
I start out with $p(x,y), q(x,y) : \R^2 \to \R$. I want to adjust the $x$ by something $y$-dependent,
so I want to adjust this to $\hat p = p(h(x,y),y)$ and $\hat q = q(h(x,y), y)$. But I assume $h$ is monotone in $x$,
so always $h_x(x,y) > 0$.

The determinant I start out with is
\[\Delta := \left|\begin{array}{cc} p_x & p_y \cr q_x & q_y\end{array}\right| =  p_xq_y - q_x p_y\]
and I want to investigate
\[\hat \Delta := \left|\begin{array}{cc} \hat p_x & \hat p_y \cr \hat q_x & \hat q_y\end{array}\right| =  \hat p_x\hat q_y - \hat q_x \hat p_y\]
So I reckon
\[ \hat p_x = p_xh_x \qquad \hat q_x = q_xh_x \]
\[ \hat p_y = p_y + p_xh_y \qquad \hat q_y = q_y + q_xh_y \]
so
\[ \hat \Delta = \hat p_x\hat q_y - \hat q_x \hat p_y \]
\[ = p_xh_x(q_y + q_xh_y) - q_xh_x(p_y + p_xh_y)\]
\[ = h_x \Delta + p_x q_x h_y - q_x p_x h_y \]
\[ = h_x \Delta \]
which has the same sign as $\Delta$ because I assumed $h_x > 0$ everywhere!

Can I do the same thing in 3 dimensions?
Let
\[ \hat p^i = p^i(h(x,y,z),k(y,z),\ell(z)) \]
and assume $h_x,k_y, \ell_z > 0$. Then use determinant column-scaling-and-adding invariants
to reason that
\[ \hat \Delta :=
\left|\begin{array}{ccc}
 p^i_x h_x
&  p^i_x h_y + p^i_y k_y
&  p^i_x h_z + p^i_y k_z + p^i_z \ell_z
\end{array}\right|
\]
\[ =
\left|\begin{array}{ccc}
 p^i_x h_x
&  p^i_y k_y
&  p^i_y k_z + p^i_z \ell_z
\end{array}\right|
\]
\[ =
\left|\begin{array}{ccc}
 p^i_x h_x
&  p^i_y k_y
&   p^i_z \ell_z
\end{array}\right|
\]
\[ = h_xk_y\ell_z \Delta \]

\subsection{Allowed Constraints}

Now I'm starting to think this is the rule: if I've assumed $p^1 = 0, \ldots, p^n = 0$, then I'm allowed
to make a constraint on
\[ \det_{ij} {\bd p^i \over \bd x_j}\]
Which means in particular that if I have $p = 0$, I'm allowed to make assertions about $p_x$.
And even more concretely, if I have $f = 0, f_x = 0$, I'm allowed both to demand the sign of
$f_{xx}$ (because $f_x = 0$) and, indirectly the sign of $f_y$! For I can directly demand the sign
of $f_{xx} f_y - f_{xy} f_x$, and I know $f_x = 0$ so this is the same as demanding the sign of $f_{xx} f_y$.
And $f_y > 0$ is basically the same thing as
\[(f_{xx} f_y > 0 \land f_{xx} > 0) \lor (f_{xx} f_y < 0 \land f_{xx} < 0)\]
I'm ignoring the case of $f_{xx} = 0$ but I think that's ok, as that would be a different criticality level anyhow.

\subsection{Redundant Constraints}

The story is still somewhat subtle, though. Let's define $[p,q] = p_x q_y - q_x p_y$.
Let's say I know $p = q = r = 0$.  I'm allowed to make assertions about $[p,q],[q,r],[p,r]$.
If I say $[p,q] > 0$ and $[q,r] > 0$, then those together imply that $[p,r] > 0$,
for
\[ [p,q] > 0 \iff {p_x \over p_y} > {q_x \over q_y} \]
\[ [q,r] > 0 \iff {q_x \over q_y} > {r_x \over r_y} \]
\[ [p,r] > 0 \iff {p_x \over p_y} > {r_x \over r_y} \]
so for instance the `constraint'
\[ (p = 0, q = 0, r = 0, [p,r] > 0, [p, q] > 0, [p, r] < 0)\imp \bot \]
is already automatically satisfied by every function.

Similarly, we have implications such as
$[p,q] = 0$ and $[q,r] > 0$ implies $[p,r] > 0$, and
$[p,q] = 0$ and $[q,r] = 0$ implies $[p,r] = 0$.

\subsection{Classification}

Suppose I do know that $f = 0, f_x = 0, f_{xx} > 0, f_y > 0$ at $\bx$,
which is the same thing as
\[f = 0, f_x = 0, f_{xx} > 0, f_xf_{xy} - f_yf_{xx}  < 0\]
which I might notate as $f^0, [f]^0, [[f]]^+, [f,[f]]^-$.
Does this actually {\em characterize} a local neighborhood of $\bx$, up to equivalence?

Actually, on the face of it, equivalence (by critical-point-free
cells) is stronger than just monotone reparameterization. Do I know
that equivalence respects the signs of all these determinants? Maybe
it does automatically since the point of interest changing
sign of some determinant would automatically {\em be} a critical point
by definition.

So maybe I should ask: is there a package of derivative signs that
characterise a neighborhood of a point up to reparameterization?
Or maybe at least the set of roots in the neighborhood? Warming up with the easiest examples,
if I have a point for which $p^i \ne 0$ for all $i$, there is a neighborhood around it with
no roots, merely by continuity --- so all such points are equivalent under monotone reparameterization.
If I have two functions $p^1,p^2$ for which $p^i = 0$ and $p^i_x > 0$, then I do think that I can find a monotone
reparameterization that maps the roots of the one to the other. Can I be more explicit about how I construct it?

The implicit function theorem I think tells me that there exist functions $r^1,r^2:\R \to \R$ such that
locally
\[ p^1(x,y) = 0 \iff r^1(y) = x \]
\[ p^2(x,y) = 0 \iff r^2(y) = x \]
and I want to find an $h(x,y)$ with $h_x > 0$ such that
locally
\[ p^1(h(x,y),y) = 0 \iff p^2(x,y) = 0 \]
so I try to set
\[ h(x,y) = x + r^1(y) - r^2(y) \] and reason
\[ p^1(h(x,y),y) = 0 \iff p^1(x + r^1(y) - r^2(y),y) = 0 \]
\[\iff p^1(x + r^1(y) - r^2(y),y) = 0 \]
\[\iff r^1(y) = x + r^1(y) - r^2(y)  \]
\[\iff r^2(y) = x   \]
\[\iff p^2(x,y) = 0 \]

Wait, is this the wrong calculation? Let's take just $p^1(x)$ and $p^2(x)$ and assume $p^i = 0$ and $p^i_x > 0$.
I think I ought to be able to get them to be exactly the same. I want to find an $h$ with $h_x > 0$
such that locally $p^1(h(x)) = p^2$. Easy, just appeal to the inverse function theorem
and set $h(x) = (p^{1})^{-1} \o p^2$.

Next step is the set of constraints from the beginning of the section. Suppose $p^1,p^2$ with
\[p^i = 0, p^i_x = 0, p^i_{xx} > 0, p^i_y > 0\]
at $\bz$. Can I find $h,k$ with $h_x > 0$ and $k_y > 0$ at $\bz$, such that near the origin
\[ p^1(h(x,y),k(y)) = p^2(x,y) \]
?
\subsubsection{Easier To Compare Against a Canonical Function}
For the 1-critical case, I could have fixed $p^2$ as $f = x$. In other
words, give me a $p$, assert $p = 0$ and $p_x > 0$, show me that
there's a monotone reparameterization that equates $p$ with just $f =
x$. This is just $p^{-1}$ if I put the reparameterization burden
inside $p$'s arguments, but if I put it inside the $x$, it's even
easier: just find a $h$ with $h_x > 0$ such that $p = h$. Well, $p$ itself
will do just fine!

For the 2-critical case, I can try to relate an arbitrary $p$ to $x^2 + y$.
As in, give me a $p$, assert $p = 0, p_x = 0, p_{xx} > 0, p_y > 0$,
the task is to find $h(x,y)$ and $k(y)$ with $h(0,0)=0$ and $k(0)=0$ and $h_x > 0$ and $k_y > 0$ such that locally
\[p(x,y) = h(x,y)^2 + k(y) \]
well I can observe that necessarily
\[ h(x,y) = \sqrt{p(x,y) - k(y)}   \]
so I mainly just need to ensure that $p(x,y) - k(y)$ is positive.
Can I set something like $k(y) = p(0,y)$? Let's think about that:
\[ k(y) = p(0,y) \qquad h(x,y) = \sqrt{p(x,y) - k(y)}\]
One piece of good news is that I can immediately see that $k_y > 0$.
Can I see that $h_x > 0$? The square root is annoying.
\[h_x(x,y) = {p_x(x,y)\over 2\sqrt{p(x,y) - k(y)}}\]
so at $(0,0)$, this is a $0/0$. Can we L'H\^opital it?
\[h_x(0,0) = {p_{xx}\over p_x/\sqrt{p - k}}\]
Can I solve this recursion? If I define
\[ \delta = {p_x\over 2\sqrt{p - k}}\]
then what I have is
\[\delta = p_{xx} / 2\delta\]
\[\delta^2 = p_{xx} / 2\]
\[\delta = \sqrt{p_{xx} / 2}\]
So indeed $h_x > 0$! Is this my solution, then?

\subsubsection{Is the square root legitimate?}
Here's an example function:
\[ p(x,y) = e^x  + y - 1 - x + xy\]
we can compute
\[ p_x = e^x  -1  + y \qquad  p_{xx} = e^x \qquad p_y = 1 + x\]
and see that at zero we have $p=0$ and $p_x = 0$ and $p_{xx} > 0$ and $p_y >0$ as required.
But if I take
\[ p(x,y) - p(0,y) \]
It's not clear to me that this is positive in a neighborhood of $\bz$.
\subsubsection{Tracking the local minimum instead}
Maybe the problem is that $p(0,y)$ doesn't necessarily find the real minimum of the function.
I should be looking, for each $y$, for where $p_x = 0$. The fact that $p_{xx} \ne 0$ means
that (locally) for all $y$ there exists a unique $x$ such that $p_x(x,y) =0$. Call this
function $r(y)$. Let $k(y) = p(r(y), y)$, and again $h(x,y) = \sqrt{p(x,y) - k(y)}$. I think
we still have $h_x > 0$ by previous reasoning, but I have to convince myself that
$k_y > 0$. I know
\[ k_y = p_x r_y + p_y \]
Ah, but $p_x = 0$ at $\bz$! So this falls back to $p_y$ of which I know $p_y > 0$ at $\bz$. Done.
The proof that $p(x,y) > k(y)$, i.e. $p(x,y) > p(r(y),y)$ locally holds should follow from the fact that $p_{xx} > 0$ locally,
and $p_x(r(y), y) = 0$. Let's confirm it for the example above: we set
\[ 0 = p_x = e^x - 1 + y \]
so solving for $x$, we find
\[ x = \ln (1 - y) \]
and graphing this out, this seems to work great.
\subsubsection{Interection of two functions}
Let's say I have $p = 0, q = 0, p_x > 0, q_x > 0, [p,q] > 0$. I think the functions I want to compare these to
are
\[\hat p (x,y) = x - y \]
\[\hat q (x,y) = x + y \]
\[ \]
I might then try to find $h(x,y)$ with $h_x > 0$
and $k(y)$ with $k_y > 0$ such that simultaneously
\[ p = h(x,y) - k(y)\]
\[ q = h(x,y) + k(y)\]
so I'm forced to conclude that
\[h = {p + q \over 2}\]
and
\[ k(y) = q(x,y) - h(x,y) = {q(x,y) - p(x,y) \over 2}\]
which is a big problem wince I haven't eliminated the $x$ dependency!

\subsection{Varieties}

Consider a variety of codimension 1, a single polynomial $p: \R^n \to \R$.
Where does it have 2-critical points? Where $p = 0$ and $p_x = 0$. This is
still a variety, of codimension 2.

If I have a finite collection of polynomials $p^1,\ldots,p^N : \R^n \to \R$,
and consider their product $p^1\cdots p^N$, the places where that has a 2-critical point
according to the same definition is where
\[p^1\cdots p^N = 0\]
\[\sum_{i \in [N]} p^i_x{p^1\cdots p^N\over p^i} = 0\]
whereas what I'd expect more directly from thinking about variety that arises from the
disjunction
\[p^1 = 0 \lor \cdots \lor p^N = 0\]
is that you'd have to satisfy
\[J := \bigvee_{i,j \in [N] \atop i \ne j} (p^i = 0 \land p^j = 0) \lor \bigvee_{i\in [N]}(p^i = 0 \land p^i_x = 0)\]
In the $N = 2$ case this is
\[(p = 0 \land q = 0) \lor (p = 0 \land p_x = 0) \lor (q =0 \land q_x = 0)\]
which we can abbreviate as
\[(p \land q ) \lor (p \land p_x ) \lor (q \land q_x )\]
which is the same as
\[(p \lor q) \land (p \lor q_x) \land (p_x \lor q)\]
Hm, abbreviating zero-equalities away from $J$ gives
\[J := \bigvee_{i,j \in [N] \atop i \ne j} (p^i \land   p^j)  \lor \bigvee_{i\in [N]} (p^i \land   p^i_x) \]

What about the $N = 3$ case? If I write
 $\land$ as $+$ and $\lor$ is juxtaposition, $J$ becomes
\[(p  +q)  (q+r)  (p+r )  (p+p_x )  (q+q_x)  (r+r_x) \]
\[ = (pq + pr + q + qr) (p+r)  (p+p_x )  (q+q_x)  (r+r_x) \]
\[ = (pr + q)(p+r)   (p+p_x )  (q+q_x)  (r+r_x) \]
\[ = (pr + qp+ qr)   (p+p_x )  (q+q_x)  (r+r_x) \]
\[ = (pr + pq + pqr + pp_xr + pp_xq+ p_xqr)  (q+q_x)  (r+r_x) \]
\[ = (pr + pq  + p_xqr)  (q+q_x)  (r+r_x) \]
\[ = (pqr + pq  + p_xqr + prq_x + pqq_x  + p_xqq_xr)   (r+r_x) \]
\[ = ( pq  + p_xqr + pq_xr )   (r+r_x) \]
\[ =  pqr  + p_xqr + pq_xr   + pqr_x  + p_xqrr_x + pq_xrr_x     \]
\[ =  pqr  + p_xqr + pq_xr   + pqr_x   \]
Ok, that's enough of a pattern for me to conjecture that $J \prequiv K$ when
\[ K :=  \bigvee_{i\in [N]}p^i \land \bigwedge_{i \in [N]}\left(p^i_x \lor \bigvee_{j\ne i} p^j\right)\]

Ok, let's just write $p_x$ as $q$, and write this out more like ordinary first-order logic:
\[ J := (\exists i, j. i\ne j \land p(i) \land p(j)) \lor (\exists i . p(i) \land q(i))\]
\[ K := (\exists i. p(i)) \land \forall i . (q(i) \lor \exists j . i\ne j \land p(j))\]

Let's try to prove $J \prov K$. By inversion, there are four cases.
These two are easy:
\[
\erule
{}
{ i\ne j , p(i) , p(j)  \prov \exists i. p(i) }
\]

\[
\erule
{}
{   p(i) , q(i) \prov \exists i. p(i) }
\]
For this one, regardless of what $k$ is, there's a distinct index for which $p(j)$,
since we have two distinct ones:
\[
\erule
{}
{  i\ne j , p(i) , p(j)  \prov  q(k) \lor \exists j . k\ne j \land p(j)}
\]

For this one we can condition on whether $i = k$ or not:
\[
\erule
{}
{  p(i) , q(i) \prov   q(k) \lor \exists j . k\ne j \land p(j)}
\]
if they are equal, then we can use the $q(i)$. If they aren't, we can show
there is a $j$ distinct from $k$ (namely $i$) for which $p(i)$.

Let's try to prove $K \prov J$.
\[
\erule
{}
{ p(i), \forall i . (q(i) \lor \exists j . i\ne j \land p(j)) \prov (\exists i, j. i\ne j \land p(i) \land p(j)) \lor (\exists i . p(i) \land q(i))}
\]
Hm, let's try to $\forall L$ at the parameter $i$, I guess. We then have two subgoals:
\[
\erule
{}
{ p(i),  q(i)  \prov (\exists i, j. i\ne j \land p(i) \land p(j)) \lor (\exists i . p(i) \land q(i))}
\]
\[
\erule
{}
{ i\ne j, p(i), p(j) \prov (\exists i, j. i\ne j \land p(i) \land p(j)) \lor (\exists i . p(i) \land q(i))}
\]
Great! Both of these are trivial.

Ok so in that case, the thing I'm calling $K$ here looks an awful lot like the derivative-of-product expression, call the latter $L$:
\[ K = \prod_i p^i = 0 \land \bigwedge_i  \left(p^i_x \cdot \prod_{j \ne i}  p^j\right) = 0\]
\[ L = \prod_i p^i = 0 \land \sum_i \left(p^i_x \cdot \prod_{j \ne i}  p^j\right) = 0\]
Can I find an example that distinguishes these two cases?

Trying some random examples, I haven't been able to so far. Is it the case that $L \prov K$ after all?

Let's focus on the $N = 2$ case. We suppose that we have a point where $p = 0$ or $q = 0$,
and where
\[ p_x q + p q_x = 0\]
Ah, this concrete example makes it much more apparent what's going on. If $p = 0$, then $p q_x = 0$.
If $q = 0$, then $q p_x = 0$.

What about $N= 3$? We have $p = 0 \lor q = 0 \lor r = 0$. We know
\[ p_x q r + p q_x r + pqr_x = 0\]
we want to show
\[ p_x q r = 0 \lor p q_x r = 0 \lor pqr_x = 0\]
But all we have to do is split cases and we're done. We do have $K \prequiv L$ after all.

\subsubsection{3-criticality}
When I'm looking for 2-criticality I'm computing something like
\[ \prod_{p\ne q} (p + q) \cdot \prod_{p} (p + [p])   \]
So when I'm looking for 3-criticality I probably want
\[ \prod_{p\ne q\ne r} (p + q + r) \cdot \prod_{p\ne q} (p + q + [p]) \cdot \prod_{p\ne q} (p + q + [p,q]) \cdot \prod_{p} (p + [p] + [p,[p]])   \]
In both of these cases $[p,q,\cdots]$ is notation for the jacobian determinant.

In isolation, what is $\prod_{p\ne q} p + q$? In the four-variable case I have
\[(p +q )(p+r)(p+s)(q+r)(q+s)(r+s)\]
\[ = (p +qr )(p+s)(q+r)(q+s)(r+s)\]
\[ = (p +qrs )(q+r)(q+s)(r+s)\]
\[ = (pq + pr +qrs )(q+s)(r+s)\]
\[ = (pq + prs +qrs )(r+s)\]
\[ = pqr  + pqs + prs +qrs \]
\[ = \sum_{p} \prod_{q\ne p} q\]

The first-order logic equivalence looks like

\[ \exists p, q. p \ne q \land z(p) \land z(q) \prequiv \forall p . \exists q . p \ne q \land z(q) \]
This makes intuitive sense; if there are two distinct zeros, then for every zero, there's one that isn't it.
So I can guess what
\[ \prod_{p\ne q\ne r} (p + q + r)\]
normalizes to, namely
\[ \sum_{p \ne q} \prod_{r \not\in \{p,q\}} r\]
(it doesn't matter up to equivalence whether I sum over $p,q$ or $p,q$ with $p\ne q$, but including
the $\ne$ gives a smaller sum)

\subsubsection{Concretely}

\[ (p+q+r+s)  (p + [p] + q)  (p + [p] + r)  (p + [p] + s)\]
\[ = (p + [p](r+s) + q)   (p + [p] + r)  (p + [p] + s)\]
\[ = (p  +  [p](r+s)  + pr  + qr)    (p + [p] + s)\]
\[ = p  +  [p](r+s)   + qrs\]
\[ ( p  +  [p](r+s)   + qrs)(q + [q] + p)  (q + [q] + r)(q + [q] + s) \]
\[ ( p  +  [p](r+s)   + qrs)(q + [q] + prs)  \]
\[  pq  +  [p]q(r+s)   + qrs +  p[q]  +  [p][q](r+s)   +  prs    \]
\[  (pq  +  [p]q(r+s)   + qrs +  p[q]  +  [p][q](r+s)   +  prs)(r + [r] + pqs)    \]
\[  (pq  +  [p]q(r+s)   + qrs +  p[q]  +  [p][q](r+s)   +  prs)(r + [r] + pqs)    \]
\[ =  pqr  +  [p]qr   + qrs +  p[q]r  +  [p][q]r   +  prs + pq[r]  +  [p]q[r]s    +  p[q][r]  +  [p][q][r]s + pqs   \]
\[ (  pqr  +  [p]qr   + qrs +  p[q]r  +  [p][q]r   +  prs + pq[r]  +  [p]q[r]s    +  p[q][r]  +  [p][q][r]s + pqs )(s + [s] + pqr)  \]
\[ =
    qrs +  [p][q]rs   +  prs   +  [p]q[r]s  +  p[q][r]s  +  [p][q][r]s + pqs +\]
\[  pqr[s]  +  [p]qr[s]   +  p[q]r[s]  +  [p][q]r[s]    + pq[r][s]      +  p[q][r][s]   +   pqr \]
I don't think this calculation is actually helping me understand what's going on.

\subsection{A guess}
Let $\exists \#(p,q,r,\ldots)$ mean
that $p,q,r,\ldots$ are all distinct.
Write a unary predicate as overline.
My guess for the meaning of 3-criticality is
\def\hh#1{\overline {#1}}
\[ C := C_1 \lor C_2 \lor C_3 \lor C_4 \]
\[C_1 := \exists \#(p,q,r).\hh p\land \hh q \land \hh r\]
\[C_2 := \exists \#(p,q).\hh p\land \hh q \land \hh{[p]} \]
\[C_3 := \exists \#(p,q).\hh p\land \hh q \land \hh {[p,q]}\]
\[C_4 := \exists p.\hh p\land \hh {[p]} \land \hh {[p,[p]]}  \]
and a guess for a related predicate is
\[ B := B_1 \land B_2 \land B_3 \land B_4 \]
\[B_1 := \exists p . \hh p\]
\[B_2 := \forall p . \hh{[p]} \lor  \exists q \ne p . \hh q\]
\[B_3 := \forall \#(p,q) . \hh{[p]} \lor \hh{[q]} \lor \hh{[p,q]} \lor  \exists r \not\in\{ p,q\} . \hh r\]
\[B_4 := \forall p . \hh{[p,[p]]} \lor  \exists q \ne p . \hh q\]
I think if I didn't slip up I can see a proof of $B \prequiv C$.

So if I read overline as `equals zero', then I know that
\[B_1 \iff \exists i . \hh{p^i} \iff \hh{\prod_i p^i}\]
and I think I can reason that
\[ B_1 \land B_2 \iff \hh{\prod_i p^i} \land (\forall i . \hh{p^i_x} \lor \exists j\ne i . \hh{p^j} )\]
\[ \iff \hh{\prod_i p^i} \land \left(\forall i . \hh{p^i_x \cdot \prod_{j\ne i}  {p^j}} \right)\]
\[ \iff \hh{\prod_i p^i} \land \hh{\sum_i  p^i_x \cdot \prod_{j\ne i}  {p^j}} \]
\[ \iff \hh{\prod_i p^i} \land \hh{{\bd \over \bd x} \prod_i p^i} \]
Ok, now to try to massage $B_3$ and $B_4$ into something sensible.

Note that
\[\left(\bd \over \bd x\right)^2 \prod_i p^i =
\sum_{i} p^i_{xx}\prod_{j\ne i} p^j + \sum_{\#(i,j)} p_x^ip_x^j\prod_{k\not\in\{i,j\}} p^k\]
\[{\bd \over \bd y }\prod_i p^i =
\sum_{i} p^i_{y}\prod_{j\ne i} p^j \]

\end{document}
