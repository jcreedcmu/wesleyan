\documentclass{article}
\usepackage[tmargin=0.05in, bmargin=0.05in]{geometry}
\input{theorem}
\input{prooftree}
\usepackage{stmaryrd}
\usepackage{latexsym}
\usepackage{yfonts}
\usepackage{amsmath}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{tikz}
\usetikzlibrary{calc,arrows,cd,decorations.pathreplacing}
\usetikzlibrary{decorations.pathmorphing}
\usepackage{tcolorbox}
\tcbuselibrary{breakable}
\usepackage{listings}
\lstset{basicstyle=\ttfamily\footnotesize,breaklines=true}

\def\prequiv{\dashv\vdash}
\def\fdom{\mathbf{d}f}
\def\fcod{\mathbf{e}f}
\def\thn{\mathrel|}
\def\coe{\mathsf{coe}}
\def\bpush{\mathbf{push}}
\def\bpull{\mathbf{pull}}

\def\wo{\setminus}
\def\fA{\mathsf{A}}
\def\fC{\mathsf{C}}
\def\fX{\mathsf{X}}
\def\ff{\mathsf{f}}
\def\fm{\mathsf{m}}
\def\rspan{\mathsf{Span}}
% \def\El#1{\mathsf{El}({#1})}
\def\El#1{\ulcorner{#1}\urcorner}
\def\U{\mathsf{U}}
\def\uu{\mathsf{u}}
\def\uni{\U}
\def\mor#1#2{#1 \to \underline{#2}}

\def\ridp{\mathsf{idp}}
\def\ssem#1{\langle\!\langle{#1}\rangle\!\rangle}

\definecolor{lred}{rgb}{0.95,0.8,0.8}
\definecolor{cyan}{rgb}{0.80,0.95,0.95}
\definecolor{orange}{rgb}{0.95,0.90,0.80}
\definecolor{bluegray}{rgb}{0.6,0.6,0.85}
\definecolor{lgray}{gray}{0.9}
\newtcolorbox{declbox}{colframe=lred,colback=lred,grow to right by=-10mm,grow to left by=-10mm,
boxrule=0pt,boxsep=0pt,breakable}
\newtcolorbox{thmbox}{colframe=cyan,colback=cyan,grow to right by=-10mm,grow to left by=-10mm,
boxrule=0pt,boxsep=0pt,breakable}
\newtcolorbox{defnbox}{colframe=orange,colback=orange,grow to right by=-10mm,grow to left by=-10mm,
boxrule=0pt,boxsep=0pt,breakable}
\newtcolorbox{tangentbox}{colframe=lgray,colback=lgray,grow to right by=-10mm,grow to left by=-10mm,
boxrule=0pt,boxsep=0pt,breakable}

\input{linear}
\def\rpath#1{[#1]}
\def\rfst{\mathsf{fst}\,}
\def\rsnd{\mathsf{snd}\,}
\def\bunfold{\mathbf{unfold}}
\def\bfold{\mathbf{fold}}
\def\susp#1{\langle {#1} \rangle}
\def\ssusp#1{\langle\!\langle {#1} \rangle\!\rangle}
\def\dto{\mathrel{\dot\to}}
\def\dns{{\downarrow}}
\def\ups{{\uparrow}}
\def\sprov{\Vdash}
\def\signat{{\color{red}\Sigma}}
\def\ep#1{\underline{#1}}
\def\wat{\mathbin{@}}
\def\wwat{\mathbin{\underline\wat}}
\def\cn{{:}}
\def\dv{{\div}}
\def\ww{{\mathsf w}}
\def\base{{\textfrak b}}

\def\munit{I}
\def\tt{{\mathsf t}}
\def\ii{{\mathbb I}}
\def\T{\mathbb{T}}
\def\pp{\textsf{\textbf p}}
\def\nn{{\mathsf n}}
\def\PP{\textsf{\textbf P}}
\def\NN{\textsf{\textbf N}}
\def\zz{{\mathsf z}}
\def\sem#1{[\![#1]\!]}
\def\usem#1{\lceil\!\!\lceil{#1}\rceil\!\!\rceil}
\def\zem#1{\langle\!\langle#1\rangle\!\rangle}
\def\col#1{{\mathsf{C}}_{#1}}
\def\lax{\mathop\bigcirc}
\def\rjust{\mathsf{just}}
\def\rcvt{\mathsf{cvt}}
\def\rtype{\mathsf{type}}
\def\rdtype{\mathsf{disc}}
\def\rkind{\mathsf{kind}}
\def\rmode{\mathsf{mode}}
\def\rprel{\mathbf{Prel}}
\def\rrfam{\mathbf{Rfam}}
\def\rset{\mathbf{Set}}
\def\rfinset{\mathbf{FinSet}}
\def\rrel{\mathbf{Rel}}
\def\rcat{\mathbf{Cat}}
\def\rfam{\mathbf{Fam}}
\def\binj{\mathbf{inj}}
\def\rid{\mathsf{id}}
\def\rtrue{\mathrel\mathrm{true}}
\def\pbck{\ar[dr, phantom, pos=0, "\lrcorner"]}
\def\bpbck{\ar[ddrr, phantom, pos=0, "\lrcorner"]}
\def\erule#1#2{\begin{prooftree}#1\justifies #2\end{prooftree}}
\def\lpar{\bindnasrepma}
\def\lamp{\binampersand}
\def\btwo{\mathbf{2}}
\def\bone{\mathbf{1}}

\newarrow {Equiv} =====
\def\FinSet{\mathbf{FinSet}}
\def\x{\times}
\def\maps{\ar[dd,mapsto,shorten <=2em, shorten >=2em]}
\def\mapt{\ar[ddd,mapsto,shorten <=2em, shorten >=2em]}
\def\frommaps{\ar[from=uu,mapsto,shorten <=2em, shorten >=2em, crossing over]}
\def\frommapt{\ar[from=uuu,mapsto,shorten <=2em, shorten >=2em, crossing over]}
\def\C{\mathbf{C}}
\def\G{\mathbf{G}}
\def\S{\mathbf{S}}
\def\D{\mathbf{D}}
\def\E{\mathbf{E}}
\def\M{\mathbf{M}}
\def\pair#1#2{\langle#1,#2\rangle}
\def\ltri{{\lhd}}
\def\rtri{{\rhd}}
\def\tri{\rhd}
\def\o{\circ}
\def\rctx{\,\mathsf{ctx}}
\def\rdctx{\,\mathsf{dctx}}
\def\del{\partial}
\def\also#1{\ \textcolor{blue}{\celse #1}}
\newcounter{nodemaker}
\setcounter{nodemaker}{0}
\def\twocell#1#2{%
  \global\edef\mynodeone{twocell\arabic{nodemaker}}%
  \stepcounter{nodemaker}%
  \global\edef\mynodetwo{twocell\arabic{nodemaker}}%
  \stepcounter{nodemaker}%
  \ar[#1,phantom,shift left=3,""{name=\mynodeone}]%
  \ar[#1,phantom,shift right=3,""'{name=\mynodetwo}]%
  \ar[Rightarrow,#2,from=\mynodeone,to=\mynodetwo]%
}
\def\twocellswap#1#2{%
  \global\edef\mynodeone{twocell\arabic{nodemaker}}%
  \stepcounter{nodemaker}%
  \global\edef\mynodetwo{twocell\arabic{nodemaker}}%
  \stepcounter{nodemaker}%
  \ar[#1,phantom,shift right=3,""{name=\mynodeone}]%
  \ar[#1,phantom,shift left=3,""'{name=\mynodetwo}]%
  \ar[Rightarrow,#2,from=\mynodeone,to=\mynodetwo]%
}
\def\maps{\ar[dd,mapsto,shorten <=2em, shorten >=2em]}
\author{Jason Reed}
\definecolor{fib}{HTML}{ff7f00}
\definecolor{green}{HTML}{007f00}
\definecolor{red}{HTML}{df3f3f}
\definecolor{opfib}{HTML}{007fff}

% For lightening colors I used
% http://trycolors.com/ 6 parts white to 1 part color
\definecolor{opfibl}{HTML}{DBEDFF}
\definecolor{greenl}{HTML}{DBEDDB}
\definecolor{redl}{HTML}{FAE4E4}
\def\njudge#1{\fcolorbox{opfib}{white}{#1}}
\def\nrule#1{\fcolorbox{white}{opfibl}{#1}}
\def\neqn#1{\fcolorbox{white}{redl}{#1}}



\def\judge#1{\vskip 2em\noindent \njudge{$#1$}\vskip 0.5em\noindent}
\def\rule#1{\vskip 2em \noindent \nrule{$#1$}\vskip 0.5em\noindent}
\def\eqn#1{\vskip 2em \noindent \neqn{$#1$}\vskip 0.5em\noindent}
\def\prof{\nrightarrow}
\def\tensor{\otimes}
\def\htensor{\mathrel{\hat\otimes}}
\def\lol{\multimap}
\def\hlol{\mathrel{\hat\multimap}}
\def\wtf{{\color{red}???}}

\def\re{\mathbf{E}}
\def\sh{\sharp}
\def\shp{\mathrel{\sharp}}
\def\zero{\texttt{0}}
\begin{document}

\def\
\tikzset{
   commutative diagrams/.cd,
   arrow style=tikz,
   diagrams={>=stealth}}

\section{Slight Sense of Panic}
Ok, I'm trying to go back and remind myself of which pieces of the foundation are secure
or not. The simply-typed case seems fine, and needs no distinction between $\Gamma$ and
$\Gamma^\op$. I just need to observe that if $\delta \in \Delta$, then also $\bar \delta$
is indeed an object in $\Delta$, and I can say that
\[ (A \to B)_\delta = A_{\bar \delta}\to B_{\delta} \]
and so have a meaningful way to talk about transport, i.e.
\[(A\to B)_\phi(f) = \lambda x . B_\phi(f ( A_{\bar \phi}(x)))\]
If I want to talk about dependency, I can do so in a way that preserves $\Delta$.
It seems like there must be some operation $\dag$ on contexts that means
context formation can go like
\[
\erule
{\Delta \prov \Gamma \rctx \qquad \Delta;\dag \Gamma \prov A : \rtype}
{\Delta \prov \Gamma, x - A \rctx}
\]
to match what happens when all the $\Gamma$s drop away in the simply-typed case.
The choice --- at this early point in the development --- of whether $A$ or $\bar A$
appears up top is simply a matter of convention, isn't it? A well-formed context
written using the other rule would have the same data in a different form.

Ok, so if I stare at the $\Pi$ formation rule
\[
\erule
{\Delta ; \dag \Gamma\prov A : \rtype \qquad \Delta ; \Gamma, x - A \prov B : \rtype}
{\Delta ; \Gamma \prov (x : A) \to B : \rtype}
\]
it's again without any loss that I just pass $A$ up to the right without any modification.
I know in my gut I'm going to want a $-$ assumption, but it's the semantics that will justify
it. The first premise is forced by the second.

Ok, let's barge into the semantics, then. We hope for
\begin{declbox}
  $\Gamma_\phi : \rtype$.\\
  $\Gamma_\tau : \Gamma_{\psi} \to \Gamma_{\phi} $\\
  $A_\delta : \Gamma_\delta \to \rtype$\\
  $A_\phi : (g : \Gamma_\phi) \to A_{\delta}(\Gamma_{\ltri\phi} g) \to A_{\epsilon}(\Gamma_{\rtri\phi} g)$
\end{declbox}

and we think $((x : A) \to B)_\delta$ is going to have the rough shape
that it did for the simply-typed case. An nontrivial conjecture that might
be worth questioning, but which nicely forces
some other things into place is that {\em we don't have to do any transport}
in the definition of the object part. Given that conjecture,
we basically have to write
\begin{defnbox}
  $((x : A) \to B)_\delta(g) = (x : A_{\bar \delta}(g)) \to B_{\delta}(g, x)$
\end{defnbox}
and for this we need
\begin{thmbox}
$(\dag \Gamma)_{\bar \delta} = \Gamma_\delta$\\
$(\Gamma, x - A)_{ \delta} = (g : \Gamma_\delta) \x (A_{\bar\delta} (g))$
\end{thmbox}
And for the morphism part we'd say
\[((x : A) \to B)_\phi : (g : \Gamma_\phi) \to ((x : A_{\bar \delta}(\Gamma_{\ltri \phi} g)) \to B_{\delta}(\Gamma_{\ltri \phi} g, x))
\]\[\to ((x : A_{\bar \epsilon}(\Gamma_{\rtri \phi} g)) \to B_{\epsilon}(\Gamma_{\rtri \phi} g, x))\]
and here we're in much the same situation as before.

We know we have
\[A_{\bar \phi} : (g : (\dag\Gamma)_{ \bar\phi}) \to A_{\bar \epsilon}((\dag \Gamma)_{\ltri\bar\phi} g) \to A_{\bar \delta}((\dag\Gamma)_{\rtri\bar\phi} g) \]
which if we assume
(the reason that I don't want to choose
$(\dag \Gamma)_{\bar\phi} = \Gamma_{\bar \phi}$ is that I need able to transport
from $(\dag \Gamma)_{\bar\phi}$ to $\Gamma_\delta$ and $\Gamma_\epsilon$, not
to  $\Gamma_{\bar \delta}$ and $\Gamma_{\bar \epsilon}$)
\begin{thmbox}
$(\dag \Gamma)_{\bar\phi} = \Gamma_\phi$\\
$(\dag \Gamma)_{\bar\tau} = \Gamma_\tau$
\end{thmbox}
means
\[A_{\bar \phi} : (g : \Gamma_{\phi}) \to A_{\bar \epsilon}( \Gamma_{\rtri\phi} g) \to A_{\bar \delta}(\Gamma_{\ltri\phi} g) \]
and then we also have
\[B_\phi : (g : (\Gamma, x - A)_\phi) \to B_\delta((\Gamma, x - A)_{\ltri \phi} g)
\to B_\epsilon((\Gamma, x - A)_{\rtri \phi} g)\]
which we need to make a guess about how $\phi$ works on negative contexts. So we
guess
\begin{defnbox}
  $ (\Gamma, x - A)_{ \phi} = (g : \Gamma_\phi) \x (A_{\bar\epsilon} (\Gamma_{\rtri \phi} g)) $
\end{defnbox}
Note the guess of $\epsilon$ (that is, $A_{\bar\epsilon} (\Gamma_{\rtri \phi} g)$)
not $\delta$ (that is, $A_{\bar\delta} (\Gamma_{\ltri \phi} g)$) here! Even though it
otherwise seems like we might have a choice when picking something
that reduces to the special case $(\Gamma, x - A)_\delta = (g :
\Gamma_{\rid_\delta}) \x A_{\bar \delta}(g)$ that we already had motivated.
(this being since both the domain and codomain of $\rid_\delta$ are
$\delta$) This is very important, I think! Remember from
\texttt{2018-05-23.ltx}, when I said
\begin{quote}
 A critical thing seems to be that when computing
the meaning of $B_\phi$, I get that the second argument $(\_ : A_{\bar \epsilon}(\cdots))$
of it is at something $\epsilon$-like, so that I can plug in the naked variable $x$ that
I have, rather than some transport expression. When I substitute a transport expression,
then I get some constraint that has to be satisfied for the result to be well-typed.
\end{quote}
Now we know that $B$-transport expands to
\[B_\phi : (g : \Gamma_\phi) (a : A_{\bar\epsilon} (\Gamma_{\rtri \phi} g)) \to B_\delta((\Gamma, x - A)_{\ltri \phi} (g, a))
\to B_\epsilon((\Gamma, x - A)_{\rtri \phi} (g, a))\]
and we have to remember how to actually do context transport at minus assumptions.
In a situation like
\begin{tikzcd}
  \delta' \ar[d, "\iota"'] \ar[r, "\psi"] & \epsilon'\ar[from=d, "\omega"']\\
\delta \ar[r,"\phi"'] & \epsilon
\end{tikzcd}
we need to make
\[ (\Gamma, x - A)_\tau :
(g : \Gamma_\psi) (a : A_{\bar\epsilon'} (\Gamma_{\rtri \psi} g))
\to
(k : \Gamma_\phi) \x (A_{\bar\epsilon} (\Gamma_{\rtri \phi} k))\]
so we set
\begin{defnbox}
  $(\Gamma, x - A)_\tau(g, a) = (\Gamma_\tau g, A_{\bar \omega}(\Gamma_{\rtri\tau}g ,a))$
\end{defnbox}
Let's type-check this;
\[A_{\bar \omega} : (g : \Gamma_{\omega}) \to A_{\bar \epsilon'}( \Gamma_{\rtri\omega} g) \to A_{\bar \epsilon}(\Gamma_{\ltri\omega} g) \]
Means that all we need are $\rtri \omega \o \rtri \tau = \rtri \psi$
to make the $A_{\bar \epsilon'}$ match, and
$\ltri \omega \o \rtri \tau = \rtri \phi \o \tau$
to make the $A_\epsilon$ match. But these both hold.

Now we know that $B$-transport expands to
\[B_\phi : (g : \Gamma_\phi) (a : A_{\bar\epsilon} (\Gamma_{\rtri \phi} g)) \to
 B_\delta(\Gamma_{\ltri \phi} g, A_{\bar \phi}(g ,a) )
\to B_\epsilon(\Gamma_{\rtri \phi} g, a)\]

Now we know the morphism part must be
\[((x : A) \to B)_\phi : (g : \Gamma_\phi) \to ((x : A) \to B)_{\delta}(\Gamma_{\ltri\phi} g) \to ((x : A) \to B)_{\epsilon}(\Gamma_{\rtri\phi} g)\]
i.e.
\[((x : A) \to B)_\phi : (g : \Gamma_\phi) \to ((x : A_{\bar \delta}(\Gamma_{\ltri\phi} g)) \to B_{\delta}(\Gamma_{\ltri\phi} g, x)) \]
\[\to (x : A_{\bar \epsilon}(\Gamma_{\rtri\phi} g)) \to B_{\epsilon}(\Gamma_{\rtri\phi} g, x)\]
and we define it
\begin{defnbox}
  $((x : A)\to B)_\phi(g, f) = \lambda x . B_\phi((g, x), f (A_{\bar \phi}(g, x)))$
\end{defnbox}
Armed with
\[A_{\bar \phi} : (g : \Gamma_{\phi}) \to A_{\bar \epsilon}( \Gamma_{\rtri\phi} g) \to A_{\bar \delta}(\Gamma_{\ltri\phi} g) \]
\[B_\phi : (g : \Gamma_\phi) (a : A_{\bar\epsilon} (\Gamma_{\rtri \phi} g)) \to
 B_\delta(\Gamma_{\ltri \phi} g, A_{\bar \phi}(g ,a) )
\to B_\epsilon(\Gamma_{\rtri \phi} g, a)\]
we check
\[g : \Gamma_\phi, x : A_{\bar \epsilon}(\Gamma_{\rtri \phi} g) \prov A_{\bar \phi}(g, x) : A_{\bar\delta}(\Gamma_{\ltri \phi} g) \]
\[\cdots, f : (x : A_{\bar \delta}(\Gamma_{\ltri\phi} g)) \to B_{\delta}(\Gamma_{\ltri\phi} g, x)
 \prov f(A_{\bar \phi}(g, x)) : B_{\delta}(\Gamma_{\ltri\phi} g, A_{\bar \phi}(g, x)) \]
And everything's fine.

\section{Popping Up}
Summarizing definitions and theorems, we have
\begin{thmbox}
$(\dag \Gamma)_{\bar\phi} = \Gamma_\phi$\\
$(\dag \Gamma)_{\bar\tau} = \Gamma_\tau$
\end{thmbox}
\begin{defnbox}
  $ (\Gamma, x - A)_{ \phi} = (g : \Gamma_\phi) \x (A_{\bar\epsilon} (\Gamma_{\rtri \phi} g)) $\\
  $(\Gamma, x - A)_\tau(g, a) = (\Gamma_\tau g, A_{\bar \omega}(\Gamma_{\rtri\tau}g ,a))$\\
  $((x : A) \to B)_\delta(g) = (x : A_{\bar \delta}(g)) \to B_{\delta}(g, x)$\\
  $((x : A)\to B)_\phi(g, f) = \lambda x . B_\phi((g, x), f (A_{\bar \phi}(g, x)))$
\end{defnbox}
I think going through the same above business with $\Sigma$s would motivate the correct
definitions for plus assumptions, which I'm just going to be sloppy and think that they
are
\begin{defnbox}
  $ (\Gamma, x + A)_{ \phi} = (g : \Gamma_\phi) \x A_{\delta} (\Gamma_{\ltri \phi} g) $\\
  $ (\Gamma, x - A)_{ \phi} = (g : \Gamma_\phi) \x A_{\bar\epsilon} (\Gamma_{\rtri \phi} g) $\\
  $(\Gamma, x + A)_\tau(g, a) = (\Gamma_\tau g, A_{\iota}(\Gamma_{\ltri\tau}g ,a))$\\
  $(\Gamma, x - A)_\tau(g, a) = (\Gamma_\tau g, A_{\bar \omega}(\Gamma_{\rtri\tau}g ,a))$\\
\\
  $((x : A) \to B)_\delta(g) = (x : A_{\bar \delta}(g)) \to B_{\delta}(g, x)$\\
  $((x : A) \x B)_\delta (g) = (x : A_\delta(g)) \x B_\delta(g, x)$\\
  $((x : A)\to B)_\phi(g, f) = \lambda x . B_\phi((g, x), f (A_{\bar \phi}(g, x)))$\\
  $((x : A)\x B)_\phi(g, (a, b)) = (A_\phi(g, a), B_\phi((g, a), b))$
\end{defnbox}
Now at this point I'm finally ready to conjecture that in this setting,
 $\dag \Gamma$ is {\em just} the
sign-flipping operation, with no variable substitutions from ${}^\op$ping $\Delta$.

Let's try to prove the the above theorems, then.

The object part works like
\[(\dag (\Gamma, x + A))_{ \bar \phi} = (\dag \Gamma, x - A)_{\bar \phi}\]
\[= (\dag \Gamma, x - A)_{\bar \phi}\]
\[ = (g : (\dag \Gamma)_{\bar\phi}) \x A_{\delta} ((\dag \Gamma)_{\rtri \bar\phi} g)\]
\[ = (g :  \Gamma_\phi) \x A_{\delta} (\Gamma_{\ltri \phi} g)\]
\[= (\Gamma, x + A)_{  \phi} \]
And the morphism part works like
\[(\dag (\Gamma, x + A))_{ \bar \tau}(g, a) = (\dag \Gamma, x - A)_{\bar \tau}(g, a)\]
\[ = ((\dag \Gamma)_{\bar\tau} g, A_{\iota}((\dag \Gamma)_{\rtri{\bar\tau}}g ,a)) \]
\[ = ( \Gamma_{\tau} g, A_{\iota}(\Gamma_{\ltri\tau}g ,a)) \]
\[ = (\Gamma, x + A)_\tau(g,a) \]
Ok, so everything's fine there.

Let's fix in our minds the notion that $\Gamma^\op$ flips sign and $\bar \Gamma$ flips variables. Define another notion of context, $\rctx'$, where I simply
{\em write} the minus-type assumptions as $\bar A$ instead of $A$.
That is, I intend the theorem
\begin{thmbox}
  $\Delta \prov \Gamma \rctx \qquad\iff \qquad \Delta\prov \tilde \Gamma  \rctx'$
\end{thmbox}
where $\tilde \Gamma$ selectively changes $x - A$ to $x - \bar A$ but leaves $x + A$
alone.

I know I have the rule
\[
\erule
{\Delta \prov \Gamma \rctx \qquad \Delta;\Gamma^\op \prov A : \rtype}
{\Delta \prov \Gamma, x - A \rctx}
\]
So do I then have the rule
\[
\erule
{\Delta \prov \Gamma \rctx' \qquad \Delta;{\bar \Gamma}^\op \prov \bar A : \rtype}
{\Delta \prov \Gamma, x - A \rctx'}
\]
?
Expanding the `definition' of $\rctx'$, this is the claim
\[
\erule
{\Delta \prov \tilde \Gamma \rctx \qquad \Delta;{\bar \Gamma}^\op \prov \bar A : \rtype}
{\Delta \prov \tilde \Gamma, x - \bar A \rctx}
\]
which doesn't seem obviously provable, which is very weird. Oh, but maybe
I just need some other induction hypothesis, or something.
Let's be clear about the different types of things involved.
I have contexts $\Gamma$ with
\[
\erule
{\Delta \prov \Gamma \rctx \qquad \Delta;\Gamma^\op \prov A : \rtype}
{\Delta \prov \Gamma, x - A \rctx}
\]
and a judgment $\Delta ; \Gamma \prov A : \rtype$. Then I have things $\Omega$
with
\[
\erule
{\Delta \prov \Omega \rctx' \qquad \Delta;{\bar \Omega}^\op \prov \bar A : \rtype}
{\Delta \prov \Omega, x - A \rctx'}
\]
And I guess there is a corresponding judgment $\Delta ; \Omega \prov A : \rtype$.
The putative relationship between them is
\[ \Delta ; \Omega \prov A : \rtype \iff \Delta ;\tilde\Omega \prov A : \rtype\]
\[ \Delta ; \Gamma \prov A : \rtype \iff \Delta ;\tilde\Gamma \prov A : \rtype\]
So I want to prove
\[ \Delta \prov \Gamma \rctx \qquad\iff \qquad \Delta\prov \tilde \Gamma  \rctx'\]
inductively, so let's take the case
\[ \Delta \prov (\Gamma, x - A) \rctx \qquad\iff \qquad \Delta\prov (\tilde \Gamma, x - \bar A)  \rctx'\]
and unpacking we get
\[
\erule
{ \Delta \prov \Gamma \rctx\qquad \Delta ; \Gamma^\op \prov A : \rtype}
{ \Delta \prov (\Gamma, x - A) \rctx}
\ \iff\ %
\erule
{ \Delta \prov \tilde\Gamma \rctx'\qquad \Delta ; \overline{\tilde\Gamma}^\op \prov \bar A : \rtype}
{   \Delta\prov (\tilde \Gamma, x - \bar A)  \rctx'}
\]
This isn't working, either? Weird. Ok, let's just start with
\[
\erule
{\Delta \prov \Gamma \rctx \qquad \Delta;\Gamma^\op \prov A : \rtype}
{\Delta \prov \Gamma, x - A \rctx}
\]
and assert the theorem and rewrite. I get
\[
\erule
{\Delta \prov \tilde\Gamma \rctx' \qquad \Delta;\tilde\Gamma^\op \prov A : \rtype}
{\Delta \prov \tilde\Gamma, x - \bar A \rctx'}
\]
which means
\[
\erule
{\Delta \prov \Omega \rctx' \qquad \Delta;\Omega^\op \prov \bar A : \rtype}
{\Delta \prov \Omega, x - A \rctx'}
\]
where ${}^\op$ is {\em only} sign-flipping. Wow, I am super confused.

Let me at least examine the thing I have. It's essential that
$\dagger$ --- which I'm taking to be ${}^\op$ --- is an operation that
preserves context validity in the same $\Delta$.

Given the rules
\[
\erule
{\Delta \prov \Gamma \rctx \qquad \Delta;\Gamma \prov A : \rtype}
{\Delta \prov \Gamma, x + A \rctx}
\qquad
\erule
{\Delta \prov \Gamma \rctx \qquad \Delta;\Gamma^\op \prov A : \rtype}
{\Delta \prov \Gamma, x - A \rctx}
\]
I should be able to prove
\begin{thmbox}
  if $\Delta \prov \Gamma\rctx$, then $\Delta \prov \Gamma^\op\rctx$
\end{thmbox}
\noindent for ${}^\op$ just flipping signs.

If \[\erule
{\Delta \prov \Gamma \rctx \qquad \Delta;\Gamma \prov A : \rtype}
{\Delta \prov \Gamma, x + A \rctx}
\]
we can show
\[\erule
{\Delta \prov \Gamma^\op \rctx \qquad \Delta;\Gamma \prov A : \rtype}
{\Delta \prov \Gamma^\op, x - A \rctx}\]
and if
\[\erule
{\Delta \prov \Gamma \rctx \qquad \Delta;\Gamma^\op \prov A : \rtype}
{\Delta \prov \Gamma, x - A \rctx}\]
then we can show
\[
\erule
{\Delta \prov \Gamma^\op \rctx \qquad \Delta;\Gamma^\op \prov A : \rtype}
{\Delta \prov \Gamma^\op, x + A \rctx}
\]
\section{Revisiting the Variable Rule}
The real crux of everything I think is when
\[ \alpha : \C  \prov A : \rtype \]
 can I make some term
\[ \alpha : \C ; x - A  \prov x^* : A \]
Recall that a term $\Delta ; \Gamma \prov M : A$ needs to have
$M_d : (g : \Gamma_{dd}) \to A_{dd}(g)$ for every object $d\in \ssem \Delta$, subject to
\[ (g : \Gamma_{ff}) \to A_{fd}(\Gamma_{\rtri \fdom}g, M_d(\Gamma_{\fdom}g )) \equiv A_{ef}(\Gamma_{\rtri \fcod} g, M_e( \Gamma_{\fcod} g))\]

So the object part is forced. $(x^*)_d(a) = a$.
In this case, the type $A$ does not depend on anything, so we're just concerned with checking
\[ (g : \Gamma_{ff}) \to A_{fd}( M_d(\Gamma_{\fdom}g )) \equiv A_{ef}( M_e( \Gamma_{\fcod} g))\]

In this case $\Gamma = x - A$, so $\Gamma_{ff} = A_{de}$, and $\Gamma_{\fdom} = A_{df}$
and $\Gamma_{\fcod} = A_{fe}$, so we have to check
\[ (g : \Gamma_{de}) \to A_{fd}( M_d(A_{df}g )) \equiv A_{ef}( M_e( A_{fe} g))\]
so, yeah, everything's kosher here so far. I still fear higher dependencies.

\subsection{Quick Check of Variable Rule}
Actually, things are looking more or less ok now?
\\ \vskip 1em
Let's say in the  negative substitution principle
\begin{thmbox}
  If $\Delta ; \Gamma^\op \prov M :  A$ and $\Delta ; \Gamma, x - A \prov J$, then
  $\Delta ; \Gamma \prov [M/x]J$.
\end{thmbox}
our $J$ is
\[\erule
{
x -  A \in \Gamma
}
{\Delta ; \Gamma \prov x^* : A^* }
\]
Then we have indeed that
$\Delta; \Gamma \prov M^* : A^*$.
\subsection{Check that star operation can be synthesized}
I want to convince myself that we could sort of define the operation
$\dash^*$ as an explicit substitution, even though doing this check
will look sort of circular, as we need to use $\dash^*$ to define what
explicit substitutions are. Nonetheless, while not a reasonable way to
{\em define} things, it serves as a nice sanity-check.

In general we'd have
\[
\erule
{\Gamma \prov \theta : \Gamma' \qquad \Delta;\Gamma \prov M : \theta A}
{\Gamma \prov \theta[M/x] : \Gamma', x + A}
\qquad
\erule
{\Gamma \prov \theta : \Gamma' \qquad \Delta;\Gamma^\op \prov M : (\theta (A^*))^*}
{\Gamma \prov \theta[M/x] : \Gamma', x - A}
\]
and specifically for the substitution $\Gamma \prov \theta_\Gamma : \Gamma^\op$
that's meant to have the property that $\theta_\Gamma J = J^*$, we inductively
define
\[\theta_{\Gamma, x \pm A} = \theta_\Gamma[x^*/x]\]
so that
\[
\erule
{\Gamma \prov \theta_\Gamma : \Gamma^\op \qquad \Delta;\Gamma, x - A \prov x^* :  A^*}
{\Gamma, x - A \prov \theta_\Gamma[x^*/x] : \Gamma^\op, x + A}
\qquad
\erule
{\Gamma \prov \theta_\Gamma : \Gamma^\op \qquad \Delta;\Gamma^\op, x - A \prov x^* : A^*}
{\Gamma, x + A \prov \theta_\Gamma[x^*/x] : \Gamma^\op, x - A}
\]

\section{Trying Small Examples}

Ok, so I could do
\[\alpha : \C ; x - A \prov x^* : A\]
What's one harder than that? Something like
\[\alpha : \C ; x + A, y - B \prov y : B^*\]
maybe? For this I want
\[\alpha : \C ; x - A \prov B : \rtype\]
so hopefully somehow
\[\alpha : \C ; x + A \prov B^* : \rtype\]
Oh, this seems really sketchy.
Anyway, the stuff I have available looks like
for $\phi : \delta \to \epsilon \in \sem \Delta$,
\[A_{\bar \delta} :  \rtype \]
\[B_\delta : A_{\bar \delta} \to \rtype \]
\[A_{\bar \phi} :  A_{\bar \epsilon} \to A_{\bar \delta}  \]
\[B_\phi : (a : A_{\bar \epsilon} ) \to B_\delta( A_{\bar \phi}(a))
\to B_\epsilon( a)\]
So what could $B^*$ possibly be? We know
\[B^*_\delta : A_\delta \to \rtype\]
So like...
\[B^*_\delta(a) = B_{\bar\delta}(a)\]
? Now we have to define
\[B^*_\phi : (a : A_{\delta} ) \to B_\delta( a) \to B_\epsilon(A_\phi(a)) \]
So what can we possibly set this to? We have
\[B_{\bar\phi} : (a : A_{\delta} ) \to B_{\bar\epsilon}( A_{ \phi}(a))
\to B_{\bar\delta}( a)\]
And this is the mess that I'm in.

\subsection{I do need this, don't I?}

If I try to prove
\[  \prov P \imp P \]
this focalizes to
\[  \prov \dns(P \imp \ups P) \wat 1 \]
then
\[  \prov \forall \phi . (P \imp \ups P) \wat \phi \to 1 \rtri \phi \]
\[   (P \imp \ups P) \wat \phi \prov 1 \rtri \phi \]
\[   P \wat \alpha , \ups P \wat \psi  \prov 1 \rtri (\alpha \lol \psi) \]
\[   P \wat \alpha,  \ups P \wat \psi  \prov \alpha \rtri \psi \]
\[   P \wat \alpha,  (\forall \beta .  P \wat \beta \to \beta \rtri \psi)  \prov \alpha \rtri \psi \]
So, yeah, $P$ certainly occurs with both polarities. I have to deal with this
\subsection{Can I escape it by fixing the lambda rule}
So let the formation rule stay
\[
\erule
{\Delta ; \Gamma^\op \prov A : \rtype \qquad \Delta ; \Gamma, x - A \prov B : \rtype }
{\Delta ; \Gamma \prov (x : A) \to B : \rtype}
\]
Can I, like, define a twisted version of the term judgment so I could say like
\[
\erule
{\Delta ; \Gamma^\op, x + A \prov M - B }
{\Delta ; \Gamma \prov \lambda x . M :  (x : A) \to B }
\]
where $B$ is still a type in $\Gamma$, but $M$ is a term in $\Gamma^\op, x + A$
so that it can really use $x$?

This suggests that I have the same object part requirement --- recall that I had
no apparent problem defining the object part even for types involving minus
assumptions --- but somehow a dual requirement for the end.

Oh no, but then what does the minus variable rule look like? It can't be
\[
\erule
{}
{\Delta ; \Gamma, x +  A \prov x - A}
\]
because then the type invariants don't work out.
But it can't be
\[
\erule
{}
{\Delta ; \Gamma, x -  A \prov x - A}
\]
because then I can't use it with the lambda rule.



\[
\erule
{\Delta ; \Gamma^\op, x + A \prov M - A }
{\Delta ; \Gamma \prov \lambda x . M :  (x : A) \to A }
\]

The real problem is that I can't even always form $A \to A$ for types
$A$ depending on variables.

\section{Attacking This From Orbit}

Ok, let's step way back. The reason I think that anything like this at
all should work comes from squinting at the cartesian closed structure
on presheaf categories
\[(A \imp B)(C) = \C[yC \x A, B]\]
and feeling that that $yC \x \dash$ looks a lot like a shift operator.
Let's recall the way in which the shift is connected to that kind of kripke
quantification.

If I have an unpolarized implication like $A\imp B$, I can polarize it
in a way where every proposition is translated to a positive,
and one clause of that translation is

\[(A \imp B)^+ = \dns (A^+ \imp \ups B^+)\]

In the resource-passing semantics, this positive proposition takes
as an argument a positive resource, so I get something like

\[\lambda \gamma .\quad \forall \phi .  (A^+ \lol \ups B^+) \wat \phi \to \gamma \tri \phi\]
\[= \lambda \gamma .\quad \forall \phi .  (\exists \alpha \psi . A^+ \wat \alpha \x \ups B^+ \wat \psi \x R_\lol(\alpha, \psi, \phi)) \to \gamma \tri \phi\]
\[= \lambda \gamma .\quad \forall \phi  \alpha \psi . A^+ \wat \alpha \to \ups B^+ \wat \psi \to R_\lol(\alpha, \psi, \phi) \to \gamma \tri \phi\]

Now comes a stage where I have a lot of wiggle room, and I can pick the categories
in which $\phi, \psi$ and $\alpha, \gamma$ live in, and I can pick $R_\lol$ and $\tri$
to be whatever I want. Let's work from the other direction, then and see what I need.
I know I want to end up at

\[ \lambda \gamma .\quad \forall  \alpha . \alpha \le \gamma  \to A^+ \wat \alpha \to  B^+ \wat \alpha \]

The $\gamma$ plays the role of $C$, the yoneda embedding is the $\le$, and the $\alpha$
is the object argument of the natural transformation. Now I remember from doing
this calcluation previously that I'm actually going to end up with
\[ \lambda \gamma .\quad \forall  \alpha . \alpha \le \gamma  \to A^+ \wat \alpha \to
\dns \ups  B^+ \wat \alpha \]
which is fine. That expands to
\[ \lambda \gamma .\quad \forall  \alpha \psi . \alpha \le \gamma  \to A^+ \wat \alpha \to
 \ups  B^+ \wat \psi \to  \alpha \tri \psi \]
So it looks like we just have to ensure that
\[\alpha \le \gamma \to \alpha \tri \psi  \equiv \forall \phi . R_{\lol}(\alpha \psi \phi) \to \gamma \tri \phi\]

And I also sort of remember from doing this calculation previously
that I set $\phi = (\bar\phi, \phi_0)$ and $\psi = (\bar\psi, \psi_0)$
where $\bar\phi$ is of the same syntactic sort as $\alpha, \beta, \gamma$, etc.,
and $\phi_0$ is its own thing, with some abstract
predicate $\#(\phi_0)$, and I defined
\[ \alpha \tri (\bar\phi, \phi_0) = \alpha \sim \bar\phi \to \#(\phi_0) \]
for $\sim$ being either $\le$ or $\ge$. Doing that changes my goal to
\[\alpha \le \gamma \to \alpha \sim \bar\psi \to \#(\psi_0) \equiv \forall \phi . R_{\lol}(\alpha \psi \phi) \to \gamma \sim \bar\phi \to \#(\phi_0)\]
So definitely $R_\lol(\alpha(\bar\psi\psi_0)(\bar\phi\phi_0))$ should at least
be of the form $\psi_0 = \phi_0 \x R'(\alpha\bar\psi\bar\phi)$ so that I get
\[\alpha \le \gamma \to \alpha \sim \bar\psi \to \#(\psi_0) \equiv \forall \bar\phi . R'(\alpha \bar\psi \bar\phi) \to \gamma \sim \bar\phi \to \#(\psi_0)\]
and if I choose the right orientation of $\sim$, then I should be able to yoneda-reduce
the right-hand-side and get merely
\[\alpha \le \gamma \to \alpha \sim \bar\psi \to \#(\psi_0) \equiv R'(\alpha \bar\psi \gamma)  \to \#(\psi_0)\]
So that forces my hand to set
\[R'(\alpha \bar\psi\gamma) =  \alpha \le \gamma \x \alpha \sim \bar\psi\]

So let's set about carefully determining which way $\psi$ is oriented. The idea
is that $\alpha, \beta, \gamma$ etc. come from some category $\C$, and $\alpha \le \beta$ is
secretly $\C[\alpha, \beta]$. I've oriented the $\le$ that I did stick in to coincide
with the yoneda embedding $\C \to \hat\C$. That is, $A^+$ and $B^+$ are {\em contravariant}
functoris in their argument, and so too is $\dash \le \gamma$. So this suggests that
$\sim$ is $\le$, so that $R'$ can consistently be a contravariant functor in $\alpha$,
and covariant in the other two arguments.

\subsection{The Story}
Let's try this from the top, then. We start with two syntactic sorts
\[\begin{tabular}{r@{ }c@{ }c@{ }l}
  Pos. Resources&$r^+$&$::=$&$\alpha \celse \cdots$\\
  Neg. Resources&$r^-$&$::=$&$\phi \celse \cdots$\\
\end{tabular}\]
and we want to compile them away into
\[\begin{tabular}{r@{ }c@{ }c@{ }l}
  Kripke worlds&$c$&$::=$&$ \cdots$\\
  Nonce worlds&$\zeta$&$::=$&$ \cdots$\\
\end{tabular}\]

We postulate a relation $c \le c'$ which is the hom of $\C$. We postulate
some arbitrary unary relation $\#(\zeta)$. We say that every $r^+$ is actually
a $c$, and we say every $r^-$ is actually a pair $(c, \zeta)$.
The relation $r^+ \tri r^-$ is defined by
\[
\erule
{c \le c' \to \#(\zeta)}
{c \tri (c', \zeta)}
\]
If I have categories $\PP$ and $\NN$, so that positive and negative
props are interpreted as $\PP \to \rset$ and $\NN \to \rset$,
 they would be like $\C$ and $\C^\op \x Z$ for the mere set $Z$ the $\zeta$s live in.
And $\rtri : \PP \x \NN \to \rset$.
 And now
we define the profunctor-like $R : \PP^? \x \NN^? \x \NN^? \to \rset$ by
\[
\erule
{(c \le c') \x (c \le c'')}
{R(c, (c', \zeta), (c'', \zeta))}
\]
So given $c : \C$, we compute
\[\dns (A^+ \lol \ups B^+) \wat c = \forall (c', \zeta) .  (A^+ \lol \ups B^+) \wat (c', \zeta) \to c \tri (c', \zeta) \]
\[ = \forall (c', \zeta) .  (A^+ \lol \ups B^+) \wat (c', \zeta) \to c \le c' \to \#( \zeta) \]
\[ = \forall (c', \zeta) .  (\exists c_2 c_3 \zeta'. (A^+ \wat c_2) \x (\ups B^+ \wat (c_3, \zeta')) \x R(c_2 (c_3 \zeta') (c' \zeta))) \to c \le c' \to \#( \zeta) \]
\[ = \forall c' \zeta c_2 c_3 \zeta'. (A^+ \wat c_2) \to (\ups B^+ \wat (c_3, \zeta')) \to R(c_2 (c_3 \zeta') (c' \zeta)) \to c \le c' \to \#( \zeta) \]
\[ = \forall c' \zeta c_2 c_3 . (A^+ \wat c_2) \to (\ups B^+ \wat (c_3, \zeta)) \to R(c_2 (c_3 \zeta) (c' \zeta)) \to c \le c' \to \#( \zeta) \]
\[ = \forall c' \zeta c_2 c_3 . (A^+ \wat c_2) \to (\ups B^+ \wat (c_3, \zeta)) \to c_2 \le c_3 \to c_2 \le c' \to c \le c' \to \#( \zeta) \]
\[ = \forall c' \zeta c_2 c_3 . (A^+ \wat c_2) \to (\ups B^+ \wat (c_3, \zeta)) \to c_2 \le c_3 \to c_2 \le c' \to c \le c' \to \#( \zeta) \]
???
\[ = \forall c' . c \le c' \to (A^+ \wat c') \to \forall (c*, \zeta) . \ups B^+ \wat (c*, \zeta) \to c' \le c* \to \# (\zeta) \]
\[ = \forall c' . c \le c' \to (A^+ \wat c') \to \forall (c*, \zeta) . \ups B^+ \wat (c*, \zeta) \to c' \tri (c*, \zeta) \]
\[ = \forall c' . c \le c' \to (A^+ \wat c') \to \dns \ups B^+ \wat c' \]

\subsection{Trying Again}
Roman letters are objects of $\C$. The hom of $\C$ is $\le$. Frames are pairs $(c, \zeta)$.
$a \tri (b, \zeta)$ is defined as $a R b \to \#\zeta$ for some $R$. The relation $[a(b\zeta)(c\zeta')]$ is defined god only knows how, except that I know it requires $\zeta = \zeta'$.
$A$ and $B$ are positive propositions.

I start with the knowledge that I want the calculation to run like
\[\dns (A \lol \ups B) \wat c  \]
\[= \forall a \zeta . (A \lol \ups B) \wat (a, \zeta) \to  c \tri (a, \zeta) \]
\[= \forall a \zeta . (A \lol \ups B) \wat (a, \zeta) \to  c R a \to \# \zeta \]
\[= \forall a \zeta . (\exists f e \zeta' . A \wat f \x  \ups B \wat (e, \zeta') \x [f(e\zeta')(a, \zeta)]) \to  c R a \to \# \zeta \]
\[= \forall a \zeta  f e \zeta' . A \wat f \to  \ups B \wat (e, \zeta') \to [f(e\zeta')(a, \zeta)] \to  c R a \to \# \zeta \]
\[= \forall a \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta) \to [fea] \to  c R a \to \# \zeta \]
\[ ??? \]
\[= \forall d b \zeta  .   A \wat d \to
\ups B \wat (b, \zeta) \to c \le d \to d R b \to \# \zeta \]
\[= \forall d . c \le d \to  A \wat d \to \forall b \zeta .
\ups B \wat (b, \zeta) \to  d R b \to \# \zeta \]
\[= \forall d . c \le d \to  A \wat d \to \forall b \zeta .
\ups B \wat (b, \zeta) \to  d \tri (b, \zeta) \]
\[= \forall d . c \le d \to  A \wat d \to \dns \ups B \wat d \]

One attempt is to think that I can yoneda away the $cRa$ and make it look like
\[\dns (A \lol \ups B) \wat c  \]
\[= \forall a \zeta . (A \lol \ups B) \wat (a, \zeta) \to  c \tri (a, \zeta) \]
\[= \forall a \zeta . (A \lol \ups B) \wat (a, \zeta) \to  c R a \to \# \zeta \]
\[= \forall a \zeta . (\exists f e \zeta' . A \wat f \x  \ups B \wat (e, \zeta') \x [f(e\zeta')(a, \zeta)]) \to  c R a \to \# \zeta \]
\[= \forall a \zeta  f e \zeta' . A \wat f \to  \ups B \wat (e, \zeta') \to [f(e\zeta')(a, \zeta)] \to  c R a \to \# \zeta \]
\[= \forall a \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta) \to [fea] \to  c R a \to \# \zeta \]
\[= \forall  \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta) \to [fec] \to \# \zeta \eqno(y)\]
\[ ??? \]
\[= \forall f e \zeta  .   A \wat f \to
\ups B \wat (e, \zeta) \to c \le f \to f R e \to \# \zeta \]
\[= \forall f . c \le f \to  A \wat f \to \forall e \zeta .
\ups B \wat (e, \zeta) \to  f R e \to \# \zeta \]
\[= \forall f . c \le f \to  A \wat f \to \forall e \zeta .
\ups B \wat (e, \zeta) \to  f \tri (e, \zeta) \]
\[= \forall f . c \le f \to  A \wat f \to \dns \ups B \wat f \]
This seems to leave me only with incoherent choices for how to orient $R$.
Is it possible that I should instead yoneda away something else? Could it be that
the $a$ yoneda-unifies with something else and the $cRa$ becomes the thing in the Kripke
expression? Let's try that. I have two options, for it to unify with $f$ or with $e$.
\subsubsection{Trying $a$ unifying with $f$}
We set $[fea] = aSf \x eTf$ for some relations $S$ and $T$. With
$S$ being whatever orientation causes yoneda-unification.
\[\dns (A \lol \ups B) \wat c  \]
\[= \forall a \zeta . (A \lol \ups B) \wat (a, \zeta) \to  c \tri (a, \zeta) \]
\[= \forall a \zeta . (A \lol \ups B) \wat (a, \zeta) \to  c R a \to \# \zeta \]
\[= \forall a \zeta . (\exists f e \zeta' . A \wat f \x  \ups B \wat (e, \zeta') \x [f(e\zeta')(a, \zeta)]) \to  c R a \to \# \zeta \]
\[= \forall a \zeta  f e \zeta' . A \wat f \to  \ups B \wat (e, \zeta') \to [f(e\zeta')(a, \zeta)] \to  c R a \to \# \zeta \]
\[= \forall a \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta) \to [fea] \to  c R a \to \# \zeta \]
\[=  \forall \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta) \to eTf \to  c R f \to \# \zeta \]
\[= \forall f e \zeta  .   A \wat f \to
\ups B \wat (e, \zeta) \to c \le f \to f R e \to \# \zeta \]
\[= \forall f . c \le f \to  A \wat f \to \forall e \zeta .
\ups B \wat (e, \zeta) \to  f R e \to \# \zeta \]
\[= \forall f . c \le f \to  A \wat f \to \forall e \zeta .
\ups B \wat (e, \zeta) \to  f \tri (e, \zeta) \]
\[= \forall f . c \le f \to  A \wat f \to \dns \ups B \wat f \]
Ok, in this case $R$ must be $\le$ and $T$ must be $\ge$. The relation
$fea$ becomes $a \le f \x f \le e$ which is variance-incoherent also.
\subsubsection{Trying $a$ unifying with $e$}
We set $[fea] = aSe \x eTf$ for some relations $S$ and $T$. With
$S$ being whatever orientation causes yoneda-unification.

\[\dns (A \lol \ups B) \wat c  \]
\[= \forall a \zeta . (A \lol \ups B) \wat (a, \zeta) \to  c \tri (a, \zeta) \]
\[= \forall a \zeta . (A \lol \ups B) \wat (a, \zeta) \to  c R a \to \# \zeta \]
\[= \forall a \zeta . (\exists f e \zeta' . A \wat f \x  \ups B \wat (e, \zeta') \x [f(e\zeta')(a, \zeta)]) \to  c R a \to \# \zeta \]
\[= \forall a \zeta  f e \zeta' . A \wat f \to  \ups B \wat (e, \zeta') \to [f(e\zeta')(a, \zeta)] \to  c R a \to \# \zeta \]
\[= \forall a \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta) \to [fea] \to  c R a \to \# \zeta \]
\[= \forall a \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta) \to aSe \to eTf  \to  c R a \to \# \zeta \]
\[= \forall  \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta)  \to eTf  \to  c R e \to \# \zeta \eqno(y, aSe) \]
\[ ??? \]
\[= \forall d b \zeta  .   A \wat d \to
\ups B \wat (b, \zeta) \to c \le d \to d R b \to \# \zeta \]
\[= \forall d . c \le d \to  A \wat d \to \forall b \zeta .
\ups B \wat (b, \zeta) \to  d R b \to \# \zeta \]
\[= \forall d . c \le d \to  A \wat d \to \forall b \zeta .
\ups B \wat (b, \zeta) \to  d \tri (b, \zeta) \]
\[= \forall d . c \le d \to  A \wat d \to \dns \ups B \wat d \]
This seems to not work because I have the wrong variables
to unify with the latter half of the argument.
\subsubsection{Trying $e$ unifying with $f$}
Somehow, I don't think this will work, but might as well try.
Set $[fea] = fSe \x eTa$, where $fSe$ yoneda-unifies.

\[\dns (A \lol \ups B) \wat c  \]
\[= \forall a \zeta . (A \lol \ups B) \wat (a, \zeta) \to  c \tri (a, \zeta) \]
\[= \forall a \zeta . (A \lol \ups B) \wat (a, \zeta) \to  c R a \to \# \zeta \]
\[= \forall a \zeta . (\exists f e \zeta' . A \wat f \x  \ups B \wat (e, \zeta') \x [f(e\zeta')(a, \zeta)]) \to  c R a \to \# \zeta \]
\[= \forall a \zeta  f e \zeta' . A \wat f \to  \ups B \wat (e, \zeta') \to [f(e\zeta')(a, \zeta)] \to  c R a \to \# \zeta \]
\[= \forall a \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta) \to [fea] \to  c R a \to \# \zeta \]
\[= \forall a \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta) \to fSe \to eTa \to  c R a \to \# \zeta \]
\[= \forall a \zeta   e  . A \wat e \to  \ups B \wat (e, \zeta)  \to eTa \to  c R a \to \# \zeta \eqno(y, fSe) \]

No, this leaves $A$ and $B$ at the same world, which is bad.

\subsection{Maybe I need a mixed-variance relation?}


So I start with the special case I have a functor $\lol : \P \x \N \to \N$ and give
\[ P \lol N = \int^{\alpha : \PP, \phi : \NN} P(\alpha) \x N(\alpha) \x \NN( \alpha \lol \phi, \dash) \]
as the semantics of $P \lol N  : \NN \to \rset$ given
$P : \PP \to \rset$ and
$N : \NN \to \rset$.
I can take that generalize to a relation
\[R : \PP^\op \x \NN^\op \x \NN  \to \rset \]
and say
\[ P \lol N =  \int^{\alpha : \PP, \phi : \NN} P(\alpha) \x N(\alpha) \x R(\alpha,  \phi, \dash) \]
but also I see no reason why I can't say even for
\[R : (\PP \x \PP^\op) \x (\NN \x \NN^\op) \x \NN  \to \rset \]
that
\[ P \lol N =  \int^{\alpha : \PP, \phi : \NN} P(\alpha) \x N(\alpha) \x R(\alpha, \alpha, \phi, \phi, \dash) \]

So let's go back to the original plan of making $cRa$ yoneda-collapse.
\[\dns (A \lol \ups B) \wat c  \]
\[= \forall a \zeta . (A \lol \ups B) \wat (a, \zeta) \to  c \tri (a, \zeta) \]
\[= \forall a \zeta . (A \lol \ups B) \wat (a, \zeta) \to  c R a \to \# \zeta \]
\[= \forall a \zeta . (\exists f e \zeta' . A \wat f \x  \ups B \wat (e, \zeta') \x [f(e\zeta')(a, \zeta)]) \to  c R a \to \# \zeta \]
\[= \forall a \zeta  f e \zeta' . A \wat f \to  \ups B \wat (e, \zeta') \to [f(e\zeta')(a, \zeta)] \to  c R a \to \# \zeta \]
\[= \forall a \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta) \to [fea] \to  c R a \to \# \zeta \]
\[= \forall  \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta) \to [fec] \to \# \zeta \eqno(y, cRa)\]
\[= \forall f e \zeta  .   A \wat f \to
\ups B \wat (e, \zeta) \to c \le f \to f R e \to \# \zeta \eqno ([fec])\]
\[= \forall f . c \le f \to  A \wat f \to \forall e \zeta .
\ups B \wat (e, \zeta) \to  f R e \to \# \zeta \]
\[= \forall f . c \le f \to  A \wat f \to \forall e \zeta .
\ups B \wat (e, \zeta) \to  f \tri (e, \zeta) \]
\[= \forall f . c \le f \to  A \wat f \to \dns \ups B \wat f \]

So we know that we have to set $[fec] = c \le f \x fRe$.
I think {\em either} orientation of $R$ will make $[fec]$ come out to the same
thing, and all that really determines it is $\ltri$ and maybe yoneda considerations.

For isn't it true that
\[\int^{\alpha, \beta \in \C, \bar\alpha, \bar\beta \in \C^\op} \C[\bar\alpha, \beta] \x F(\alpha, \beta, \bar \alpha, \bar\beta) =
\int^{\alpha, \beta \in \C, \bar\alpha, \bar\beta \in \C^\op} \C[\bar\beta, \alpha] \x F(\alpha, \beta, \bar \alpha, \bar\beta)\]
or something? Given something of the former, I have some objects $a, b\in \C$ and
$\C[a, b] \x F(a, b, a, b)$... no, Idunno.

\subsection{Recalling (co)end/(co)friend}

Loregian's summary of the yoneda lemma says that for any $K : \rset^{\C^\op}$
and $H : \rset^\C$ we have
\[ K \equiv \int^c \C[\dash, c] \x Kc \equiv \int_c \C[c, \dash] \to Kc \]
\[ H \equiv \int^c \C[c, \dash] \x Hc \equiv \int_c \C[\dash, c] \to Hc \]
in logical language
\[ Kd \equiv \exists c. d \le c \x Kc \equiv \forall c. c \le d \to Kc \]
\[ Hd \equiv \exists c . c \le d \x Hc \equiv \forall c. d\le c \to Hc \]
So a salient special case of these --- I think it's going to be the essential
one I need to think about --- is
\[b \le c  \equiv \exists a . b \le a \x a \le c \]

which is interesting in its own right, because it shows off how the
coend is different from a bare existential. The bare existential
quantifier would conceptually have potentially {\em lots} of witnesses
in the interval between $b$ and $c$, at the very least $b$ and $c$ themselves ---
but the coend equates all of those witnesses. It's a kind of proof-irrelevance `up to'
the morphisms involved.


So if I define
\[c \tri (a, \zeta) = c\le \alpha \to \# \zeta\]
\[[fea] = a \le f \x f \le e \]
I can reason
\[\dns (A \lol \ups B) \wat c  \]
\[= \forall a \zeta . (A \lol \ups B) \wat (a, \zeta) \to  c \tri (a, \zeta) \]
\[= \forall a \zeta . (A \lol \ups B) \wat (a, \zeta) \to  c \le a \to \# \zeta \]
\[= \forall a \zeta . (\exists f e \zeta' . A \wat f \x  \ups B \wat (e, \zeta') \x [f(e\zeta')(a, \zeta)]) \to  c \le a \to \# \zeta \]
\[= \forall a \zeta  f e \zeta' . A \wat f \to  \ups B \wat (e, \zeta') \to [f(e\zeta')(a, \zeta)] \to  c \le a \to \# \zeta \]
\[= \forall a \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta) \to [fea] \to  c \le a \to \# \zeta \]
\[= \forall a \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta) \to a \le f \to f \le e \to  c \le a \to \# \zeta \eqno([fea]) \]
\[= \forall \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta) \to (\exists a .   c \le a \x a \le f ) \to f \le e    \to \# \zeta  \]
\[= \forall \zeta  f e  . A \wat f \to  \ups B \wat (e, \zeta) \to   c \le  f \to f \le e    \to \# \zeta  \eqno(y)\]
\[= \forall f . c \le f \to  A \wat f \to \forall e \zeta .
\ups B \wat (e, \zeta) \to  f \le e \to \# \zeta \]
\[= \forall f . c \le f \to  A \wat f \to \forall e \zeta .
\ups B \wat (e, \zeta) \to  f \tri (e, \zeta) \]
\[= \forall f . c \le f \to  A \wat f \to \dns \ups B \wat f \]

\subsection{Products}
How about trying to define $\x$ as a certain kind of $\tensor$?
I'd conjecture either $R_\tensor[abc] = (c \le a) \x (c \le b)$
or $R_\tensor[abc] = (a \le c) \x (b \le c)$, whichever works out with the right variance...
There aren't necessarily shifts involved, so that makes things easier, I think.
Or do I want shifts to give enough quantifiers to do yoneda-ish things to in the first place?

Well, recall the ninja yoneda lemma says that for a covariant functor $A : \PP \to \rset$,
I should expect
\[Hc \equiv \exists a . a \le c \x H a \]
so
\[(A \tensor  B) \wat c  \]
\[= \exists a b . A(a) \x B(b) \x (a \le c) \x (b \le c) \]
\[= \exists  b . A(c) \x B(b)  \x (b \le c) \]
\[= A(c) \x B(c)  \]
so there's the pointwise product I expect. And if the underlying
category $\PP$ has coproducts, this could be just $R_\tensor[abc] =
((a + b) \le c)$. And maybe this makes sense because I'm working kind
of contravariantly to how I might otherwise more normally be doing
things. But interestingly this definition works even if $\PP$ has no
particular structure; I'm sort of (co)freely adding the products. Or
importing them from the metatheory or something.

\subsection{Focusing and inequalities}

How does a proof of the ninja yoneda lemmas actually look?

The easy direction involves reflexivity:
\[
\begin{prooftree}
\[
\justifies
  \alpha : \C ; H(\alpha) \prov \alpha \le \alpha \x H(\alpha)
\]
\justifies
  \alpha : \C ; H(\alpha) \prov \exists \beta . \beta \le \alpha \x H(\beta)
\end{prooftree}
\]
The other direction necessarily involves some use of a left rule for $\le$:
\[
\begin{prooftree}
\[
\[
\justifies
\gamma : \C ; H(\gamma) \prov H(\gamma)
\]
\using \le L
\justifies
\alpha : \C, \beta : \C ; \beta \le \alpha, H(\beta) \prov H(\alpha)
\]
\justifies
  \alpha : \C ; \exists \beta . \beta \le \alpha \x H(\beta) \prov H(\alpha)
\end{prooftree}
\]
Alternatively I could think of $\le$ as a positive atom, and every covariant
atomic proposition as baking in an appropriate quantifier?

Like $H^+(\alpha)$ really means $\exists \beta . \beta \le \alpha \x h^+(\alpha)$
for some underlying $h^+(\alpha)$ that isn't particularly covariant.
And $H^-(\alpha)$ really means $\forall \beta . \alpha \le \beta \to h^-(\beta)$.
In this case $\le$ inverts on the left into some judgmental $\sqsubseteq$, and
synchronously requires it on the right. To check that the ninja yoneda
lemma holds, we basically just need to check that `two bits of slack' is the
same as `one bit of slack', i.e. the equivalence of for instance
\[\exists \beta . \beta \le \alpha \x \exists \gamma . \gamma \le \beta \x H(\gamma)
\prequiv
\exists \beta . \beta \le \alpha \x H(\beta)\]
but we do have
\[\exists \beta . \beta \le \alpha \x \exists \gamma . \gamma \le \beta \x H(\gamma) \]
\[\prequiv \exists \gamma . (\exists \beta . \gamma \le \beta \x \beta \le \alpha) \x H(\gamma)\]
\[\prequiv \exists \gamma .  \gamma \le \alpha \x H(\gamma)\]
\[\prequiv_\alpha \exists \beta .  \beta \le \alpha \x H(\gamma)\]
and the ouroborous ninja-yoneda lemma
\[\gamma \le \alpha \prequiv \exists \beta . \gamma \le \beta \x \beta \le \alpha\]
can I think be proved directly without resorting to augmenting the atom $\le$
itself with any quantification. Let's try:

\[
\begin{prooftree}
\[
\justifies
\gamma \le \alpha \prov   \gamma \le \gamma \x \gamma \le \alpha
\]
\justifies
\gamma \le \alpha \prov \exists \beta . \gamma \le \beta \x \beta \le \alpha
\end{prooftree}
\]

\[
\begin{prooftree}
\[
\justifies
 \gamma \sqsubseteq \beta, \beta \sqsubseteq \alpha \prov [\gamma \le \alpha]
\]
\justifies
 \exists \beta . \gamma \le \beta \x \beta \le \alpha \prov \gamma \le \alpha
\end{prooftree}
\]
Sure, super easy, no problem.

\section{A high-level story I don't understand yet}

I say: the syntactic diagnostic sign of
what it is to be a (focusing-)positive contravariant proposition
\[ \alpha : \C \prov P(\bar\alpha) : \rtype \]
is that
\[ P(\alpha) \prequiv \exists \beta . \alpha \le \beta \x P(\beta) \]
and to be a negative covariant proposition
\[ \alpha : \C \prov N(\alpha) : \rtype \]
is
\[ N(\alpha) \prequiv \forall \beta . \alpha \le \beta \to N(\beta) \]
and I could fill out a table
\[\begin{tabular}{ccl}
  {\bf focusing}&{\bf variance}\\
$-$&$-$&$X(\alpha) \prequiv \forall \beta . \beta \le \alpha \to X(\beta)$\\
$+$&$-$&$X(\alpha) \prequiv \exists \beta . \alpha \le \beta \x X(\beta)$\\
$-$&$+$&$X(\alpha) \prequiv \forall \beta . \alpha \le \beta \to X(\beta)$\\
$+$&$+$&$X(\alpha) \prequiv \exists \beta . \beta \le \alpha \x X(\beta)$\\
\end{tabular}\]
for all the possibilities.

I know that given a positive contravariant $P$ and a negative covariant $N$, I can make
\[ P(\bar \alpha) \to N(\alpha) \]
which is negative covariant. But also, given positive {\bf co}variant $Q$ and
negative covariant $N$,
I can make
\[ \forall \beta . \alpha \le \beta \to Q( \beta) \to N(\beta) \]
which is still covariant in $\alpha$. It's the end $\forall$ that's letting
me use $\beta$ in a covariant functor on the left of a $\to$.
Now I want to show that this expression is equal to something else convenient.
Like... can I confine the quantifier to $Q$, somehow? Can I show
\[(\exists \beta . \alpha \le \beta \x  Q( \beta)) \to N(\alpha)
\prequiv
\forall \beta . \alpha \le \beta \to Q( \beta) \to N(\beta)
\]
Forward direction:
\[
\begin{prooftree}
\[
\[
\[
\justifies
\forall \beta . \alpha \le \beta \to N(\beta), \alpha \le \beta \prov N(\beta)
\]
\justifies
N(\alpha), \alpha \le \beta \prov N(\beta)
\]
\[
\justifies
\alpha \le \beta, Q(\beta) \prov \exists \beta . \alpha \le \beta \x Q(\beta)
\]
\justifies
(\exists \beta . \alpha \le \beta \x  Q( \beta)) \to N(\alpha),
\alpha \le \beta, Q(\beta)  \prov  N(\beta)
\]
\justifies
(\exists \beta . \alpha \le \beta \x  Q( \beta)) \to N(\alpha)
\prov
\forall \beta . \alpha \le \beta \to Q( \beta) \to N(\beta)
\end{prooftree}
\]
works fine. Backwards direction:
\[
\begin{prooftree}
\[
\[
\[
\textcolor{red}{XXX}
\justifies
\alpha \le \beta , \forall \gamma . \beta \le \gamma \to N(\gamma)
 \prov N(\alpha)
\]
\justifies
\alpha \le \beta , N(\beta)
 \prov N(\alpha)
\]
\justifies
\forall \beta . \alpha \le \beta \to Q( \beta) \to N(\beta)
,
\alpha \le \beta ,  Q( \beta) \prov N(\alpha)
\]
\justifies
\forall \beta . \alpha \le \beta \to Q( \beta) \to N(\beta)
\prov
(\exists \beta . \alpha \le \beta \x  Q( \beta)) \to N(\alpha)
\end{prooftree}
\]
Dang, this seems like the wrong direction. Or can I try using reflexivity and
expanding $Q$ instead:
\[
\begin{prooftree}
\[
\[
\justifies
\alpha \le \alpha
\]
\[
\textcolor{red}{XXX}
\justifies
\alpha \le \beta , Q(\beta)
 \prov Q(\alpha)
\]
\justifies
\forall \beta . \alpha \le \beta \to Q( \beta) \to N(\beta)
,
\alpha \le \beta ,  Q( \beta) \prov N(\alpha)
\]
\justifies
\forall \beta . \alpha \le \beta \to Q( \beta) \to N(\beta)
\prov
(\exists \beta . \alpha \le \beta \x  Q( \beta)) \to N(\alpha)
\end{prooftree}
\]
No, this is the wrong direction, too.

\subsection{But this is what I wrote down on paper}
On paper I did the calculation of the equivalence of the sort of ill-varianced
\[P(\alpha) \to N(\alpha) \prequiv \forall \beta . \alpha \le \beta \to P(\beta) \to N(\beta)\]
On the basis of
\[
\begin{prooftree}
\[
\[
\justifies
 N(\alpha) , \alpha \le \beta \prov N(\beta)
\]
\[
\justifies
 \alpha \le \beta , P(\beta) \prov P(\alpha)
\]
\justifies
P(\alpha) \to N(\alpha) , \alpha \le \beta , P(\beta) \prov N(\beta)
\]
\justifies
P(\alpha) \to N(\alpha) \prov \forall \beta . \alpha \le \beta \to P(\beta) \to N(\beta)
\end{prooftree}
\]
and
\[
\begin{prooftree}
\[
\justifies
 \alpha \le \alpha
\]
\[
\justifies
  P(\alpha) \to N(\alpha) \prov P(\alpha) \to N(\alpha)
\]
\justifies
\forall \beta . \alpha \le \beta \to P(\beta) \to N(\beta) \prov P(\alpha) \to N(\alpha)
\end{prooftree}
\]
Or is this really ill-varianced? I'm not sure. In a sequents-are-ends world, there's nothing
wrong with $P(\alpha) \to N(\alpha)$ for {\em some} flipping of the $\alpha$s, regardless
of what kind of functor $P$ is. The variance of $P$ is established nonetheless
by the assumption that $P(\alpha) \prequiv \exists \alpha \le \beta \x P(\beta)$.
And what we're establishing with this little proof is that if $N$ is negative covariant,
and $P$ is positive contravariant, then $P \to N$ is negative covariant.

\subsection{The difference between positive and negative propositions?}

Ninja yoneda seems to be saying that every covariant proposition is negative and
positive. Do I have, with enough shifts, that if
\[N(a) \prequiv \forall b . a \le b \to N(b)\]
then also
\[N(c) \prequiv \exists d . d \le c \x N(d)\]
? Oh, yeah, most likely, this is just an issue of focusing. I remember now.

\[
\begin{prooftree}
\[
\justifies
c \le c
\]
\[
\justifies
N(c) \prov N(c)
\]
\justifies
N(c) \prov \exists d . d \le c \x N(d)
\end{prooftree}
\]

\[
\begin{prooftree}
\[
\[
\[
\justifies
 d \le c \prov d \le c
\]
\[
\justifies
  N(c) \prov N(c)
\]
\justifies
 d \le c , \forall b . d \le b \to N(b) \prov N(c)
\]
\justifies
 d \le c , N(d) \prov N(c)
\]
\justifies
 \exists d . d \le c \x N(d) \prov N(c)
\end{prooftree}
\]
Right, this is all fine. So the real diagnostic is about variance, and separately
I know I have confidence that I can express it in a focally positive or negative
way if that became useful for completeness proofs.
\subsection{Why this is important}
This is a syntactic diagnostic for variance that didn't require me do any particular
magical thing with $\le$ or groveling through type expressions counting the number
of flips. Perhaps I can prove a dependent version.
We say: a type $X$ depending on a category-typed variable $\alpha$ is covariant
when
\[X(\alpha) \equiv \forall \beta . \alpha \le \beta \to X(\beta)\]
and it's contravariant when
\[X(\alpha) \equiv \exists \beta . \alpha \le \beta \x X(\beta)\]
Can I show something formal to go with the informal ``$(x : A) \to B$
is covariant when $A$ is contravariant and $B$ is covariant''?
I want to say something like
\[ \alpha : \C ; x : A(\alpha) \prov  B(\alpha, x) \equiv \forall \beta .\alpha \le \beta \to B(\beta, x) \]
but that's ill-typed, right? I have to transport $x$ somehow to get it to fit into
$B(\beta, x)$. Maybe I should head to agda. But just sketching it out, I'd have
the assumption
\[ \alpha : \C ; x : A(\alpha) \prov  B(\alpha, x) \equiv \forall \beta .
(p : \alpha \le \beta) \to B(\beta, A_p(x)) \]
...wait, that requires that $A$ transports covariantly. That seems very wrong.
Look, $A$'s contravariance looks like either of
\[\alpha : \C \prov A(\alpha) \equiv \exists \beta . \alpha \le \beta \x A(\beta)\]
\[\alpha : \C \prov A(\alpha) \equiv \forall \beta . \beta \le \alpha \to A(\beta)\]
Maybe... I decisively need to use the `$\bar\beta$' of the end to do
reverse transport? No, that seems kind of weird.

The forward direction is easy. The backward direction has a $\beta$ to work with,
and a $p : \beta \le \alpha$ we could plug into transport. Also
a function $f : (x : A(\beta)) \to B(\beta, x)$. We have
a $x : A(\alpha)$ and we want to make $B(\alpha, x)$.
We can reverse-transport along $p$ to get $A_p(x) : A(\beta)$. Applying
$f$ we get $f (A_p(x)) : B(\beta, A_p(x))$.
And, now that we have a $p : \beta \le \alpha$ and a $B(\beta, A_p(x))$, we
can do $B$-transport to get a $B(\alpha, x)$ as required.
So now I need to check round-trips, and I probably want agda to do that. But this is
promising!

Right, a sudden thought I had was that the appropriate requirement is not just
that, e.g. an equivalence $A(\alpha) \equiv \exists \beta . \alpha \le \beta \x A(\beta)$
exists, but that {\em the map}
\[A(\alpha) \to \exists \beta . \alpha \le \beta \x A(\beta)\]
that chooses $\beta = \alpha$ and pops in the reflexivity is an equivalence.
Or dually, {\em the map}
\[ (\forall \beta . \alpha \le \beta \to A(\beta)) \to A(\alpha) \]
that chooses $\beta = \alpha$ and pops in reflexivity is an equivalence.

Right, a sudden thought I had was that the appropriate requirement is not just
that, e.g. an equivalence $A(\alpha) \equiv \exists \beta . \alpha \le \beta \x A(\beta)$
exists, but that {\em the map}
\[A(\alpha) \to \exists \beta . \alpha \le \beta \x A(\beta)\]
that chooses $\beta = \alpha$ and pops in the reflexivity is an equivalence.
Or dually, {\em the map}
\[ (\forall \beta . \alpha \le \beta \to A(\beta)) \to A(\alpha) \]
that chooses $\beta = \alpha$ and pops in reflexivity is an equivalence.

\subsection{What Did I learn From Agda}

A weird/interesting thing about saying that

\[\alpha : \C ; x : A(\alpha) \prov B(\alpha, x) \equiv \exists \beta .
(p : \beta \le \alpha) \x B(\beta, A_p(x))\]

comes from observing that the round-trip starting at
\[ \exists \beta . (p : \beta \le \alpha) \x B(\beta, A_p(x))\]
is the identity. It's possible to get very confused if I forget (as I did!)
that that existential is {\em not} an ordinary $\Sigma$-type, and is instead
a coend. For if I indulge in that confusion, I start with a package
$\langle \beta, p, b\rangle$, I carry it back to
$\iota \langle \beta, p, b\rangle : B(\alpha, x)$ via the assumed
inverse-to-the-canonical-reflexivity-map that exists as a witness
to $B$'s covariance, and then I pop that into the canonical-reflexivity-map
to get $\langle \alpha, \rid_\alpha, \iota \langle \beta, p, b\rangle\rangle$.

If I think of these as ordinary existential bundles, then I'd take a look
at the equation
\[\langle \alpha, \rid_\alpha, \iota \langle \beta, p, b\rangle\rangle = \langle \beta, p, b\rangle\]
and conclude that $\alpha = \beta$, an unwarrantedly strong conclusion given
the assumption that merely $\beta \le \alpha$! So I have to be more careful about that.
So what can I conclude from
\[t \equiv u : \int^{c\in\C} P_{cc}\]
? I know $t$ and $u$ both {\em can} be represented as
$\langle c, p \rangle$ for $p : P_{cc}$, but not uniquely.
The coequalizing condition is that for any $f : c \to d$
\[
\erule
{P_{fc}(p_c)  =  P_{df}(p_d)  : P_{dc}}
{\langle c, p_c\rangle = \langle d , p_d \rangle}
\]
Perhaps... is this rule invertible, then? Can I canonically get out of
any path a morphism that witnesses the equality?
It seems like knowing ${\langle c, p_c\rangle = \langle d , p_d \rangle}$
shouldn't be enough to give a morphism $c \to d$, because then it'd be enough
to give a morphism $d \to c$ also. Is it enough to give a span $c \leftarrow \bullet \to d$?
Then if it actually is the case that ${\langle c, p_c\rangle = \langle d , p_d \rangle}$
was constructed out of $f : c \to d$, we'd return $c \leftarrow c \to d$, and if
 symmetrically  we had done
\[
\erule
{P_{cg}(p_c)  =  P_{gd}(p_d)  : P_{cd}}
{\langle c, p_c\rangle = \langle d , p_d \rangle}
\]
for a $g : d \to c$, we could return $c \leftarrow d \to d$.

No, we probably need to demand the underlying category has pullbacks
to make this work. I'd rather not if I could get away with it.
It really should be an arbitrary zigzag in $\C$, shouldn't it?
\subsection{How do I think about general types}
So when I say something like
\[ A(\alpha) \equiv \forall \beta . \beta \le \alpha \to A(\beta) \]
\[ A(\alpha) \equiv \exists \beta . \beta \le \alpha \x A(\beta) \]
the assumption is that $A$'s argument is actually doubled, and I'm talking about
half of it at a time. So that assumption could be internalized by saying that
every type $A(\dash)$ has a different version $\circ A(\dash, \dash)$
such that
\[A(\alpha) = \circ A(\alpha, \alpha) \]
\[ \circ A(\alpha_1, \alpha_2) \equiv \forall \beta . \beta \le \alpha \to A(\alpha_1, \beta) \]
\[ \circ A(\alpha_1, \alpha_2) \equiv \exists \beta . \beta \le \alpha \x A(\beta, \alpha_2) \]

\end{document}
